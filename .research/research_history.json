{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data efficient classification",
    "novel regularization techniques",
    "augmentation image classification"
  ],
  "research_study_list": [
    {
      "title": "Automated Data Augmentations for Graph Classification",
      "full_text": "Published as a conference paper at ICLR 2023 AUTOMATED DATA AUGMENTATIONS FOR GRAPH CLASSIFICATION Youzhi Luo1∗, Michael McThrow2, Wing Au2, Tao Komikado3, Kanji Uchino2, Koji Maruhashi3, Shuiwang Ji1 1Texas A&M University, TX, USA 2Fujitsu Research of America, INC., CA, USA 3Fujitsu Research, Fujitsu Limited, Kanagawa, Japan {yzluo,sji}@tamu.edu {mmcthrow,WAu,komikado.tao,kanji,maruhashi.koji}@fujitsu.com ABSTRACT Data augmentations are effective in improving the invariance of learning ma- chines. We argue that the core challenge of data augmentations lies in designing data transformations that preserve labels. This is relatively straightforward for im- ages, but much more challenging for graphs. In this work, we propose GraphAug, a novel automated data augmentation method aiming at computing label-invariant augmentations for graph classiﬁcation. Instead of using uniform transformations as in existing studies, GraphAug uses an automated augmentation model to avoid compromising critical label-related information of the graph, thereby producing label-invariant augmentations at most times. To ensure label-invariance, we de- velop a training method based on reinforcement learning to maximize an estimated label-invariance probability. Experiments show that GraphAug outperforms pre- vious graph augmentation methods on various graph classiﬁcation tasks. 1 I NTRODUCTION Many real-world objects, such as molecules and social networks, can be naturally represented as graphs. Developing effective classiﬁcation models for these graph-structured data has been highly desirable but challenging. Recently, advances in deep learning have signiﬁcantly accelerated the progress in this direction. Graph neural networks (GNNs) (Gilmer et al., 2017), a class of deep neural network models speciﬁcally designed for graphs, have been widely applied to many graph representation learning and classiﬁcation tasks, such as molecular property prediction (Wang et al., 2022b; Liu et al., 2022; Wang et al., 2022a; 2023; Yan et al., 2022). However, just like deep models on images, GNN models can easily overﬁt and fail to achieve satis- factory performance on small datasets. To address this issue, data augmentations can be used to gen- erate more data samples. An important property of desirable data augmentations is label-invariance, which requires that label-related information should not be compromised during the augmentation process. This is relatively easy and straightforward to achieve for images (Taylor & Nitschke, 2018), since commonly used image augmentations, such as ﬂipping and rotation, can preserve almost all information of original images. However, ensuring label-invariance is much harder for graphs be- cause even minor modiﬁcation of a graph may change its semantics and thus labels. Currently, most commonly used graph augmentations (You et al., 2020) are based on random modiﬁcation of nodes and edges in the graph, but they do not explicitly consider the importance of label-invariance. In this work, we propose GraphAug, a novel graph augmentation method that can produce label- invariant augmentations with an automated learning model. GraphAug uses a learnable model to automate augmentation category selection and graph transformations. It optimizes the model to maximize an estimated label-invariance probability through reinforcement learning. Experimen- tal results show that GraphAug outperforms prior graph augmentation methods on multiple graph classiﬁcation tasks. The codes of GraphAug are available in DIG (Liu et al., 2021) library. ∗Work was done while the author was at Fujitsu Research of America, INC. 1 arXiv:2202.13248v4  [cs.LG]  28 Feb 2023Published as a conference paper at ICLR 2023 2 B ACKGROUND AND RELATED WORK 2.1 G RAPH CLASSIFICATION WITH NEURAL NETWORKS In this work, we study the problem of graph classiﬁcation. Let G = (V,E,X ) be an undirected graph, where V is the set of nodes and Eis the set of edges. The node feature matrix of the graphG is X ∈R|V|×d where the i-th row of X denotes the d-dimensional feature vector for the i-th node in G. For a graph classiﬁcation task with kcategories, the objective is to learn a classiﬁcation model f : G→y∈{1,...,k }that can predict the categorical label of G. Recently, GNNs (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Xu et al., 2019; Gilmer et al., 2017; Gao & Ji, 2019) have shown great success in various graph classiﬁcation problems. Most GNNs use the message passing mechanism to learn graph node embeddings. Formally, the message passing for any node v∈V at the ℓ-th layer of a GNN model can be described as hℓ v = UPDATE ( hℓ−1 v ,AGG ({ mℓ jv : j ∈N(v) })) , (1) where N(v) denotes the set of all nodes connected to the nodevin the graph G, hℓ v is the embedding outputted from the ℓ-th layer for v, mℓ jv is the message propagated from the node jto the node vat the ℓ-th layer and is usually a function ofhℓ−1 v and hℓ−1 j . The aggregation function AGG(·) maps the messages from all neighboring nodes to a single vector, and the function UPDATE(·) updates hℓ−1 v to hℓ v using this aggregated message vector. Assuming that the GNN model has Llayers, the graph representation hG is computed by a global pooling function READOUT over all node embeddings as hG = READOUT ({ hL v : v∈V }) . (2) Afterwards, hG is fed into a multi-layer perceptron (MLP) model to compute the probability that G belongs to each of the categories {1,...,k }. Despite the success of GNNs, a major challenge in many graph classiﬁcation problems is data scarcity. For example, GNNs have been extensively used to predict molecular properties from graph structures of molecules. However, the manual labeling of molecules usually requires expensive wet lab experiments, so the amount of labeled molecule data is usually not large enough for expressive GNNs to achieve satisfactory prediction accuracy. In this work, we address this data scarcity chal- lenge with data augmentations. We focus on designing advanced graph augmentation strategies to generate more data samples by performing transformations on data samples in the dataset. 2.2 D ATA AUGMENTATIONS Data augmentations have been demonstrated to be effective in improving the performance for image and text classiﬁcation. For images, various image transformation or distortion techniques have been proposed to generate artiﬁcial image samples, such as ﬂipping, cropping, color shifting (Krizhevsky et al., 2012), scaling, rotation, and elastic distortion (Sato et al., 2015; Simard et al., 2003). And for texts, useful augmentation techniques include synonym replacement, positional swaps (Ratner et al., 2017a), and back translation (Sennrich et al., 2016). These data augmentation techniques have been widely used to reduce overﬁtting and improve robustness in training deep neural network models. In addition to hand-crafted augmentations, automating the selection of augmentations with learnable neural network model has been a recent emerging research area. Ratner et al. (2017b) selects and composes multiple image data augmentations using an LSTM (Hochreiter & Schmidhuber, 1997) model, and proposes to make the model avoid producing out-of-distribution samples through adver- sarial training. Cubuk et al. (2019) proposes AutoAugment, which adopts reinforcement learning based method to search optimal augmentations maximizing the classiﬁcation accuracy. To speed up training and reduce computational cost, a lot of methods have been proposed to improve AutoAug- ment through either faster searching mechanism (Ho et al., 2019; Lim et al., 2019), or advanced optimization methods (Hataya et al., 2020; Li et al., 2020; Zhang et al., 2020). 2.3 D ATA AUGMENTATIONS FOR GRAPHS While image augmentations have been extensively studied, doing augmentations for graphs is much more challenging. Images are Euclidean data formed by pixel values organized in matrices. Thus, 2Published as a conference paper at ICLR 2023 many well studied matrix transformations can naturally be used to design image augmentations, such as ﬂipping, scaling, cropping or rotation. They are either strict information lossless transformation, or able to preserve signiﬁcant information at most times, so label-invariance is relatively straight- forward to be satisﬁed. Differently, graphs are non-Euclidean data formed with nodes connected by edges in an irregular manner. Even minor structural modiﬁcation of a graph can destroy important information in it. Hence, it is very hard to design generic label-invariant transformations for graphs. Currently, designing data augmentations for graph classiﬁcation (Zhao et al., 2022; Ding et al., 2022; Yu et al., 2022) is a challenging problem. Some studies (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) propose interpolation-based mixup methods for graph augmentations, and Kong et al. (2022) propose to augment node features through adversarial learning. Nonetheless, most commonly used graph augmentation methods (Hamilton et al., 2017; Wang et al., 2020; You et al., 2020; Zhou et al., 2020a; Rong et al., 2020; Zhu et al., 2021a) are based on the random modiﬁcation of graph structures or features, such as randomly dropping nodes, perturbing edges, or masking node features. However, such random transformations are not necessarily label-invariant, because important label-related information may be randomly compromised (see Section 3.2 for detailed analysis and discussion). Hence, in practice, these augmentations do not always improve the performance of graph classiﬁcation models. 3 T HE PROPOSED GRAPH AUG METHOD While existing graph augmentation methods do not consider the importance of label-invariance, we dive deep into this challenging problem and propose to solve it by automated data augmentations. Note that though automated data augmentations have been applied to graph contrastive learning (You et al., 2021; Yin et al., 2022; Suresh et al., 2021; Hassani & Khasahmadi, 2022; Xie et al., 2022) and node classiﬁcation (Zhao et al., 2021; Sun et al., 2021), they have not been studied in supervised graph classiﬁcation. In this work, we propose GraphAug, a novel automated data augmentation framework for graph classiﬁcation. GraphAug automates augmentation category selection and graph transformations through a learnable augmentation model. To produce label-invariant augmentations, we optimize the model to maximize an estimated label-invariance probability with reinforcement learning. To our best knowledge, GraphAug is the ﬁrst work successfully applying automated data augmentations to generate new graph data samples for supervised graph classiﬁcation. 3.1 A UGMENTATION BY SEQUENTIAL TRANSFORMATIONS Similar to the automated image augmentation method in Ratner et al. (2017b), we consider graph augmentations as a sequential transformation process. Given a graph G0 sampled from the train- ing dataset, we map it to the augmented graph GT with a sequence of transformation functions a1,a2,...,a T generated by an automated data augmentation model g. Speciﬁcally, at the t-th step (1 ≤t ≤T), let the graph obtained from the last step be Gt−1, we ﬁrst use the augmentation model to generate at based on Gt−1, and map Gt−1 to Gt with at. In summary, this sequential augmentation process can be described as at = g(Gt−1), G t = at(Gt−1), 1 ≤t≤T. (3) In our method, a1,a2,...,a T are all selected from three categories of graph transformations: • Node feature masking (MaskNF), which sets some values in node feature vectors to zero; • Node dropping (DropNode), which drops certain portion of nodes from the input graph; • Edge perturbation (PerturbEdge), which produces the new graph by removing existing edges from the input graph and adding new edges to the input graph. 3.2 L ABEL -INVARIANT AUGMENTATIONS Most automated image augmentation methods focus on automating augmentation category selection. For instance, Ratner et al. (2017b) automate image augmentations by generating a discrete sequence from an LSTM (Hochreiter & Schmidhuber, 1997) model, and each token in the sequence represents a certain category of image transformation, such as random ﬂip and rotation. Following this setting, 3Published as a conference paper at ICLR 2023 our graph augmentation model g also selects the augmentation category at each step. Speciﬁcally, g will generate a discrete token ct representing the category of augmentation transformation at, denoting whether MaskNF, DropNode, or PerturbEdge will be used at the t-th step. We have experimented to only automate augmentation category selection and use the graph trans- formations that are uniformly operated on each graph element, such as each node, edge, or node feature. For example, the uniform DropNode will randomly drop each node in the graph with the same probability. These transformations are commonly used in other studies (You et al., 2020; Zhu et al., 2021a; Rong et al., 2020), and we call them as uniform transformations. However, we ﬁnd that this automated composition of multiple uniform transformations does not improve classiﬁca- tion performance (see Section 4.3 for details). We argue that it is because uniform transformations have equal chances to randomly modify each graph element, thus may accidentally damage signif- icant label-related information and change the label of the original data sample. For instance, in a molecular graph dataset, assuming that all molecular graphs containing a cycle are labeled as toxic because the cyclic structures are exactly the cause of toxicity. If we are using DropNode transfor- mation, dropping any node belonging to the cycle will damage this cyclic structure, and map a toxic molecule to a non-toxic one. By default, data augmentations only involve modifying data samples while labels are not changed, so data augmentations that are not label-invariant may ﬁnally produce many noisy data samples and greatly harm the training of the classiﬁcation model. We use the TRIANGLES dataset (Knyazev et al., 2019) as an example to study the effect of label- invariance. The task in this dataset is classifying graphs by the number of triangles (the cycles formed by only three nodes) contained in the graph. As shown in Figure 3 of Appendix A, the uni- form DropNode transformation is not label-invariant because it produces data samples with wrong labels through dropping nodes belonging to triangles, and the classiﬁcation accuracy is low when the classiﬁcation model is trained on these data samples. However, if we intentionally avoid dropping nodes in triangles, training the classiﬁcation model with this label-invariant data augmentation im- proves the classiﬁcation accuracy. The signiﬁcant performance gap between these two augmentation strategies clearly demonstrates the importance of label-invariance for graph augmentations. Based on the above analysis and experimental results, we can conclude that uniform transforma- tions should be avoided in designing label-invariant graph augmentations. Instead, we generate transformations for each element in the graph by the augmentation modelgin our method. Next, we introduce the detailed augmentation process in Section 3.3 and the training procedure in Section 3.4. 3.3 A UGMENTATION PROCESS Our augmentation model gis composed of three parts. They are a GNN based encoder for extracting features from graphs, a GRU (Cho et al., 2014) model for generating augmentation categories, and four MLP models for computing probabilities. We use graph isomorphism network (GIN) (Xu et al., 2019) model as the encoder. At the t-th augmentation step ( 1 ≤t ≤T), let the graph obtained from the last step be Gt−1 = (Vt−1,Et−1,Xt−1), we ﬁrst add a virtual node vvirtual into Vt−1 and add edges connecting the virtual node with all the nodes in Vt−1. In other words, a new graph G′ t−1 = (V′ t−1,E′ t−1,X′ t−1) is created from Gt−1 such that V′ t−1 = Vt−1 ∪{vvirtual}, E′ t−1 = Et−1 ∪{(vvirtual,v) :v∈Vt−1}, and X′ t−1 ∈R|V′ t−1|×d is the concatenation of Xt−1 and a trainable initial feature vector for the virtual node. We use the virtual node here to extract graph-level information because it can capture long range interactions in the graph more effectively than a pooling based readout layer (Gilmer et al., 2017). The GNN encoder performs multiple message passing operations on G′ t−1 to obtain r- dimensional embeddings {ev t−1 ∈Rr : v∈Vt−1}for nodes in Vt−1 and the virtual node embedding evirtual t−1 ∈Rr. Afterwards, the probabilities of selecting each augmentation category is computed from evirtual t−1 as qt = GRU(qt−1,evirtual t−1 ), p C t = MLPC(qt),where qt is the hidden state vector of the GRU model at thet-th step, and the MLP model MLPC outputs the probability vectorpC t ∈R3 denoting the probabilities of selecting MaskNF, DropNode, or PerturbEdge as the augmentation at the t-th step. The exact augmentation categoryctfor the t-th step is then randomly sampled from the categorical distribution with the probabilities in pC t . Finally, as described below, the computation of transformation probabilities for all graph elements and the process of producing the new graph Gt from Gt−1 vary depending on ct. 4Published as a conference paper at ICLR 2023 DropNode virtual node GNN  encoder virtual node  embedding node  embeddings GRU MaskNF PerturbEdge sample node  embeddings node features virtual node feature  masked node  features 1 0 … node  embeddings sample for each  node feature sample for  each node virtual node  embedding sample for  each edge edge embeddings Extract node embeddings Select  augmentation category Perform transformation + , + , Figure 1: An illustration of the process of producing Gt from Gt−1 with the augmentation model. • If ct is MaskNF, then for any node v ∈Vt−1, the probabilities pM t,v ∈Rd of masking each node feature of v is computed by the MLP model MLP M taking the node embedding ev t−1 as input. Afterwards, a binary vector oM t,v ∈{0,1}d is randomly sampled from the Bernoulli distribution parameterized with pM t,v. If the k-th element of oM t,v is one, i.e., oM t,v[k] = 1, the k-th node feature of vis set to zero. Such MaskNF transformation is performed for every node feature in Xt−1. • If ctis DropNode, then the probabilitypD t,v of dropping any nodev∈Vt−1 from Gt−1 is computed by the MLP model MLP D taking the node embedding ev t−1 as input. Afterwards, a binary value oD t,v ∈{0,1}is sampled from the Bernoulli distribution parameterized with pD t,v and vis dropped from Vt−1 if oD t,v = 1. Such DropNode transformation is performed for every node in Vt−1. • If ct is PerturbEdge, the transformations involve dropping some existing edges from Et−1 and adding some new edges into Et−1. We consider the set Et−1 as the droppable edge set, and we create an addable edge set Et−1, by randomly sampling at most |Et−1|addable edges from the set {(u,v) : u,v ∈Vt−1,(u,v) /∈Et−1}. For any (u,v) in Et−1, we compute the probability pP t,(u,v) of dropping it by the MLP model MLP P taking [eu t−1 + ev t−1,1] as input, where [·,·] denotes the concatenation operation. For any (u,v) in Et−1, we compute the probability pP t,(u,v) of adding an edge connecting uand vby MLPP taking [eu t−1 + ev t−1,0] as input. Afterwards, for every (u,v) ∈Et−1, we randomly sample a binary value oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v), and drop (u,v) from Et−1 if oP t,(u,v) = 1. Similarly, we randomly sample oP t,(u,v) for every (u,v) ∈Et−1 but we will add (u,v) into Et−1 if oP t,(u,v) = 1. An illustration of the process of producing Gt from Gt−1 with our augmentation model is given in Figure 1. We also provide the detailed augmentation algorithm in Algorithm 1 of Appendix B. 3.4 L ABEL -INVARIANCE OPTIMIZATION WITH REINFORCEMENT LEARNING As our objective is generating label-invariant augmentations at most times, the ideal augmentation model g should assign low transformation probabilities to graph elements corresponding to label- related information. For instance, when DropNode is used, if the dropping of some nodes will damage important graph substructures and cause label changing, the model g should assign very low dropping probabilities to these nodes. However, we cannot directly make the model learn to produce label-invariant augmentations through supervised training because we do not have ground truth labels denoting which graph elements are important and should not be modiﬁed. To tackle this issue, we implicitly optimize the model with a reinforcement learning based training method. We formulate the sequential graph augmentations as a Markov Decision Process (MDP). This is intuitive and reasonable, because the Markov property is naturally satisﬁed, i.e., the output graph at any transformation step is only dependent on the input graph, not on previously performed transfor- mation. Speciﬁcally, at the t-th augmentation step, we deﬁne Gt−1, the graph obtained from the last step, as the current state, and the process of augmenting Gt−1 to Gt is deﬁned as state transition. The action is deﬁned as the augmentation transformation at generated from the model g, which includes the augmentation category ct and the exact transformations performed on all elements of Gt−1. The probability p(at) of taking action at for different ct is is described as below. 5Published as a conference paper at ICLR 2023 • If ct is MaskNF, then the transformation probability is the product of masking or unmasking probabilities for features of all nodes in Vt−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ v∈Vt−1 d∏ k=1 ( pM t,v[k] )oM t,v[k] ( 1 −pM t,v[k] )1−oM t,v[k] . (4) • If ct is DropNode, then the transformation probability is the product of dropping or non-dropping probabilities for all nodes in Vt−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ v∈Vt−1 ( pD t,v )oD t,v ( 1 −pD t,v )1−oD t,v . (5) • If ct is PerturbEdge, then the transformation probability is the product of perturbing or non- perturbing probabilities for all edges in Et−1 and Et−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ (u,v)∈Et−1∪Et−1 ( pP t,(u,v) )oP t,(u,v) ( 1 −pP t,(u,v) )1−oP t,(u,v) . (6) We use the predicted label-invariance probabilities from a reward generation modelsas the feedback reward signal in the above reinforcement learning environment. We use graph matching network (Li et al., 2019) as the backbone of the reward generation model s (see Appendix C for detailed in- troduction). When the sequential augmentation process starting from the graph G0 ends, s takes (G0,GT) as inputs and outputs s(G0,GT), which denotes the probability that the label is invariant after mapping the graph G0 to the graph GT. We use the logarithm of the predicted label-invariance probability, i.e., RT = logs(G0,GT), as the return of the sequential augmentation process. Then the augmentation model gis optimized by the REINFORCE algorithm (Sutton et al., 2000), which updates the model by the policy gradient ˆgθ computed as ˆgθ = RT∇θ ∑T t=1 log p(at), where θ denotes the trainable parameters of g. Prior to training the augmentation model g, we ﬁrst train the reward generation model on manually sampled graph pairs from the training dataset. Speciﬁcally, a graph pair (G1,G2) is ﬁrst sampled from the dataset and passed into the reward generation model to predict the probability that G1 and G2 have the same label. Afterwards, the model is optimized by minimizing the binary cross entropy loss. During the training of the augmentation model g, the reward generation model is only used to generate rewards, so its parameters are ﬁxed. See Algorithm 2 and 3 in Appendix B for the detailed training algorithm of reward generation model and augmentation model. 3.5 D ISCUSSIONS AND RELATIONS WITH PRIOR METHODS Advantages of our method. Our method explicitly estimates the transformation probability of each graph element by the automated augmentation model, thereby eliminating the negative effect of adopting a uniform transformation probability. Also, the reinforcement learning based training method can effectively help the model detect critical label-related information in the input graph, so the model can avoid damaging it and produce label-invariant augmentations with greater chances. We will show these advantages through extensive empirical studies in Section 4.1 and 4.2. Besides, the use of sequential augmentation, i.e., multiple steps of augmentation, can naturally help produce more diverse augmentations and samples, and the downstream classiﬁcation model can beneﬁt from diverse training samples. We will demonstrate it through ablation studies in Section 4.3. Relations with prior automated graph augmentations. Several automated graph augmentation methods (You et al., 2021; Yin et al., 2022; Suresh et al., 2021; Hassani & Khasahmadi, 2022) have been proposed to generate multiple graph views for contrastive learning based pre-training. However, their augmentation models are optimized by contrastive learning objectives, which are not related to graph labels. Hence, their augmentation methods may still damage label-related infor- mation, and we experimentally show that they do not perform as well as GraphAug in supervised learning scenarios in Section 4.2. Though a recent study (Trivedi et al., 2022) claims that label- invariance is also important in contrastive learning, to our best knowledge, no automated graph augmentations have been proposed to preserve label-invariance in contrastive learning. Besides, we notice that a very recent study (Yue et al., 2022) also proposes a label-invariant automated augmenta- tion method named GLA for semi-supervised graph classiﬁcation. However, GLA is fundamentally 6Published as a conference paper at ICLR 2023 Table 1: The testing accuracy on the COLORS and TRIANGLES datasets with the GIN model. We report the average accuracy and standard deviation over ten runs on ﬁxed train/validation/test splits. Dataset No augmentation Uniform MaskNF Uniform DropNode Uniform PerturbEdge Uniform Mixture GraphAug COLORS 0.578±0.012 0.507±0.014 0.547±0.012 0.618±0.014 0.560±0.016 0.633±0.009 TRIANGLES0.506±0.006 0.509±0.020 0.473±0.006 0.303±0.010 0.467±0.007 0.513±0.006 Figure 2: The changing curves of average rewards and label-invariance ratios on the validation set of the COLORS and TRIANGLES datasets as the augmentation model training proceeds. The results are averaged over ten runs, and the shadow shows the standard deviation. different from GraphAug. For a graph data sample, GLA ﬁrst obtains its graph-level representation by a GNN encoder. Then the augmentations are performed by perturbing the representation vector and label-invariant representations are selected by an auxiliary classiﬁcation model. However, our GraphAug directly augments the graph data samples, and label-invariance is ensured by our pro- posed training method based on reinforcement learning. Hence, GraphAug can generate new data samples to enrich the existing training dataset while GLA cannot achieve it. Due to the space limitation, we will discuss computational cost, augmentation step number, pre- training reward generation models, limitations, and relation with more prior methods in Appendix D. 4 E XPERIMENTS In this section, we evaluate the proposed GraphAug method on two synthetic graph datasets and seven benchmark datasets. We show that in various graph classiﬁcation tasks, GraphAug can consis- tently outperform previous graph augmentation methods. In addition, we conduct extensive ablation studies to evaluate the contributions of some components in GraphAug. 4.1 E XPERIMENTS ON SYNTHETIC GRAPH DATASETS Data. We ﬁrst show that our method can indeed produce label-invariant augmentations and outper- form uniform transformations through experiments on two synthetic graph datasets COLORS and TRIANGLES, which are synthesized by running the open sourced data synthesis code1 of Knyazev et al. (2019). The task of COLORS dataset is classifying graphs by the number of green nodes, and the color of a node is speciﬁed by its node feature. The task of TRIANGLES dataset is classifying graphs by the number of triangles (three-node cycles). We use ﬁxed train/validation/test splits for experiments on both datasets. See more information about these two datasets in Appendix E.1. Setup. We ﬁrst train the reward generation model until it converges, then train the automated aug- mentation model. To check whether our augmentation model can learn to produce label-invariant augmentations, at different training iterations, we calculate the average rewards and the label- invariance ratio achieved after augmenting graphs in the validation set. Note that since we ex- plicitly know how to obtain the labels of graphs from data generation codes, we can calculate label- invariance ratio, i.e., the ratio of augmented graphs that preserve their labels. To compare GraphAug with other augmentation methods, we train a GIN (Xu et al., 2019) based classiﬁcation model with different augmentations for ten times, and report the averaged testing classiﬁcation accuracy. We compare our GraphAug method with not using any data augmentations, and four graph augmenta- tion baseline methods. Speciﬁcally, the augmentation methods using uniform MaskNF, DropNode, and PerturbEdge transformations, and a mixture of these three uniform transformations (Uniform Mixture), i.e., randomly picking one to augment graphs at each time, are used as baselines. To en- 1https://github.com/bknyaz/graph_attention_pool 7Published as a conference paper at ICLR 2023 Table 2: The performance on seven benchmark datasets with the GIN model. We report the average ROC-AUC and standard deviation over ten runs for the ogbg-molhiv dataset, and the average ac- curacy and standard deviations over three 10-fold cross-validation runs for the other datasets. Note that for JOAOv2, AD-GCL, and AutoGCL, we evaluate the augmentation methods of them under the supervised learning setting, so the numbers here are different from those in their papers. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv No augmentation0.704±0.004 0.731±0.004 0.806±0.003 0.827±0.013 0.794±0.003 0.804±0.003 0.756±0.014 Uniform MaskNF0.702±0.008 0.720±0.006 0.815±0.002 0.788±0.012 0.777±0.006 0.794±0.002 0.741±0.010Uniform DropNode0.707±0.004 0.728±0.006 0.815±0.004 0.787±0.003 0.777±0.002 0.787±0.003 0.717±0.011Uniform PerturbEdge0.668±0.006 0.728±0.007 0.816±0.003 0.764±0.008 0.555±0.014 0.545±0.006 0.755±0.013Uniform Mixture0.707±0.004 0.730±0.009 0.815±0.003 0.779±0.014 0.776±0.006 0.783±0.003 0.746±0.010 DropEdge 0.707±0.002 0.733±0.012 0.812±0.003 0.779±0.005 0.762±0.007 0.780±0.002 0.762±0.010M-Mixup 0.706±0.003 0.736±0.004 0.811±0.005 0.798±0.015 0.788±0.005 0.803±0.003 0.753±0.013G-Mixup 0.715±0.006 0.748±0.004 0.811±0.009 0.805±0.020 0.654±0.043 0.686±0.037 0.771±0.005FLAG 0.709±0.007 0.747±0.008 0.803±0.006 0.835±0.015 0.804±0.002 0.804±0.002 0.765±0.011 JOAOv2 0.700±0.003 0.707±0.008 0.688±0.003 0.775±0.016 0.675±0.003 0.670±0.006 0.744±0.014AD-GCL 0.699±0.008 0.712±0.008 0.670±0.008 0.837±0.010 0.634±0.003 0.641±0.004 0.762±0.013AutoGCL 0.684±0.008 0.707±0.007 0.745±0.002 0.783±0.022 0.705±0.003 0.737±0.002 0.704±0.016 GraphAug 0.722±0.004 0.762±0.004 0.829±0.002 0.853±0.008 0.811±0.002 0.816±0.001 0.774±0.010 sure fair comparison, we use the same hyper-parameter setting in training classiﬁcation models for all methods. See hyper-parameters and more experimental details in Appendix E.1. Results. The changing curves of average rewards and label-invariance ratios are visualized in Fig- ure 2. These curves show that as the training proceeds, our model can gradually learn to obtain higher rewards and produce augmentations leading to higher label-invariance ratio. In other words, they demonstrate that our augmentation model can indeed learn to produce label-invariant augmentations after training. The testing accuracy of all methods on two datasets are presented in Table 1. From the results, we can clearly ﬁnd using some uniform transformations that do not satisfy label-invariance, such as uniform MaskNF on the COLORS dataset, achieve much worse performance than not us- ing augmentations. However, using augmentations produced by the trained GraphAug models can consistently achieve the best performance, which demonstrates the signiﬁcance of label-invariant augmentations to improving the performance of graph classiﬁcation models. We further study the training stability and generalization ability of GraphAug models, conduct an exploration experiment about training GraphAug models with adversarial learning, compare with some manually designed label-invariant augmentations, and compare label-invariance ratios with baseline methods on the COLORS and TRIANGLES datasets. See Appendix F.1, F.2, F.3, and F.6 for details. 4.2 E XPERIMENTS ON GRAPH BENCHMARK DATASETS Data. We further demonstrate the advantages of our GraphAug method over previous graph aug- mentation methods on six widely used datasets from the TUDatasets benchmark (Morris et al., 2020), including MUTAG, NCI109, NCI1, PROTEINS, IMDB-BINARY , and COLLAB. We also conduct experiments on the ogbg-molhiv dataset, which is a large molecular graph dataset from the OGB benchmark (Hu et al., 2020). See more information about datasets in Appendix E.2. Setup. We evaluate the performance by testing accuracy for the six datasets of the TUDatasets benchmark, and use testing ROC-AUC for the ogbg-molhiv dataset. We use two classiﬁcation mod- els, including GIN (Xu et al., 2019) and GCN (Kipf & Welling, 2017). We use the 10-fold cross- validation scheme with train/validation/test splitting ratios of 80%/10%/10% on the datasets from the TU-Datasets benchmark, and report the averaged testing accuracy over three different runs. For the ogbg-molhiv dataset, we use the ofﬁcial train/validation/test splits and report the averaged testing ROC-AUC over ten runs. In addition to the baselines in Section 4.1, we also compare with previ- ous graph augmentation methods, including DropEdge (Rong et al., 2020), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al., 2022). Besides, we compare with three automated augmentations proposed for graph self-supervised learning, including JOAOv2 (You et al., 2021), AD-GCL (Suresh et al., 2021), and AutoGCL (Yin et al., 2022). Note that we take their trained augmentation modules as the data augmenter, and evaluate the performance of supervised classiﬁcation models trained on the samples produced by these data augmenters. For fair compar- ison, we use the same hyper-parameter setting in training classiﬁcation models for GraphAug and baseline methods. See hyper-parameters and more experimental details in Appendix E.2. 8Published as a conference paper at ICLR 2023 Table 3: Results of ablation studies about learnable and sequential augmentation. We report the av- erage accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug w/o learnable graph transformation 0.696±0.006 0.724 ±0.003 0.760 ±0.003 GraphAug w/o learnable category selection 0.702±0.004 0.746 ±0.009 0.796 ±0.006 GraphAug w/o sequential augmentation 0.712±0.002 0.753 ±0.003 0.809 ±0.004 GraphAug 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Results. The performance of different methods on all seven datasets with the GIN model is sum- marized in Table 2, and see Table 9 in Appendix F.4 for the results of the GCN model. According to the results, our GraphAug method can achieve the best performance among all graph augmenta- tion methods over seven datasets. Similar to the results in Table 1, for molecule datasets including MUTAG, NCI109, NCI1, and ogbg-molhiv, using some uniform transformations based augmenta- tion methods dramatically degrades the classiﬁcation accuracy. On the other hand, our GraphAug method consistently outperforms baseline methods, such as mixup methods and existing automated data augmentations in graph self-supervised learning. The success on graph benchmark datasets once again validates the effectiveness of our proposed GraphAug method. 4.3 A BLATION STUDIES In addition to demonstrating the effectiveness of GraphAug, we conduct a series of ablation experi- ments and use empirical results to answer (1) why we make augmentation automated and learnable, (2) why we use sequential, multi-step augmentation, (3) why we adopt a combination of three differ- ent transformations (MaskNF, DropNode, PerturbEdge) instead of using only one, and (4) why we use virtual nodes. We present the ablation studies (1) and (2) in this section and leave (3) and (4) in Appendix F.5. For all ablation studies, we train GIN based classiﬁcation models on the PROTEINS, IMDB-BINARY , and NCI1 datasets, and use the same evaluation pipeline as Section 4.2. Ablation on learnable graph transformation and category selection. We ﬁrst show that making the model learn to generate graph transformations for each graph element and select augmenta- tion category are both important. We compare with a variant of GraphAug that does not learn graph transformations but simply adopts uniform transformations, and another variant that randomly select the category of graph transformation, instead of explicitly predicting it. The classiﬁcation accuracy on three datasets of these two variants are presented in the ﬁrst two rows of Table 3. Results show that the performance of two variants is worse, and particularly removing learnable graph transfor- mation will signiﬁcantly degrade the performance. It is demonstrated that learning to generate graph transformations and select augmentation category are both key success factors of GraphAug. Ablation on sequential augmentation. We next show the advantage of sequential augmentation over one-step augmentation. We compare with the variant of GraphAug that performs only one step of augmentation, i.e., with the augmentation step number T=1, and present its performance in the third row of Table 3. It is clear that using one step of augmentation will result in worse performance over all datasets. We think this demonstrates that the downstream classiﬁcation model will beneﬁt from the diverse training samples generated from sequential and multi-step augmentation. 5 C ONCLUSIONS AND FUTURE WORK We propose GraphAug, the ﬁrst automated data augmentation framework for graph classiﬁcation. GraphAug considers graph augmentations as a sequential transformation process. To eliminate the negative effect of uniform transformations, GraphAug uses an automated augmentation model to generate transformations for each element in the graph. In addition, GraphAug adopts a reinforce- ment learning based training procedure, which helps the augmentation model learn to avoid damag- ing label-related information and produce label-invariant augmentations. Through extensive empiric studies, we demonstrate that GraphAug can achieve better performance than many existing graph augmentation methods on various graph classiﬁcation tasks. In the future, we would like to explore simplifying the current training procedure of GraphAug and applying GraphAug to other graph representation learning problems, such as the node classiﬁcation problem. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT We have provided the detailed algorithm pseudocodes in Appendix B and experimental setting de- tails in Appendix E for reproducing the results. The source codes of our method are included in DIG (Liu et al., 2021) library. REFERENCES Alessandro Bicciato and Andrea Torsello. GAMS: Graph augmentation with module swapping. In Proceedings of the 11th International Conference on Pattern Recognition Applications and Methods (ICPRAM 2022), pp. 249–255, 2022. Kyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder– decoder for statistical machine translation. In Proceedings of the 2014 conference on em- pirical methods in natural language processing (EMNLP) , pp. 1724–1734, Doha, Qatar, Oc- tober 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://www.aclweb.org/anthology/D14-1179. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. AutoAugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113–123, 2019. Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. SIGKDD Explor. Newsl., 24(2):61–77, dec 2022. ISSN 1931-0145. doi: 10.1145/ 3575637.3575646. URL https://doi.org/10.1145/3575637.3575646. Hongyang Gao and Shuiwang Ji. Graph U-Nets. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pp. 2083–2092. PMLR, 09–15 Jun 2019. Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh (eds.),Proceedings of the 34th International Conference on Machine Learning, volume 70 ofProceedings of Machine Learning Research, pp. 1263–1272, International Convention Centre, Sydney, Australia, 2017. Hongyu Guo and Yongyi Mao. Intrusion-free graph mixup.arXiv preprint arXiv:2110.09344, 2021. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017. Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-Mixup: Graph data augmentation for graph classiﬁcation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research , pp. 8230–8248. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/han22c.html. Kaveh Hassani and Amir Hosein Khasahmadi. Learning graph augmentations to learn graph repre- sentations. arXiv preprint arXiv:2201.09830, 2022. Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster AutoAugment: Learning augmentation strategies using backpropagation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 1–16, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58595-2. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efﬁcient learning of augmentation policy schedules. In International Conference on Machine Learning, pp. 2731–2741. PMLR, 2019. Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. 10Published as a conference paper at ICLR 2023 Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems , volume 33, pp. 22118–22133. Curran As- sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf. Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceddings of the 3rd international conference on learning representations, 2015. Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net- works. In 5th International Conference on Learning Representations, 2017. Boris Knyazev, Graham W Taylor, and Mohamed Amer. Understanding attention and generalization in graph neural networks. Advances in Neural Information Processing Systems , 32:4202–4212, 2019. Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. FLAG: Adversarial data augmentation for graph neural networks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classiﬁcation with deep con- volutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012. Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 580–595, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58542-6. Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net- works for learning the similarity of graph structured objects. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 3835–3845. PMLR, 09–15 Jun 2019. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast AutoAug- ment. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso- ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 6add07cf50424b14fdf649da87843d01-Paper.pdf. Hongyi Ling, Zhimeng Jiang, Youzhi Luo, Shuiwang Ji, and Na Zou. Learning fair graph represen- tations via automated data augmentations. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1_OGWcP1s9w. Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. Journal of Machine Learning Research, 22(240):1–9, 2021. URL http://jmlr.org/papers/v22/ 21-0343.html. Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3D molecular graphs. In International Conference on Learning Representa- tions, 2022. URL https://openreview.net/forum?id=givsRXsOt9r. Koji Maruhashi, Masaru Todoriki, Takuya Ohwa, Keisuke Goto, Yu Hasegawa, Hiroya Inakoshi, and Hirokazu Anai. Learning multi-way relations via tensor decomposition with neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. TUDataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020. URL www. graphlearning.io. 11Published as a conference paper at ICLR 2023 Joonhyung Park, Hajin Shim, and Eunho Yang. Graph Transplant: Node saliency-guided graph mixup with local structure preservation. Proceedings of the AAAI Conference on Artiﬁcial Intel- ligence, 2022. Alex Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Chris R´e. Data augmentation with Snorkel. https://www.snorkel.org/blog/tanda, 2017a. Accessed:2022-01-24. Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher R ´e. Learning to compose domain-speciﬁc transformations for data augmentation. In Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS’17, pp. 3239–3249, Red Hook, NY , USA, 2017b. Curran Associates Inc. ISBN 9781510860964. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. DropEdge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Repre- sentations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr. Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. APAC: Augmented pattern classiﬁcation with neural networks. arXiv preprint arXiv:1505.03229, 2015. Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation mod- els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 86–96, Berlin, Germany, Au- gust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL https://aclanthology.org/P16-1009. P.Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings., pp. 958–963, 2003. doi: 10.1109/ICDAR.2003.1227801. Junwei Sun, Bai Wang, and Bin Wu. Automated graph representation learning for node classiﬁca- tion. In 2021 International Joint Conference on Neural Networks (IJCNN) , pp. 1–7, 2021. doi: 10.1109/IJCNN52387.2021.9533811. Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to im- prove graph contrastive learning. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https: //openreview.net/forum?id=ioyq7NsR1KJ. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural informa- tion processing systems, pp. 1057–1063, 2000. Luke Taylor and Geoff Nitschke. Improving deep learning with generic data augmentation. In 2018 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 1542–1547. IEEE, 2018. Puja Trivedi, Ekdeep Singh Lubana, Yujun Yan, Yaoqing Yang, and Danai Koutra. Augmentations in graph contrastive learning: Current methodological ﬂaws & towards better practices. In Proceed- ings of the ACM Web Conference 2022, WWW ’22, pp. 1538–1549, New York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3512200. URL https://doi.org/10.1145/3485447.3512200. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li `o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=rJXMpikCZ. Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. ComENet: Towards complete and efﬁcient message passing for 3d molecular graphs. In Alice H. Oh, Alekh Agarwal, Danielle Bel- grave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022a. URL https://openreview.net/forum?id=mCzMqeWSFJ. Limei Wang, Haoran Liu, Yi Liu, Jerry Kurtin, and Shuiwang Ji. Hierarchical protein representations via complete 3d graph networks. InInternational Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9X-hgLDLYkQ. 12Published as a conference paper at ICLR 2023 Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. GraphCrop: Subgraph cropping for graph classiﬁcation. arXiv preprint arXiv:2009.10564, 2020. Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classiﬁcation. In Proceedings of the Web Conference 2021 , WWW ’21, pp. 3663–3674, New York, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450383127. doi: 10. 1145/3442381.3449796. URL https://doi.org/10.1145/3442381.3449796. Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi, Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. Advanced graph and sequence neural net- works for molecular property prediction and drug discovery. Bioinformatics, 38(9):2579–2586, 02 2022b. ISSN 1367-4803. doi: 10.1093/bioinformatics/btac112. URL https://doi.org/ 10.1093/bioinformatics/btac112. Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A uniﬁed review. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2022. doi: 10.1109/TPAMI.2022.3170559. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations , 2019. URL https: //openreview.net/forum?id=ryGs6iA5Km. Keqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crys- tal material property prediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=pqCT3L-BU9T. Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. AutoGCL: Automated graph contrastive learning via learnable view generators. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 36(8):8892–8900, Jun. 2022. doi: 10.1609/aaai.v36i8.20871. URL https://ojs.aaai.org/index.php/AAAI/article/view/20871. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems , 33:5812–5823, 2020. Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning au- tomated. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con- ference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 12121–12132. PMLR, 18–24 Jul 2021. Shuo Yu, Huafei Huang, Minh N. Dao, and Feng Xia. Graph augmentation learning. In Companion Proceedings of the Web Conference 2022, WWW ’22, pp. 1063–1072, New York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450391306. doi: 10.1145/3487553.3524718. URL https://doi.org/10.1145/3487553.3524718. Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. XGNN: Towards Model-Level Explanations of Graph Neural Networks , pp. 430–438. Association for Computing Machinery, New York, NY , USA, 2020. ISBN 9781450379984. URL https://doi.org/10.1145/3394486. 3403085. Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12241–12252. PMLR, 18–24 Jul 2021. Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. Label-invariant augmentation for semi-supervised graph classiﬁcation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=rg_yN3HpCp. Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial AutoAugment. In Interna- tional Conference on Learning Representations , 2020. URL https://openreview.net/ forum?id=ByxdUySKvS. 13Published as a conference paper at ICLR 2023 Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. Proceedings of the AAAI Conference on Artiﬁcial In- telligence, 35(12):11015–11023, May 2021. doi: 10.1609/aaai.v35i12.17315. URL https: //ojs.aaai.org/index.php/AAAI/article/view/17315. Tong Zhao, Gang Liu, Stephan G ¨unneman, and Meng Jiang. Graph data augmentation for graph machine learning: A survey. arXiv preprint arXiv:2202.08871, 2022. Jiajun Zhou, Jie Shen, and Qi Xuan. Data Augmentation for Graph Classiﬁcation, pp. 2341–2344. Association for Computing Machinery, New York, NY , USA, 2020a. ISBN 9781450368599. URL https://doi.org/10.1145/3340531.3412086. Jiajun Zhou, Jie Shen, Shanqing Yu, Guanrong Chen, and Qi Xuan. M-Evolve: Structural-mapping- based data augmentation for graph classiﬁcation. IEEE Transactions on Network Science and Engineering, 8(1):190–200, 2020b. Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021a. URL https://openreview.net/forum?id=UuUbIYnHKO. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, WWW ’21, pp. 2069–2080, New York, NY , USA, 2021b. Association for Computing Machinery. ISBN 9781450383127. doi: 10.1145/3442381.3449802. URL https://doi.org/10.1145/ 3442381.3449802. 14Published as a conference paper at ICLR 2023 A V ISUALIZATION OF DIFFERENT AUGMENTATION METHODS (a) An illustration of a data sam- ple from the TRIANGLES dataset. Red nodes represent the nodes be- longing to triangles. The label of this data sample is 4 since there are four triangles. Training without any augmentations on the TRIAN- GLES dataset achieves the average testing accuracy of 0.506 ±0.006. (b) The data sample generated by augmenting the data sample in (a) with the uniform DropNode transformation. Note that two nodes originally belonging to tri- angles are removed, and the label is changed to 1. Training with the uniform DropNode transformation achieves the average testing accu- racy of 0.473 ± 0.006. (c) The data sample generated by augmenting the data sample in (a) with the label-invariant DropN- ode transformation (the DropNode with GT method in Appendix F.3), which intentionally avoids drop- ping nodes in triangles. Training with this label-invariant augmen- tation achieves the average testing accuracy of 0.522 ± 0.007. Figure 3: Comparison of different augmentation methods on the TRIANGLES dataset. We use a GIN (Xu et al., 2019) based classiﬁcation model to evaluate different augmentation methods, and report the average accuracy and standard deviation over ten runs on a ﬁxed train/validation/test split. In (a), we show a graph data sample with 4 triangles. In (b) and (c), we the data samples generated by augmenting the data sample in (a) with two different augmentation methods. We can clearly ﬁnd that using the uniform DropNode transformation degrades the classiﬁcation performance but using the label-invariant augmentation improves the performance. 15Published as a conference paper at ICLR 2023 B A UGMENTATION AND TRAINING ALGORITHMS Algorithm 1: Augmentation Algorithm of GraphAug 1: Input: Graph G0 = (V0,E0,X0); total number of augmentation steps T; the augmentation model gcomposed of GNN-encoder, GRU, and four MLP models MLPC, MLPM, MLPD, MLPP 2: Initialize the hidden state q0 of the GRU model to zero vector 3: for t= 1to T do 4: Obtain G′ t−1 by adding a virtual node to Gt−1 5: evirtual t−1 ,{ev t−1 : v∈Vt−1}= GNN-encoder(G′ t−1) 6: qt = GRU(qt−1,evirtual t−1 ) 7: pC t = MLPC(qt) 8: Sample ct from the categorical distribution of pC t 9: if ct is MaskNF then 10: for v∈Vt−1 do 11: pM t,v = MLPM(ev t−1) 12: Sample oM t,v from the Bernoulli distribution parameterized with pM t,v 13: for k= 1to ddo 14: Set the k-th node feature of vto zero if oM t,v[k] = 1 15: else if ct is DropNode then 16: for v∈Vt−1 do 17: pD t,v = MLPD(ev t−1) 18: Sample oD t,v from the Bernoulli distribution parameterized with pD t,v 19: Drop the node vfrom Vt−1 if oD t,v = 1 20: else if ct is PerturbEdge then 21: Obtain the addable edge set Et−1 by randomly sampling at most |Et−1|addable edges from {(u,v) :u,v ∈Vt−1,(u,v) /∈Et−1} 22: for (u,v) ∈Et−1 do 23: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,1] ) 24: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 25: Drop (u,v) from Et−1 if oP t,(u,v) = 1 26: for (u,v) ∈Et−1 do 27: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,0] ) 28: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 29: Add (u,v) into Et−1 if oP t,(u,v) = 1 30: Set Gt as the outputted graph from the t-th augmentation step 31: Output GT 16Published as a conference paper at ICLR 2023 Algorithm 2: Training Algorithm of the reward generation model of GraphAug 1: Input: Graph dataset D; batch size B; learning rate α; the reward generation model swith the parameter ϕ 2: repeat 3: Sample a batch Gof Bdata samples from D 4: L= 0 5: for G∈G do 6: Randomly sample a graph G+ with the same label as Gfrom Gand a graph G−with different label as G 7: L= L−log s(G,G+) −log(1 −s(G,G−)) 8: Update the parameter ϕof sas ϕ= ϕ−α∇ϕL/B 9: until the training converges 10: Output the trained reward generation model s 17Published as a conference paper at ICLR 2023 Algorithm 3: Training Algorithm of the augmentation model of GraphAug 1: Input: Graph dataset D; batch size B; learning rate α; total number of augmentation steps T; the augmentation model gwith the parameter θcomposed of GNN-encoder, GRU, and four MLP models MLPC, MLPM, MLPD, MLPP; the trained reward generation model s 2: repeat 3: Sample a batch Gof Bdata samples from D 4: ˆgθ = 0 5: for G∈G do 6: Set G0 = (V0,E0,X0) as G 7: Initialize the hidden state q0 of the GRU model to zero vector 8: for t= 1to T do 9: Obtain G′ t−1 by adding a virtual node to Gt−1 10: evirtual t−1 ,{ev t−1 : v∈Vt−1}= GNN-encoder(G′ t−1) 11: qt = GRU(qt−1,evirtual t−1 ) 12: pC t = MLPC(qt) 13: Sample ct from the categorical distribution of pC t , set log p(at) = logpC t (ct) 14: if ct is MaskNF then 15: for v∈Vt−1 do 16: pM t,v = MLPM(ev t−1) 17: Sample oM t,v from the Bernoulli distribution parameterized with pM t,v 18: for k= 1to ddo 19: log p(at) = logp(at) +oM t,v[k] logpM t,v[k] + (1−oM t,v[k]) log(1−pM t,v[k]) 20: Set the k-th node feature of vto zero if oM t,v[k] = 1 21: else if ct is DropNode then 22: for v∈Vt−1 do 23: pD t,v = MLPD(ev t−1) 24: Sample oD t,v from the Bernoulli distribution parameterized with pD t,v 25: log p(at) = logp(at) +oD t,vlog pD t,v + (1−oD t,v) log(1−pD t,v) 26: Drop the node vfrom Vt−1 if oD t,v = 1 27: else if ct is PerturbEdge then 28: Obtain the addable edge set Et−1 by randomly sampling at most |Et−1|addable edges from {(u,v) :u,v ∈Vt−1,(u,v) /∈Et−1} 29: for (u,v) ∈Et−1 do 30: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,1] ) 31: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 32: log p(at) = logp(at) +oP t,(u,v) log pP t,(u,v) + (1−oP t,(u,v)) log(1−pP t,(u,v)) 33: Drop (u,v) from Et−1 if oP t,(u,v) = 1 34: for (u,v) ∈Et−1 do 35: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,0] ) 36: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 37: log p(at) = logp(at) +oP t,(u,v) log pP t,(u,v) + (1−oP t,(u,v)) log(1−pP t,(u,v)) 38: Add (u,v) into Et−1 if oP t,(u,v) = 1 39: Set Gt as the outputted graph from the t-th augmentation step 40: ˆgθ = ˆgθ + logs(G0,GT)∇θ ∑T t=1 log p(at) 41: Update the parameter θof gas θ= θ+ αˆgθ/B. 42: until the training converges 43: Output the trained augmentation model g 18Published as a conference paper at ICLR 2023 C D ETAILS OF REWARD GENERATION MODEL We use the graph matching network (Li et al., 2019) as the reward generation modelsto predict the probability s(G0,GT) that G0 and GT have the same label (here G0 is a graph sampled from the dataset, i.e., the starting graph of the sequential augmentation process, andGT is the graph produced from T steps of augmentation by the augmentation model). The graph matching network takes both G0 = (V0,E0,X0) and GT = (VT,ET,XT) as input, performs multiple message operations on them with a shared GNN model separately. The computational process of the message passing for any node vin G0 at the ℓ-th layer of the model is hℓ v = UPDATE ( hℓ−1 v ,AGG ({ mℓ jv : j ∈N(v) }) ,µGT v ) , (7) which is the same as the message passing of vanilla GNNs in Equation (1) other than involving propagating the messageµGT v from the graph GT to the node vin G0. The message µGT v is extracted by an attention based module as wiv = exp ( sim ( hℓ−1 v ,hℓ−1 i )) ∑ u∈VT exp ( sim ( hℓ−1v ,hℓ−1u )),µGT v = ∑ i∈VT wiv(hℓ−1 v −hℓ−1 i ),v ∈V0, (8) where sim(·,·) computes the similarity between two vectors by dot-product. The message passing for any node inGT is similarly computed as in Equation (7), and this also involves propagating mes- sage from G0 to nodes in GT with the attention module in Equation (8). Afterwards, the graph-level representations hG0 and hGT of G0 and GT are separately obtained from their node embeddings as in Equation (2). We pass |hG0 −hGT |, the element-wise absolute deviation of hG0 and hGT , to an MLP model to compute s(G0,GT). 19Published as a conference paper at ICLR 2023 D M ORE DISCUSSIONS Discussions about computational cost. Considering a graph with|V|nodes and |E|edges, the time complexity of performing our deﬁned DropNode or MaskNF transformation on it isO(|V|), and the time complexity is O(|E|) for the PerturbEdge transformation since both edge dropping or addition is operated on at most |E|edges. Hence, the time complexity of augmenting the graph forT steps is O(T|V|+ T|E|). This time cost is affordable for most real-world applications. We test the average time used to augment a graph on each benchmark dataset used in our experiments with our trained augmentation model, see Table 10 for time results. We can ﬁnd that for most dataset, our method only takes a very small amount of time ( < 0.05 s) to augment a graph in average. Besides, during the training of the augmentation model, the computation of rewards by the reward generation model involves attention module (see Equation (9)), which causes an extra computational cost ofO(|V|2). In practice, this does not have much effect on small graphs, but may lead to large computation and memory cost on large graphs. Discussions about augmentation step number. For the number of augmentation steps T, we do not let the model to learn or decide T itself but make T a ﬁxed hyper-parameter to avoid the model being stucked in the naive solution of not doing augmentation at all (i.e., learnT = 0). This strategy is also adopted by previous image augmentation method (e.g. AutoAugment (Cubuk et al., 2019)). A larger T encourages the model to produce more diverse augmentations but makes it harder to keep label-invariance. We experimentally ﬁnd that if T ≥8, it is hard to obtain sufﬁciently high reward for the model. Hence, we tune T in [1,8] for each dataset to achieve the best trade-off between producing diverse augmentations and keeping label-invariance. Discussions about pre-training reward generation model. In our method, before training the augmentation model, we ﬁrst pre-train the reward generation model and make it ﬁxed while training the augmentation model. Such a training pipeline has both advantages and disadvantages. The ad- vantages of using the ﬁxed/pre-trained reward generation model are two-fold. (1) First, pre-training the reward generation model enables it to accurately predict whether two input graphs have the same labels or not, so that the generated reward signals can provide accurate feedback for the augmen- tation model. (2) Second, using the ﬁxed reward generation model can stabilize the training of the augmentation model in practice. As we shown in Appendix F.2, if the reward generation model is not ﬁxed and jointly trained with the augmentation model, the training becomes unstable and mod- els consistently diverge. The disadvantage of pre-training the reward generation model is that this training pipeline is time-consuming, because we have to train two models every time to obtain the ﬁnally usable graph augmenter. Limitations of our method. There are some limitations in our method. (1) First, our method adopts a complicated two-step training pipeline which ﬁrst trains the reward generation model and then trains the augmentation model. We have tried simplifying it to one-step training through adversarial training as in Ratner et al. (2017b). However, we found it to be very unstable and the augmenta- tion model consistently diverges (see Appendix F.2 for an exploration experiment about adversarial training on the COLORS and TRIANGLES dataset). We leave the problem of simplifying the train- ing to the future. (2) Second, our augmentation method will take extra computational cost in both training the augmentation model and providing augmented samples for the downstream graph clas- siﬁcation training. The time and resource cost of training models can be large on the large datasets. For instance, on the ogbg-molhiv dataset, we ﬁnd it takes the total time of around 10 hours to train the reward generation model and augmentation model before we obtain a ﬁnally usable graph aug- menter. Given that the performance improvement is not signiﬁcant on the ogbg-molhiv dataset, such a large time cost is not a worthwhile investment. Our GraphAug mainly targets on improving the graph classiﬁcation performance by generating more training data samples for the tasks with small datasets, particularly for those that need huge cost to manually collect and label new data samples. But for the classiﬁcation task with sufﬁcient training data, the beneﬁts of using GraphAug are limited and not worth the large time and resource cost to train GraphAug models. Relations with automated image augmentations. GraphAug are somehow similar to some auto- mated image augmentations (Cubuk et al., 2019; Zhang et al., 2020) in that they both use sequential augmentation and reinforcement learning based training. However, they are actually fundamen- tally different. Label-invariance is not a problem in automated image augmentations because the used image transformations ensure label-invariance. On the other hand, as discussed in Section 3.2, 20Published as a conference paper at ICLR 2023 it is non-trivial to make graph transformations ensure label-invariance. In GraphAug, the learn- able graph transformation model and the reinforcement learning based training are used to produce label-invariant augmentations, which are actually the main contribution of GraphAug. Another fundamental difference between GraphAug and automated image augmentations lies in the reward design. Many automated image augmentation methods, such as AutoAugment (Cubuk et al., 2019), train a child network model on the training data and use the achieved classiﬁcation accuracy on the validation data as the reward. Instead, our GraphAug uses the label-invariance probability predicted by the reward generation model as the reward signal to train the augmentation model. We argue that such reward design has several advantages over using the classiﬁcation accuracy as the reward. (1) First, maximizing the label-invariance probability can directly encourage the augmentation model to produce label-invariant augmentations. However, the classiﬁcation accuracy is not directly related to label-invariance, so using it as the reward feedback does not necessarily make the augmentation model learn to ensure label-invariance. (2) Second, predicting the label-invariance probability only needs one simple model inference process that is computationally cheap, while obtaining the clas- siﬁcation accuracy is computationally expensive because it needs to train a model from scratch. (3) Most importantly, our reward generation scheme facilitates the learning of the augmentation model by providing the reward feedback for every individual graph . Even in the same dataset, the label-related structures or patterns in different graphs may vary a lot, hence, good augmentation strategies for different graphs can be different. However, the classiﬁcation accuracy evaluates the classiﬁcation performance when using the produced graph augmentations to train models on the overall dataset, which does not provide any feedback about whether the produced augmentation on every individual graph sample is good or not. Differently, the label-invariance probability is com- puted for every individual graph sample, thereby enabling the model to capture good augmentation strategies for every individual graph. Considering these advantages, we do not use the classiﬁcation accuracy but take the label-invariance probability predicted by the reward generation model as the reward. Overall, GraphAug cannot be considered as a simple extension of automated image augmentations to graphs. Relation with prior graph augmentation methods. In addition to GLA (Yue et al., 2022), we also notice Graphair (Ling et al., 2023), another recently proposed automated graph augmentation method. However, Graphair aims to produce fairness-aware graphs for fair graph representation learning, while our method is proposed for graph classiﬁcation. Additionally, graph mixup methods (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) synthesize a new graph or graph representation from two input graphs. Because the new data sample is assigned with the combination of labels of two input graphs, mixup operations are supposed to detect and mix the label-related information of two graphs (Guo & Mao, 2021). However, our method is simpler and more intuitive because it only needs to detect and preserve the label-related information of one in- put graph. In addition, another method FLAG (Kong et al., 2022) can only augment node features, while our method can produce augmentations in node features, nodes and edges. Besides, similar to the motivation of our GraphAug, some other studies have also found that preserving important structures or node features is signiﬁcant in designing effective graph augmentations. A pioneering method in this direction is GCA (Zhu et al., 2021b), which proposes to identify important edges and node features in the graph by node centralities. GCA augments the graph by random edge dropping and node feature masking, but assigns lower perturbation probabilities to the identiﬁed important edges and node features. Also, other studies (Wang et al., 2020; Bicciato & Torsello, 2022; Zhou et al., 2020b) assume that some motif or subgraph structures in the graph is signiﬁcant, and propose to augment graphs by manually designed transformations to avoid removing them. Overall, these augmentations are based on some rules or assumptions about how to preserve important structures of the input graph. Differently, our GraphAug method does not aim to deﬁne a ﬁxed graph aug- mentation strategy for every graph. Instead, it seeks to make the augmentation model ﬁnd good augmentation strategies automatically with reinforcement learning based training. Relations with graph explainability. Our method is related to graph explainability in that the predicted transformation probabilities from our augmentation model g is similar to explainability scores of some graph explainability methods (Maruhashi et al., 2018; Yuan et al., 2020; 2021). Hence, we hope that our augmentation method can bring inspiration to researchers in the graph explainability area. 21Published as a conference paper at ICLR 2023 Table 4: Statistics of graph benchmark datasets. Datasets # graphs Average # nodes Average # edges # classes PROTEINS 1113 39.06 72.82 2 IMDB-BINARY 1000 19.77 96.53 2 COLLAB 5000 74.49 2457.78 3 MUTAG 188 17.93 19.79 2 NCI109 4127 29.68 32.13 2 NCI1 4110 29.87 32.30 2 ogbg-molhiv 41,127 25.5 27.5 2 Table 5: Some hyper-parameters for the reward generation model and its training. Datasets # layers batch size # training epochs PROTEINS 6 32 420 IMDB-BINARY 6 32 320 COLLAB 5 8 120 MUTAG 5 32 230 NCI109 5 32 200 NCI1 5 32 200 ogbg-molhiv 5 32 200 E M ORE DETAILS ABOUT EXPERIMENTAL SETTING E.1 E XPERIMENTS ON SYNTHETIC GRAPH DATASETS Data information. We synthesize the COLORS and TRIANGLES dataset by running the open sourced data synthesis code of Knyazev et al. (2019). For the COLORS dataset, we synthesize 8000 graphs for training, 1000 graphs for validation, and 1000 graphs for testing. For the TRIANGLES dataset, we synthesize 30000 graphs for training, 5000 graphs for validation, and 5000 graphs for testing. The labels of all data samples in both datasets belong to {1,..., 10}. Details of the model and training. The Adam optimizer (Kingma & Ba, 2015) is used for the training of all models. For both datasets, we use a reward generation model with 5 layers and the hidden size of 256, and the graph level embedding is obtained by sum pooling. It is trained for 1 epoch on the COLORS dataset and 200 epochs on the TRIANGLES dataset. The batch size is 32 and the learning rate is 0.0001. For the augmentation model, we use a GIN model with 3 layers and the hidden size of 64 for GNN encoder, an MLP model with 2 layers, the hidden size of 64, and ReLU as the non-linear activation function for MLP C, and an MLP model with 2 layers, the hidden size of 128, and ReLU as the non-linear activation function for MLPM, MLPD, and MLPP. The augmentation model is trained for 5 epochs with the batch size of 32 and the learning rate of 0.0001 on both datasets. To stabilize the training of the augmentation model, we manually control the augmentation model to only modify 5% of graph elements at each augmentation step during the training. On the COLORS dataset, we use a classiﬁcation model where the number of layers is 3, the hidden size is 128, and the readout layer is max pooling. On the TRIANGLES dataset, we use a classiﬁcation model where the number of layers is 3, the hidden size is 64, and the readout layer is sum pooling. On both datasets, we set the training batch size as 32 and the learning rate as 0.001 when training classiﬁcation models, and all classiﬁcation models are trained for 100 epochs. E.2 E XPERIMENTS ON GRAPH BENCHMARK DATASETS Data information. We use six datasets from the TUDatasets benchmark (Morris et al., 2020), in- cluding three molecule datasets MUTAG, NCI109, NCI1, one bioinformatics dataset PROTEINS, and two social network datasets IMDB-BINARY and COLLAB. We also use the ogbg-molhiv 22Published as a conference paper at ICLR 2023 Table 6: Some hyper-parameters for the augmentation model and its training. Datasets # augmentation steps T batch size # training epochs PROTEINS 2 32 30 IMDB-BINARY 8 32 30 COLLAB 8 32 10 MUTAG 4 16 200 NCI109 2 32 20 NCI1 2 32 20 ogbg-molhiv 2 128 10 Table 7: Some hyper-parameters for the classiﬁcation model and its training. Datasets # layers hidden size batch size PROTEINS 3 128 32 IMDB-BINARY 4 128 32 COLLAB 4 64 32 MUTAG 4 128 16 NCI109 4 128 32 NCI1 3 128 32 ogbg-molhiv 5 300 32 dataset from the OGB benchmark (Hu et al., 2020). See Table 4 for the detailed statistics of all benchmark datasets used in our experiments. Details of model and training. The Adam optimizer (Kingma & Ba, 2015) is used for training of all models. For all six datasets, we set the hidden size as 256 and the readout layer as sum pooling for the reward generation model, and the reward generation model is trained using 0.0001 as the learning rate. See other hyper-parameters about the reward generation model and its training in Table 5. The hyper-parameters of the augmentation model is the same as those in experiments of synthetic graph datasets and the learning rate is 0.0001 during its training, but we tune the batch size, the training epochs and the number of augmentation steps T on each dataset. See Table 6 for the optimal values of them on each dataset. The strategy of modifying only 5% of graph elements is also used during the training of augmentation models. Besides, for classiﬁcation models, we set the readout layer as mean pooling, and tune the number of layers, the hidden size, and the training batch size. See Table 7 for these hyper-parameters. All classiﬁcation models are trained for 100 epochs with the learning rate of 0.001. 23Published as a conference paper at ICLR 2023 Figure 4: The changing curves of training and validation loss on the COLORS and TRIANGLES datasets when training the reward generation model of GraphAug with Algorithm 2. The results are averaged over ten runs, and the shadow shows the standard deviation. Figure 5: The changing curves of training and validation rewards on the COLORS and TRIANGLES datasets when training the augmentation model of GraphAug with Algorithm 3. The results are averaged over ten runs, and the shadow shows the standard deviation. Figure 6: The changing curves of training rewards of augmentation model and training loss of reward generation model when training two models together with adversarial learning on the COLORS dataset. The results are averaged over ten runs, and the shadow shows the standard deviation. F M ORE EXPERIMENTAL RESULTS F.1 S TUDY OF TRAINING STABILITY AND GENERALIZATION Taking the COLORS and TRIANGLES datasets as examples, we show the learning curves of reward generation models and augmentation models in Figure 4 and Figure 5, respectively. The learning curves on the training set show that the training is generally very stable for both reward generation models and augmentation models since no sharp oscillation happens. Comparing the learning curves on the training and validation set, we can ﬁnd that on the COLORS dataset, the curves converge to around the same loss and rewards on the training and validation set when the training converges. Hence, reward generation model and the augmentation model both have very good generalization abilities. Differently, on more complicated TRIANGLES dataset, slight overﬁtting exists for both models but the overall generalization ability is still acceptable. Actually, to eliminatee the negative effect of overﬁtting, we always take the reward generation model with lowest validation loss and the augmentation model with highest validation reward in our experiments. In a word, our studies about training stability and generalization show that both the reward generation model and augmentation model can be trained stably and have acceptable generalization ability. 24Published as a conference paper at ICLR 2023 Table 8: The testing accuracy on the COLORS and TRIANGLES datasets with the GIN model. We report the average accuracy and standard deviation over ten runs on ﬁxed train/validation/test splits. Dataset No augmentation MaskNF with GT DropNode with GT PerturbEdge with GT GraphAug COLORS 0.578±0.012 0.627±0.013 0.627 ±0.017 n/a 0.633±0.009 TRIANGLES 0.506±0.006 n/a 0.522 ±0.007 0.524 ±0.006 0.513±0.006 F.2 A DVERSARIAL TRAINING EXPERIMENT One possible one-stage training alternative of GraphAug is the adversarial training strategy in Ratner et al. (2017b). Speciﬁcally, the augmentation model is trained jointly with the reward generation model. We construct the positive graph pair (G,G+) sampled from the dataset in which G and G+ have the same label, and use the augmentation model to augment the graph Gto G−and form the negative graph pair (G,G−). The reward generation model is then trained to minimize the loss function L= −log s(G,G+) −log (1−s(G,G−)), but the augmentation model is trained to maximize the reward log s(G,G−) received from the reward generation model. In this adversarial training method, the reward generation model can actually be considered as the discriminator model. We conduct an exploration experiment of it on the COLORS and TRIANGLES datasets, but we ﬁnd that this strategy cannot work well on both datasets. On the TRIANGLES dataset, gradient explosion consistently happens during the training but we have not yet ﬁgure out how to ﬁx it. On the COLORS dataset, we show the learning curves of two models in Figure 6. Note that different from the learning curves of GraphAug in Figure 2, the augmentation model diverges and fails to learn to obtain more rewards as the training proceeds. In other words, the augmentation model struggles to learn to generate new graphs that can deceive the reward generation model. Given these existing problems in adversarial learning, we adopts the two-stage training pipeline in GraphAug and leaves the problem of simplifying the training to the future. F.3 C OMPARISON WITH MANUALLY DESIGNED LABEL -INVARIANT AUGMENTATIONS An interesting question is how does our GraphAug compare with the manually designed label- invariant augmentation methods (assuming we can design them from some domain knowledge)? We try answering this question by empirical studies on COLORS and TRIANGLES datasets. Since we explicitly know how the labels of graphs are obtained from their data generation codes, we can design some label-invariant augmentation strategies. We compare GraphAug with three designed label-invariant augmentation methods, which are based on MaskNF, DropNode, and PerturbEdge transformations intentionally avoiding damaging label-related information. Speciﬁcally, for the COLORS dataset, we compare with MaskNF that uniformly masks the node features other than the color feature, and DropNode that uniformly drops the nodes other than green nodes. In other words, they are exactly using the ground truth labels indicating which graph elements are label-related in- formation, so we call them as MaskNF with GT and DropNode with GT. Note that no PerturbEdge with GT is deﬁned on the COLORS dataset because the modiﬁcation of edges naturally ensures label-invariance. Similarly, for the TRIANGLES dataset, we compare with DropNode with GT and PerturbEdge with GT which intentionally avoid damaging any nodes or edges in triangles. The performance of no augmentation baseline, three manually designed augmentation methods, and our GraphAug method is summarized in Table 8. It is not surprising that all augmentation meth- ods can outperform no augmentation baseline since they all can produce label-invariant training samples. Interestingly, GraphAug is a competitive method compared with these manually designed label-invariant methods. GraphAug outperforms manually designed augmentations on the COLORS dataset but fails to do it on the TRIANGLES dataset. We ﬁnd that is because GraphAug model se- lects MaskNF with higher chances than DropNode and PerturbEdge, but graph classiﬁcation models beneﬁts more from diverse topology structures produced by DropNode and PerturbEdge transforma- tions. Note that although our GraphAug may not show signiﬁcant advantages over manually designed label-invariant augmentations on these two synthetic datasets, in most scenarios, de- signing such label-invariant augmentations is impossible because we do not know which graph 25Published as a conference paper at ICLR 2023 Table 9: The performance on seven benchmark datasets with the GCN model. We report the aver- age ROC-AUC and standard deviation over ten runs for the ogbg-molhiv dataset, and the average accuracy and standard deviations over three 10-fold cross-validation runs for the other datasets. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv No augmentation0.711±0.003 0.734±0.010 0.797±0.002 0.803±0.016 0.742±0.004 0.731±0.002 0.761±0.010 Uniform MaskNF0.716±0.001 0.723±0.006 0.802±0.002 0.765±0.017 0.734±0.005 0.729±0.004 0.745±0.011 Uniform DropNode0.714±0.005 0.733±0.001 0.798±0.002 0.759±0.007 0.727±0.003 0.722±0.003 0.723±0.012 Uniform PerturbEdge0.694±0.003 0.732±0.010 0.795±0.003 0.744±0.004 0.634±0.006 0.638±0.011 0.746±0.013 Uniform Mixture0.714±0.003 0.734±0.009 0.797±0.004 0.754±0.015 0.731±0.002 0.722±0.002 0.743±0.011 DropEdge 0.710±0.006 0.735±0.013 0.797±0.004 0.762±0.003 0.724±0.004 0.723±0.003 0.757±0.012 M-Mixup 0.714±0.004 0.728±0.007 0.794±0.003 0.783±0.007 0.739±0.005 0.741±0.002 0.753±0.014 G-Mixup 0.724±0.006 0.749±0.010 0.800±0.027 0.799±0.004 0.509±0.005 0.506±0.005 0.763±0.008 FLAG 0.723±0.003 0.743±0.008 0.797±0.002 0.819±0.004 0.746±0.003 0.734±0.004 0.768±0.010 JOAOv2 0.722±0.003 0.687±0.010 0.681±0.004 0.736±0.007 0.691±0.007 0.672±0.004 0.722±0.009 AD-GCL 0.691±0.011 0.697±0.011 0.612±0.004 0.665±0.001 0.634±0.003 0.641±0.004 0.752±0.013 AutoGCL 0.668±0.008 0.719±0.002 0.745±0.002 0.769±0.022 0.707±0.002 0.714±0.005 0.701±0.014 GraphAug 0.736±0.007 0.764±0.008 0.808±0.001 0.832±0.005 0.760±0.003 0.748±0.002 0.774±0.010 Table 10: Average augmentation time per graph with the trained augmentation model. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv JOAOv2 0.0323s 0.0854s 0.2846s 0.0397s 0.0208s 0.0223s 0.0299s AD-GCL 0.0127s 0.0418s 0.1478s 0.0169s 0.0092s 0.0083s 0.0115s AutoGCL 0.0218s 0.0643s 0.2398s 0.0256s 0.0162s 0.0168s 0.0221s GraphAug 0.0073s 0.0339s 0.1097s 0.0136s 0.0075s 0.0078s 0.0106s elements are label-related. However, our GraphAug can still work in these scenarios because it can automatically learn to produce label-invariant augmentations. F.4 M ORE EXPERIMENTAL RESULTS ON GRAPH BENCHMARK DATASETS The performance of different augmentation methods on all seven datasets with the GCN model is presented in Table 9. Besides, to quantify and compare the computational cost of our method and some automated graph augmentation baseline methods on each dataset, we test the average time they use to augment each graph and summarize the average augmentation time results in Table 10. For most dataset, our method only takes a very small amount of time (<0.05s) to augment a graph in average, which is an acceptable time cost for most real-world applications. In addition, from Table 10, we can clearly ﬁnd that among all automated graph augmentation methods, our GraphAug takes the least average runtime to augment graphs. For the other baseline methods in Table 2, because they do not need the computation with neural networks in augmentations, their runtime is unsurprisingly lower ( < 0.001s per graph). However, the classiﬁcation performance of them is consistently worse than our GraphAug. Overall, our GraphAug achieves the best classiﬁcation performance, and its time cost is the lowest among all automated graph augmentations. F.5 M ORE ABLATION STUDIES Ablation on combining three different transformations. In our method, we use a combination of three different graph transformations, including MaskNF, DropNode, and PerturbEdge. Our GraphAug model are designed to automatically select one of them at each augmentation step. Here we explore how the performance will change if only one category of graph transformation is used. Speciﬁcally, we compare with three variants of GraphAug that only uses learnable MaskNF, DropN- ode, and PerturbEdge, whose performance are listed in the ﬁrst three rows of Table 11. We can ﬁnd that sometimes using a certain category of learnable augmentation gives very good results, e.g., learnable DropNode on the NCI1 dataset. However, not all categories can achieve it, and actually the optimal category varies among datasets because graph structure distributions or modalities are very 26Published as a conference paper at ICLR 2023 Table 11: Results of ablation studies about combining three different transformations. We report the average accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug with only learnable MaskNF transformation0.712±0.001 0.751 ±0.002 0.809 ±0.002 GraphAug with only learnable DropNode transformation0.716±0.003 0.752 ±0.005 0.814 ±0.002 GraphAug with only learnable PerturbEdge transformation0.702±0.009 0.754 ±0.005 0.780 ±0.001 GraphAug 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Table 12: Results of ablation studies about using virtual nodes. We report the average accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug with sum pooling 0.711±0.005 0.750 ±0.008 0.788 ±0.004 GraphAug with mean pooling 0.711±0.004 0.752 ±0.004 0.801 ±0.005 GraphAug with max pooling 0.713±0.002 0.737 ±0.005 0.795 ±0.005 GraphAug with virtual nodes 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Table 13: The label-invariance ratios on the test sets of COLORS and TRIANGLES datasets. Dataset Uniform MaskNF Uniform DropNode Uniform PerturbEdge Uniform Mixture GraphAug COLORS 0.3547 0.3560 1.0000 0.5645 0.9994 TRIANGLES 1.0000 0.6674 0.1957 0.6181 1.0000 different in different datasets. Nonetheless, GraphAug can consistently achieve good performance without manually searching the optimal category on different datasets. Hence, combining different transformations makes it easier for the GraphAug model to adapt to different graph datasets than using only one category of transformation. Ablation on using virtual nodes. In our method, virtual nodes are used to capture graph-level representation and predict augmentation categories due to two advantages. (1) First, in the message passing process of GNNs, virtual nodes can help propagate messages among far-away nodes in the graph. (2) Second, virtual nodes can learn to more effectively capture graph-level representations through aggregating more information from important nodes or structures in the graph (similar to the attention mechanism). In fact, many prior studies (Gilmer et al., 2017; Hu et al., 2020) have demonstrated the advantages of using virtual nodes in graph representation learning. To justify the advantages of using virtual nodes in GraphAug, we compare the performance of taking different ways to predict the augmentation category in an ablation experiment. Speciﬁcally, we evaluate the performance of GraphAug model variants in which virtual nodes are not used, but the augmentation category is predicted from the graph-level representations obtained by sum pooling, mean pooling, or max pooling. The results of them are summarized in the ﬁrst three rows of Table 12. From the results, we can ﬁnd that using virtual nodes achieves the best performance, hence it is the best option. F.6 E VALUATION OF LABEL -INVARIANCE PROPERTY We evaluate the label-invariance ratios of our GraphAug method and the baseline methods used in Table 1 on the test sets of two synthetic datasets. The results are summarized in Table 13. Since the label is deﬁned as the number of nodes with green colors (indicated by node features) in the COLORS dataset, Uniform DropNode and Uniform PerturbEdge will destroy label-related informa- tion and achieve a very low label-invariance ratio. Similarly, the label is deﬁned as the number of 3-cycles in the TRIANGLES dataset, Uniform DropNode and Uniform PerturbEdge also achieve a 27Published as a conference paper at ICLR 2023 very low label-invariance ratio. However, our GraphAug can achieve a very high label-invariance ratio of close to 1.0 on both datasets. Besides, combining with the classiﬁcation performance in Table 1, we can ﬁnd that only the augmentations with high label-invariance ratios can outperform no augmentation baseline. This phenomenon demonstrates that label-invariance is signiﬁcant to achieve effective graph augmentations. 28",
      "references": [
        "GAMS: Graph augmentation with module swapping",
        "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
        "AutoAugment: Learning augmentation strategies from data",
        "Data augmentation for deep graph learning: A survey",
        "Graph U-Nets",
        "Neural message passing for quantum chemistry",
        "Intrusion-free graph mixup",
        "Inductive representation learning on large graphs",
        "G-Mixup: Graph data augmentation for graph classiﬁcation",
        "Learning graph augmentations to learn graph representations",
        "Faster AutoAugment: Learning augmentation strategies using backpropagation",
        "Population based augmentation: Efﬁcient learning of augmentation policy schedules",
        "Long short-term memory",
        "Open graph benchmark: Datasets for machine learning on graphs",
        "Adam: A method for stochastic optimization",
        "Semi-supervised classiﬁcation with graph convolutional networks",
        "Understanding attention and generalization in graph neural networks",
        "FLAG: Adversarial data augmentation for graph neural networks",
        "ImageNet classiﬁcation with deep con- volutional neural networks",
        "Differentiable automatic data augmentation",
        "Graph matching net- works for learning the similarity of graph structured objects",
        "Fast AutoAugment",
        "Learning fair graph represen- tations via automated data augmentations",
        "DIG: A turnkey library for diving into graph deep learning research",
        "Spherical message passing for 3D molecular graphs",
        "Learning multi-way relations via tensor decomposition with neural networks",
        "TUDataset: A collection of benchmark datasets for learning with graphs",
        "Graph Transplant: Node saliency-guided graph mixup with local structure preservation",
        "Data augmentation with Snorkel",
        "Learning to compose domain-speciﬁc transformations for data augmentation",
        "DropEdge: Towards deep graph convolutional networks on node classiﬁcation",
        "APAC: Augmented pattern classiﬁcation with neural networks",
        "Improving neural machine translation mod- els with monolingual data",
        "Best practices for convolutional neural networks applied to visual document analysis",
        "Automated graph representation learning for node classiﬁcation",
        "Adversarial graph augmentation to im- prove graph contrastive learning",
        "Policy gradient methods for reinforcement learning with function approximation",
        "Improving deep learning with generic data augmentation",
        "Augmentations in graph contrastive learning: Current methodological ﬂaws & towards better practices",
        "Graph attention networks",
        "ComENet: Towards complete and efﬁcient message passing for 3d molecular graphs",
        "Hierarchical protein representations via complete 3d graph networks",
        "GraphCrop: Subgraph cropping for graph classiﬁcation",
        "Mixup for node and graph classiﬁcation",
        "Advanced graph and sequence neural net- works for molecular property prediction and drug discovery",
        "Self-supervised learning of graph neural networks: A uniﬁed review",
        "How powerful are graph neural networks?",
        "Periodic graph transformers for crys- tal material property prediction",
        "AutoGCL: Automated graph contrastive learning via learnable view generators",
        "Graph contrastive learning with augmentations",
        "Graph contrastive learning au- tomated",
        "Graph augmentation learning",
        "XGNN: Towards Model-Level Explanations of Graph Neural Networks",
        "On explainability of graph neural networks via subgraph explorations",
        "Label-invariant augmentation for semi-supervised graph classiﬁcation",
        "Adversarial AutoAugment",
        "Data augmentation for graph neural networks",
        "Graph data augmentation for graph machine learning: A survey",
        "Data Augmentation for Graph Classiﬁcation",
        "M-Evolve: Structural-mapping- based data augmentation for graph classiﬁcation",
        "An empirical study of graph contrastive learning",
        "Graph contrastive learning with adaptive augmentation"
      ],
      "meta_data": {
        "arxiv_id": "2202.13248v4",
        "authors": [
          "Youzhi Luo",
          "Michael McThrow",
          "Wing Yee Au",
          "Tao Komikado",
          "Kanji Uchino",
          "Koji Maruhashi",
          "Shuiwang Ji"
        ],
        "published_date": "2022-02-26T23:00:34Z",
        "github_url": "https://github.com/bknyaz/graph_attention_pool"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces GraphAug, an automated data augmentation framework for graph classification that focuses on generating label-invariant augmented graph samples. It formulates graph augmentation as a sequential transformation process, explicitly avoiding uniform random changes that may damage label-specific information, and leverages reinforcement learning to optimize augmentation strategies.",
        "methodology": "GraphAug employs a learnable augmentation model composed of a GNN-based encoder, a GRU for sequential decision making, and several MLPs to compute transformation probabilities for node feature masking (MaskNF), node dropping (DropNode), and edge perturbation (PerturbEdge). The framework uses a pre-trained reward generation model based on a graph matching network that estimates the label-invariance probability. The model is trained using a reinforcement learning based policy gradient approach (REINFORCE) to maximize the likelihood of preserving label-related information during augmentation.",
        "experimental_setup": "Experiments are conducted on two synthetic datasets (COLORS and TRIANGLES) and seven benchmark datasets from TUDatasets and the OGB benchmark (e.g., PROTEINS, IMDB-BINARY, COLLAB, MUTAG, NCI109, NCI1, ogbg-molhiv). Performance is evaluated through classification accuracy (or ROC-AUC for ogbg-molhiv) and ablation studies that compare GraphAug with baseline methods (uniform transformations, DropEdge, mixup methods, and other automated augmentation approaches) and manually designed label-invariant augmentations.",
        "limitations": "GraphAug exhibits some limitations including its complex two-stage training pipeline (pre-training a reward generation model followed by augmentation model training), significant computational cost especially on larger datasets, and challenges in adversarial joint training. Additionally, the benefits are most pronounced for small datasets, while for sufficiently large labeled datasets the performance gains might not justify the training overhead.",
        "future_research_directions": "Potential future directions include simplifying the training procedure (e.g., exploring stable one-stage or adversarial training methods), extending the augmentation framework to other graph learning tasks (such as node classification), reducing computational overhead for large-scale graphs, and exploring additional augmentation transformation techniques to further improve label-invariance and diversity in the augmented samples.",
        "experimental_code": "# File: main.py\nimport argparse\nimport random\nimport datetime\nfrom torchvision import transforms\nfrom graphdata import *\nfrom train_test import *\nimport warnings\nwarnings.filterwarnings('once')\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Run experiments with Graph Neural Networks')\n    parser.add_argument('-D', '--dataset', type=str, default='colors-3', choices=['colors-3', 'colors-4', 'colors-8', 'colors-16', 'colors-32', 'triangles', 'mnist', 'mnist-75sp', 'TU'], help='colors-n means the colors dataset with n-dimensional features; TU is any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets')\n    parser.add_argument('-d', '--data_dir', type=str, default='./data', help='path to the dataset')\n    parser.add_argument('--epochs', type=int, default=None, help='# of the epochs')\n    parser.add_argument('--batch_size', type=int, default=32, help='batch size for training data')\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning Rate')\n    parser.add_argument('--lr_decay_step', type=str, default=None, help='number of epochs after which to reduce learning rate')\n    parser.add_argument('--wdecay', type=float, default=0.0001, help='weight decay')\n    parser.add_argument('--dropout', type=float, default=0, help='dropout rate')\n    parser.add_argument('-f', '--filters', type=str, default='64,64,64', help='number of filters in each graph layer')\n    parser.add_argument('-K', '--filter_scale', type=int, default=1, help='filter scale (receptive field size), must be > 0; 1 for GCN or GIN')\n    parser.add_argument('--n_hidden', type=int, default=0, help='number of hidden units inside the graph layer')\n    parser.add_argument('--aggregation', type=str, default='mean', choices=['mean', 'sum'], help='neighbors aggregation inside the graph layer')\n    parser.add_argument('--readout', type=str, default=None, choices=['mean', 'sum', 'max'], help='type of global pooling over all nodes')\n    parser.add_argument('--kl_weight', type=float, default=100, help='weight of the KL term in the loss')\n    parser.add_argument('--pool', type=str, default=None, help='type of pooling between layers, None for global pooling only')\n    parser.add_argument('--pool_arch', type=str, default=None, help='pooling layers architecture defining whether to use fully-connected layers or GNN and to which layer to attach (e.g.: fc_prev, gnn_prev, fc_curr, gnn_curr, fc_prev_32)')\n    parser.add_argument('--init', type=str, default='normal', choices=['normal', 'uniform'], help='distribution used for initialization for the attention model')\n    parser.add_argument('--scale', type=str, default='1', help='initialized weights scale for the attention model, set to None to use PyTorch default init')\n    parser.add_argument('--degree_feature', action='store_true', default=False, help='use degree features (only for the Triangles dataset)')\n    parser.add_argument('--n_nodes', type=int, default=25, help='maximum number of nodes in the training set for collab, proteins and dd (35 for collab, 25 for proteins, 200 or 300 for dd)')\n    parser.add_argument('--cv_folds', type=int, default=5, help='number of folds for cross-validating hyperparameters for collab, proteins and dd (5 or 10 shows similar results, 5 is faster)')\n    parser.add_argument('--cv_threads', type=int, default=5, help='number of parallel threads for cross-validation')\n    parser.add_argument('--tune_init', action='store_true', default=False, help='do not tune initialization hyperparameters')\n    parser.add_argument('--ax', action='store_true', default=False, help='use AX for hyperparameter optimization (recommended)')\n    parser.add_argument('--ax_trials', type=int, default=30, help='number of AX trials (hyperparameters optimization steps)')\n    parser.add_argument('--cv', action='store_true', default=False, help='run in the cross-validation mode')\n    parser.add_argument('--seed_data', type=int, default=111, help='random seed for data splits')\n    parser.add_argument('--img_features', type=str, default='mean,coord', help='image features to use as node features')\n    parser.add_argument('--img_noise_levels', type=str, default=None, help='Gaussian noise standard deviations for grayscale and color image features')\n    parser.add_argument('--validation', action='store_true', default=False, help='run in the validation mode')\n    parser.add_argument('--debug', action='store_true', default=False, help='evaluate on the test set after each epoch (only for visualization purposes)')\n    parser.add_argument('--eval_attn_train', action='store_true', default=False, help='evaluate attention and save coefficients on the training set for models without learnable attention')\n    parser.add_argument('--eval_attn_test', action='store_true', default=False, help='evaluate attention and save coefficients on the test set for models without learnable attention')\n    parser.add_argument('--test_batch_size', type=int, default=100, help='batch size for test data')\n    parser.add_argument('--alpha_ws', type=str, default=None, help='attention labels that will be used for (weak)supervision')\n    parser.add_argument('--log_interval', type=int, default=400, help='print interval')\n    parser.add_argument('--results', type=str, default='./results', help='directory to save model checkpoints and other results, set to None to prevent saving anything')\n    parser.add_argument('--resume', type=str, default=None, help='checkpoint to load the model and optimzer states from and continue training')\n    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'], help='cuda/cpu')\n    parser.add_argument('--seed', type=int, default=111, help='random seed for model parameters')\n    parser.add_argument('--threads', type=int, default=0, help='number of threads for data loader')\n    args = parser.parse_args()\n    if args.readout in [None, 'None']:\n        args.readout = 'max'\n    set_default_lr_decay_step = args.lr_decay_step in [None, 'None']\n    if args.epochs in [None, 'None']:\n        if args.dataset.find('mnist') >= 0:\n            args.epochs = 30\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '20,25'\n        elif args.dataset == 'triangles':\n            args.epochs = 100\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '85,95'\n        elif args.dataset == 'TU':\n            args.epochs = 50\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '25,35,45'\n        elif args.dataset.find('color') >= 0:\n            if args.readout in [None, 'None']:\n                args.readout = 'sum'\n            if args.pool in [None, 'None']:\n                args.epochs = 100\n                if set_default_lr_decay_step:\n                    args.lr_decay_step = '90'\n            else:\n                args.epochs = 300\n                if set_default_lr_decay_step:\n                    args.lr_decay_step = '280'\n        else:\n            raise NotImplementedError(args.dataset)\n    args.lr_decay_step = list(map(int, args.lr_decay_step.split(',')))\n    args.filters = list(map(int, args.filters.split(',')))\n    args.img_features = args.img_features.split(',')\n    args.img_noise_levels = None if args.img_noise_levels in [None, 'None'] else list(map(float, args.img_noise_levels.split(',')))\n    args.pool = None if args.pool in [None, 'None'] else args.pool.split('_')\n    args.pool_arch = None if args.pool_arch in [None, 'None'] else args.pool_arch.split('_')\n    try:\n        args.scale = float(args.scale)\n    except:\n        args.scale = None\n    args.torch = torch.__version__\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n    return args\n\ndef load_synthetic(args):\n    train_dataset = SyntheticGraphs(args.data_dir, args.dataset, 'train', degree_feature=args.degree_feature, attn_coef=args.alpha_ws)\n    test_dataset = SyntheticGraphs(args.data_dir, args.dataset, 'val' if args.validation else 'test', degree_feature=args.degree_feature)\n    loss_fn = mse_loss\n    collate_fn = collate_batch\n    in_features = train_dataset.feature_dim\n    out_features = 1\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features)\n\ndef load_mnist(args):\n    use_mean_px = 'mean' in args.img_features\n    use_coord = 'coord' in args.img_features\n    assert use_mean_px, ('this mode is not well supported', use_mean_px)\n    gt_attn_threshold = 0 if args.pool is not None and args.pool[1] in ['gt'] and (args.filter_scale > 1) else 0.5\n    if args.dataset == 'mnist':\n        train_dataset = MNIST(args.data_dir, train=True, download=True, transform=transforms.ToTensor(), attn_coef=args.alpha_ws)\n    else:\n        train_dataset = MNIST75sp(args.data_dir, split='train', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold, attn_coef=args.alpha_ws)\n    noises, color_noises = (None, None)\n    if args.validation:\n        n_val = 5000\n        if args.dataset == 'mnist':\n            train_dataset.train_data = train_dataset.train_data[:-n_val]\n            train_dataset.train_labels = train_dataset.train_labels[:-n_val]\n            test_dataset = MNIST(args.data_dir, train=True, download=True, transform=transforms.ToTensor())\n            test_dataset.train_data = train_dataset.train_data[-n_val:]\n            test_dataset.train_labels = train_dataset.train_labels[-n_val:]\n        else:\n            train_dataset.train_val_split(np.arange(0, train_dataset.n_samples - n_val))\n            test_dataset = MNIST75sp(args.data_dir, split='train', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold)\n            test_dataset.train_val_split(np.arange(train_dataset.n_samples - n_val, train_dataset.n_samples))\n    else:\n        noise_file = pjoin(args.data_dir, '%s_noise.pt' % args.dataset.replace('-', '_'))\n        color_noise_file = pjoin(args.data_dir, '%s_color_noise.pt' % args.dataset.replace('-', '_'))\n        if args.dataset == 'mnist':\n            test_dataset = MNIST(args.data_dir, train=False, download=True, transform=transforms.ToTensor())\n            noise_shape = (len(test_dataset.test_labels), 28 * 28)\n        else:\n            test_dataset = MNIST75sp(args.data_dir, split='test', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold)\n            noise_shape = (len(test_dataset.labels), 75)\n        noises = load_save_noise(noise_file, noise_shape)\n        color_noises = load_save_noise(color_noise_file, (noise_shape[0], noise_shape[1], 3))\n    if args.dataset == 'mnist':\n        A, coord, mask = precompute_graph_images(train_dataset.train_data.shape[1])\n        collate_fn = lambda batch: collate_batch_images(batch, A, mask, use_mean_px=use_mean_px, coord=coord if use_coord else None, gt_attn_threshold=gt_attn_threshold, replicate_features=args.img_noise_levels is not None)\n    else:\n        train_dataset.precompute_graph_data(replicate_features=args.img_noise_levels is not None, threads=12)\n        test_dataset.precompute_graph_data(replicate_features=args.img_noise_levels is not None, threads=12)\n        collate_fn = collate_batch\n    loss_fn = F.cross_entropy\n    in_features = 0 if args.img_noise_levels is None else 2\n    for features in args.img_features:\n        if features == 'mean':\n            in_features += 1\n        elif features == 'coord':\n            in_features += 2\n        else:\n            raise NotImplementedError(features)\n    in_features = np.max((in_features, 1))\n    out_features = 10\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, noises, color_noises)\n\ndef load_TU(args, cv_folds=5):\n    loss_fn = F.cross_entropy\n    collate_fn = collate_batch\n    scale, init = (args.scale, args.init)\n    n_hidden_attn = float(args.pool_arch[2]) if args.pool_arch is not None and len(args.pool_arch) > 2 else 0\n    if args.pool is None:\n        datareader = DataReader(data_dir=args.data_dir, N_nodes=args.n_nodes, rnd_state=rnd_data, folds=0)\n        train_dataset = GraphData(datareader, None, 'train_val')\n        test_dataset = GraphData(datareader, None, 'test')\n        in_features = train_dataset.num_features\n        out_features = train_dataset.num_classes\n        pool = args.pool\n        kl_weight = args.kl_weight\n    elif args.pool[1] == 'gt':\n        raise ValueError('ground truth attention for TU datasets is not available')\n    elif args.pool[1] in ['sup', 'unsup']:\n        datareader = DataReader(data_dir=args.data_dir, N_nodes=args.n_nodes, rnd_state=rnd_data, folds=cv_folds)\n        if args.ax:\n            best_parameters = ax_optimize(datareader, args, collate_fn, loss_fn, None, folds=cv_folds, threads=args.cv_threads, n_trials=args.ax_trials)\n            pool = args.pool\n            kl_weight = best_parameters['kl_weight']\n            if args.tune_init:\n                scale, init = (best_parameters['scale'], best_parameters['init'])\n            n_hidden_attn, layer = (best_parameters['n_hidden_attn'], 1)\n            if layer == 0:\n                pool = copy.deepcopy(args.pool)\n                del pool[3]\n            pool = set_pool(best_parameters['pool'], pool)\n        else:\n            if not args.cv:\n                pool_thresh_values = np.array([float(args.pool[-1])])\n                n_hiddens = [n_hidden_attn]\n                layers = [1]\n            elif args.debug:\n                pool_thresh_values = np.array([0.0001, 0.1])\n                n_hiddens = [n_hidden_attn]\n                layers = [1]\n            else:\n                if args.data_dir.lower().find('proteins') >= 0:\n                    pool_thresh_values = np.array([0.002, 0.005, 0.01, 0.03, 0.05])\n                elif args.data_dir.lower().find('dd') >= 0:\n                    pool_thresh_values = np.array([0.0001, 0.001, 0.002, 0.005, 0.01, 0.03, 0.05, 0.1])\n                elif args.data_dir.lower().find('collab') >= 0:\n                    pool_thresh_values = np.array([0.001, 0.002, 0.005, 0.01, 0.03, 0.05, 0.1])\n                else:\n                    raise NotImplementedError('this dataset is not supported currently')\n                n_hiddens = np.array([0, 32])\n                layers = np.array([0, 1])\n            if args.pool[1] == 'sup' and (not args.debug) and args.cv:\n                kl_weight_values = np.array([0.25, 1, 2, 10])\n            else:\n                kl_weight_values = np.array([args.kl_weight])\n            if len(pool_thresh_values) > 1 or len(kl_weight_values) > 1 or len(n_hiddens) > 1 or (len(layers) > 1):\n                val_acc = np.zeros((len(layers), len(n_hiddens), len(pool_thresh_values), len(kl_weight_values)))\n                for i_, layer in enumerate(layers):\n                    if layer == 0:\n                        pool = copy.deepcopy(args.pool)\n                        del pool[3]\n                    else:\n                        pool = args.pool\n                    for j_, n_hidden_attn in enumerate(n_hiddens):\n                        for k_, pool_thresh in enumerate(pool_thresh_values):\n                            for m_, kl_weight in enumerate(kl_weight_values):\n                                val_acc[i_, j_, k_, m_] = cross_validation(datareader, args, collate_fn, loss_fn, set_pool(pool_thresh, pool), kl_weight, None, n_hidden_attn=n_hidden_attn, folds=cv_folds, threads=args.cv_threads)\n                ind1, ind2, ind3, ind4 = np.where(val_acc == np.max(val_acc))\n                print(val_acc)\n                print(ind1, ind2, ind3, ind4, layers[ind1], n_hiddens[ind2], pool_thresh_values[ind3], kl_weight_values[ind4], val_acc[ind1[0], ind2[0], ind3[0], ind4[0]])\n                layer = layers[ind1[0]]\n                if layer == 0:\n                    pool = copy.deepcopy(args.pool)\n                    del pool[3]\n                else:\n                    pool = args.pool\n                n_hidden_attn = n_hiddens[ind2[0]]\n                pool = set_pool(pool_thresh_values[ind3[0]], pool)\n                kl_weight = kl_weight_values[ind4[0]]\n            else:\n                pool = args.pool\n                kl_weight = args.kl_weight\n        train_dataset = GraphData(datareader, None, 'train_val')\n        test_dataset = GraphData(datareader, None, 'test')\n        in_features = train_dataset.num_features\n        out_features = train_dataset.num_classes\n        if args.pool[1] == 'sup':\n            train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n            train_loader_test = DataLoader(train_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n            start_epoch, model, optimizer, scheduler = create_model_optimizer(in_features, out_features, None, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n            for epoch in range(start_epoch, args.epochs + 1):\n                scheduler.step()\n                train_loss, acc = train(model, train_loader, optimizer, epoch, args, loss_fn, None)\n            train_loss, train_acc, attn_WS = test(model, train_loader_test, epoch, loss_fn, 'train', args, None, eval_attn=True)[:3]\n            train_dataset = GraphData(datareader, None, 'train_val', attn_labels=attn_WS)\n    else:\n        raise NotImplementedError(args.pool)\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, pool, kl_weight, scale, init, n_hidden_attn)\nif __name__ == '__main__':\n    dt = datetime.datetime.now()\n    print('start time:', dt)\n    args = parse_args()\n    args.experiment_ID = '%06d' % dt.microsecond\n    print('experiment_ID: ', args.experiment_ID)\n    if args.cv_threads > 1 and args.dataset == 'TU':\n        torch.multiprocessing.set_start_method('spawn')\n    print('gpus: ', torch.cuda.device_count())\n    if args.results not in [None, 'None'] and (not os.path.isdir(args.results)):\n        os.mkdir(args.results)\n    rnd, rnd_data = set_seed(args.seed, args.seed_data)\n    pool = args.pool\n    kl_weight = args.kl_weight\n    scale = args.scale\n    init = args.init\n    n_hidden_attn = float(args.pool_arch[2]) if args.pool_arch is not None and len(args.pool_arch) > 2 else 0\n    if args.dataset.find('colors') >= 0 or args.dataset == 'triangles':\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features = load_synthetic(args)\n    elif args.dataset in ['mnist', 'mnist-75sp']:\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, noises, color_noises = load_mnist(args)\n    else:\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, pool, kl_weight, scale, init, n_hidden_attn = load_TU(args, cv_folds=args.cv_folds)\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n    train_loader_test = DataLoader(train_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    print('test_dataset', test_dataset.split)\n    test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    start_epoch, model, optimizer, scheduler = create_model_optimizer(in_features, out_features, pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n    feature_stats = None\n    if args.dataset in ['mnist', 'mnist-75sp']:\n        feature_stats = compute_feature_stats(model, train_loader, args.device, n_batches=1000)\n\n    def test_fn(loader, epoch, split, eval_attn):\n        test_loss, acc, _, _ = test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=None, img_noise_level=None, eval_attn=eval_attn, alpha_WS_name='orig')\n        if args.dataset in ['mnist', 'mnist-75sp'] and split == 'test' and (args.img_noise_levels is not None):\n            test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=noises, img_noise_level=args.img_noise_levels[0], eval_attn=eval_attn, alpha_WS_name='noisy')\n            test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=color_noises, img_noise_level=args.img_noise_levels[1], eval_attn=eval_attn, alpha_WS_name='noisy-c')\n        return (test_loss, acc)\n    if start_epoch > args.epochs:\n        print('evaluating the model')\n        test_fn(test_loader, start_epoch - 1, 'val' if args.validation else 'test', args.eval_attn_test)\n    else:\n        for epoch in range(start_epoch, args.epochs + 1):\n            eval_epoch = epoch <= 1 or epoch == args.epochs\n            scheduler.step()\n            train_loss, acc = train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats)\n            if eval_epoch:\n                save_checkpoint(model, scheduler, optimizer, args, epoch)\n                test_fn(train_loader_test, epoch, 'train', epoch == args.epochs and args.eval_attn_train)\n            if args.validation:\n                test_fn(test_loader, epoch, 'val', epoch == args.epochs and args.eval_attn_test)\n            elif eval_epoch or args.debug:\n                test_fn(test_loader, epoch, 'test', epoch == args.epochs and args.eval_attn_test)\n    print('done in {}'.format(datetime.datetime.now() - dt))\n\n# File: train_test.py\nimport time\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom chebygin import *\nfrom utils import *\nfrom graphdata import *\nimport torch.multiprocessing as mp\nimport multiprocessing\ntry:\n    import ax\n    from ax.service.managed_loop import optimize\nexcept Exception as e:\n    print('AX is not available: %s' % str(e))\n\ndef set_pool(pool_thresh, args_pool):\n    pool = copy.deepcopy(args_pool)\n    for i, s in enumerate(pool):\n        try:\n            thresh = float(s)\n            pool[i] = str(pool_thresh)\n        except:\n            continue\n    return pool\n\ndef train_evaluate(datareader, args, collate_fn, loss_fn, feature_stats, parameterization, folds=10, threads=5):\n    print('parameterization', parameterization)\n    pool_thresh, kl_weight = (parameterization['pool'], parameterization['kl_weight'])\n    pool = args.pool\n    if args.tune_init:\n        scale, init = (parameterization['scale'], parameterization['init'])\n    else:\n        scale, init = (args.scale, args.init)\n    n_hidden_attn, layer = (parameterization['n_hidden_attn'], 1)\n    if layer == 0:\n        pool = copy.deepcopy(args.pool)\n        del pool[3]\n    pool = set_pool(pool_thresh, pool)\n    manager = multiprocessing.Manager()\n    val_acc = manager.dict()\n    assert threads <= folds, (threads, folds)\n    n_it = int(np.ceil(float(folds) / threads))\n    for i in range(n_it):\n        processes = []\n        if threads <= 1:\n            single_job(i * threads, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n        else:\n            for fold in range(threads):\n                p = mp.Process(target=single_job, args=(i * threads + fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale, init, n_hidden_attn))\n                p.start()\n                processes.append(p)\n            for p in processes:\n                p.join()\n    print(val_acc)\n    val_acc = list(val_acc.values())\n    print('average and std over {} folds: {} +- {}'.format(folds, np.mean(val_acc), np.std(val_acc)))\n    metric = np.mean(val_acc) - np.std(val_acc)\n    print('metric: avg acc - std: {}'.format(metric))\n    return metric\n\ndef ax_optimize(datareader, args, collate_fn, loss_fn, feature_stats, folds=10, threads=5, n_trials=30):\n    parameters = [{'name': 'pool', 'type': 'range', 'bounds': [0.0001, 0.02], 'log_scale': False}, {'name': 'kl_weight', 'type': 'range', 'bounds': [0.1, 10.0], 'log_scale': False}, {'name': 'n_hidden_attn', 'type': 'choice', 'values': [0, 32]}]\n    if args.tune_init:\n        parameters.extend([{'name': 'scale', 'type': 'range', 'bounds': [0.1, 2.0], 'log_scale': False}, {'name': 'init', 'type': 'choice', 'values': ['normal', 'uniform']}])\n    best_parameters, values, experiment, model = optimize(parameters=parameters, evaluation_function=lambda parameterization: train_evaluate(datareader, args, collate_fn, loss_fn, feature_stats, parameterization, folds=folds, threads=threads), total_trials=n_trials, objective_name='accuracy')\n    print('best_parameters', best_parameters)\n    print('values', values)\n    return best_parameters\n\ndef train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats=None, log=True):\n    model.train()\n    optimizer.zero_grad()\n    n_samples, correct, train_loss = (0, 0, 0)\n    alpha_pred, alpha_GT = ({}, {})\n    start = time.time()\n    for batch_idx, data in enumerate(train_loader):\n        data = data_to_device(data, args.device)\n        if feature_stats is not None:\n            data[0] = (data[0] - feature_stats[0]) / feature_stats[1]\n        if batch_idx == 0 and epoch <= 1:\n            sanity_check(model.eval(), data)\n            model.train()\n        optimizer.zero_grad()\n        mask = [data[2].view(len(data[2]), -1)]\n        output, other_outputs = model(data)\n        other_losses = other_outputs['reg'] if 'reg' in other_outputs else []\n        alpha = other_outputs['alpha'] if 'alpha' in other_outputs else []\n        mask.extend(other_outputs['mask'] if 'mask' in other_outputs else [])\n        targets = data[3]\n        loss = loss_fn(output, targets)\n        for l in other_losses:\n            loss += l\n        loss_item = loss.item()\n        train_loss += loss_item\n        n_samples += len(targets)\n        loss.backward()\n        optimizer.step()\n        time_iter = time.time() - start\n        correct += count_correct(output.detach(), targets.detach())\n        update_attn(data, alpha, alpha_pred, alpha_GT, mask)\n        acc = 100.0 * correct / n_samples\n        train_loss_avg = train_loss / (batch_idx + 1)\n        if log and (batch_idx > 0 and batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1):\n            print('Train set (epoch {}): [{}/{} ({:.0f}%)]\\tLoss: {:.4f} (avg: {:.4f}), other losses: {}\\tAcc metric: {}/{} ({:.2f}%)\\t AttnAUC: {}\\t avg sec/iter: {:.4f}'.format(epoch, n_samples, len(train_loader.dataset), 100.0 * n_samples / len(train_loader.dataset), loss_item, train_loss_avg, ['%.4f' % l.item() for l in other_losses], correct, n_samples, acc, ['%.2f' % a for a in attn_AUC(alpha_GT, alpha_pred)], time_iter / (batch_idx + 1)))\n    assert n_samples == len(train_loader.dataset), (n_samples, len(train_loader.dataset))\n    return (train_loss, acc)\n\ndef test(model, test_loader, epoch, loss_fn, split, args, feature_stats=None, noises=None, img_noise_level=None, eval_attn=False, alpha_WS_name=''):\n    model.eval()\n    n_samples, correct, test_loss = (0, 0, 0)\n    pred, targets, N_nodes = ([], [], [])\n    start = time.time()\n    alpha_pred, alpha_GT = ({}, {})\n    if eval_attn:\n        alpha_pred[0] = []\n        print('testing with evaluation of attention: takes longer time')\n    if args.debug:\n        debug_data = {}\n    with torch.no_grad():\n        for batch_idx, data in enumerate(test_loader):\n            data = data_to_device(data, args.device)\n            if feature_stats is not None:\n                assert feature_stats[0].shape[2] == feature_stats[1].shape[2] == data[0].shape[2], (feature_stats[0].shape, feature_stats[1].shape, data[0].shape)\n                data[0] = (data[0] - feature_stats[0]) / feature_stats[1]\n            if batch_idx == 0 and epoch <= 1:\n                sanity_check(model, data)\n            if noises is not None:\n                noise = noises[n_samples:n_samples + len(data[0])].to(args.device) * img_noise_level\n                if len(noise.shape) == 2:\n                    noise = noise.unsqueeze(2)\n                data[0][:, :, :3] = data[0][:, :, :3] + noise\n            mask = [data[2].view(len(data[2]), -1)]\n            N_nodes.append(data[4]['N_nodes'].detach())\n            targets.append(data[3].detach())\n            output, other_outputs = model(data)\n            other_losses = other_outputs['reg'] if 'reg' in other_outputs else []\n            alpha = other_outputs['alpha'] if 'alpha' in other_outputs else []\n            mask.extend(other_outputs['mask'] if 'mask' in other_outputs else [])\n            if args.debug:\n                for key in other_outputs:\n                    if key.find('debug') >= 0:\n                        if key not in debug_data:\n                            debug_data[key] = []\n                        debug_data[key].append([d.data.cpu().numpy() for d in other_outputs[key]])\n            if args.torch.find('1.') == 0:\n                loss = loss_fn(output, data[3], reduction='sum')\n            else:\n                loss = loss_fn(output, data[3], reduce=False).sum()\n            for l in other_losses:\n                loss += l\n            test_loss += loss.item()\n            pred.append(output.detach())\n            update_attn(data, alpha, alpha_pred, alpha_GT, mask)\n            if eval_attn:\n                assert len(alpha) == 0, 'invalid mode, eval_attn should be false for this type of pooling'\n                alpha_pred[0].extend(attn_heatmaps(model, args.device, data, output.data, test_loader.batch_size, constant_mask=args.dataset == 'mnist'))\n            n_samples += len(data[0])\n            if eval_attn and (n_samples % 100 == 0 or n_samples == len(test_loader.dataset)):\n                print('{}/{} samples processed'.format(n_samples, len(test_loader.dataset)))\n    assert n_samples == len(test_loader.dataset), (n_samples, len(test_loader.dataset))\n    pred = torch.cat(pred)\n    targets = torch.cat(targets)\n    N_nodes = torch.cat(N_nodes)\n    if args.dataset.find('colors') >= 0:\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=25)\n        if pred.shape[0] > 2500:\n            correct += count_correct(pred[2500:5000], targets[2500:5000], N_nodes=N_nodes[2500:5000], N_nodes_min=26, N_nodes_max=200)\n            correct += count_correct(pred[5000:], targets[5000:], N_nodes=N_nodes[5000:], N_nodes_min=26, N_nodes_max=200)\n    elif args.dataset == 'triangles':\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=25)\n        if pred.shape[0] > 5000:\n            correct += count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=26, N_nodes_max=100)\n    else:\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=100000.0)\n    time_iter = time.time() - start\n    test_loss_avg = test_loss / n_samples\n    acc = 100.0 * correct / n_samples\n    print('{} set (epoch {}): Avg loss: {:.4f}, Acc metric: {}/{} ({:.2f}%)\\t AttnAUC: {}\\t avg sec/iter: {:.4f}\\n'.format(split.capitalize(), epoch, test_loss_avg, correct, n_samples, acc, ['%.2f' % a for a in attn_AUC(alpha_GT, alpha_pred)], time_iter / (batch_idx + 1)))\n    if args.debug:\n        for key in debug_data:\n            for layer in range(len(debug_data[key][0])):\n                print('{} (layer={}): {:.5f}'.format(key, layer, np.mean([d[layer] for d in debug_data[key]])))\n    if eval_attn:\n        alpha_pred = alpha_pred[0]\n        if args.results in [None, 'None', ''] or alpha_WS_name == '':\n            print('skip saving alpha values, invalid results dir (%s) or alpha_WS_name (%s)' % (args.results, alpha_WS_name))\n        else:\n            file_path = pjoin(args.results, '%s_alpha_WS_%s_seed%d_%s.pkl' % (args.dataset, split, args.seed, alpha_WS_name))\n            if os.path.isfile(file_path):\n                print('WARNING: file %s exists and will be overwritten' % file_path)\n            with open(file_path, 'wb') as f:\n                pickle.dump(alpha_pred, f, protocol=2)\n    return (test_loss, acc, alpha_pred, pred)\n\ndef update_attn(data, alpha, alpha_pred, alpha_GT, mask):\n    key = 'node_attn_eval'\n    for layer in range(len(mask)):\n        mask[layer] = mask[layer].data.cpu().numpy() > 0\n    if key in data[4]:\n        if not isinstance(data[4][key], list):\n            data[4][key] = [data[4][key]]\n        for layer in range(len(data[4][key])):\n            if layer not in alpha_GT:\n                alpha_GT[layer] = []\n            alpha_GT[layer].extend(masked_alpha(data[4][key][layer].data.cpu().numpy(), mask[layer]))\n    for layer in range(len(alpha)):\n        if layer not in alpha_pred:\n            alpha_pred[layer] = []\n        alpha_pred[layer].extend(masked_alpha(alpha[layer].data.cpu().numpy(), mask[layer]))\n\ndef masked_alpha(alpha, mask):\n    alpha_lst = []\n    for i in range(len(alpha)):\n        alpha_lst.append(alpha[i][mask[i]])\n    return alpha_lst\n\ndef attn_heatmaps(model, device, data, output_org, batch_size=1, constant_mask=False):\n    labels = torch.argmax(output_org, dim=1)\n    B, N_nodes_max, C = data[0].shape\n    alpha_WS = []\n    if N_nodes_max > 1000:\n        print('WARNING: graph is too large (%d nodes) and not supported by this function (evaluation will be incorrect for graphs in this batch).' % N_nodes_max)\n        for b in range(B):\n            n = data[2][b].sum().item()\n            alpha_WS.append(np.zeros((1, n)) + 1.0 / n)\n        return alpha_WS\n    if constant_mask:\n        mask = torch.ones(N_nodes_max, N_nodes_max - 1).to(device)\n    node_ids = torch.arange(start=0, end=N_nodes_max, device=device).view(1, -1).repeat(N_nodes_max, 1)\n    node_ids[np.diag_indices(N_nodes_max, 2)] = -1\n    node_ids = node_ids[node_ids >= 0].view(N_nodes_max, N_nodes_max - 1).long()\n    with torch.no_grad():\n        for b in range(B):\n            x = torch.gather(data[0][b].unsqueeze(0).expand(N_nodes_max, -1, -1), dim=1, index=node_ids.unsqueeze(2).expand(-1, -1, C))\n            if not constant_mask:\n                mask = torch.gather(data[2][b].unsqueeze(0).expand(N_nodes_max, -1), dim=1, index=node_ids)\n            A = torch.gather(data[1][b].unsqueeze(0).expand(N_nodes_max, -1, -1), dim=1, index=node_ids.unsqueeze(2).expand(-1, -1, N_nodes_max))\n            A = torch.gather(A, dim=2, index=node_ids.unsqueeze(1).expand(-1, N_nodes_max - 1, -1))\n            output = torch.zeros(N_nodes_max).to(device)\n            n_chunks = int(np.ceil(N_nodes_max / float(batch_size)))\n            for i in range(n_chunks):\n                idx = np.arange(i * batch_size, (i + 1) * batch_size) if i < n_chunks - 1 else np.arange(i * batch_size, N_nodes_max)\n                output[idx] = model([x[idx], A[idx], mask[idx], None, {}])[0][:, labels[b]].data\n            alpha = torch.abs(output - output_org[b, labels[b]]).view(1, N_nodes_max)\n            if not constant_mask:\n                alpha = alpha[data[2][b].view(1, N_nodes_max)]\n            alpha_WS.append(normalize(alpha).data.cpu().numpy())\n    return alpha_WS\n\ndef save_checkpoint(model, scheduler, optimizer, args, epoch):\n    if args.results in [None, 'None']:\n        print('skip saving checkpoint, invalid results dir: %s' % args.results)\n        return\n    file_path = '%s/checkpoint_%s_%s_epoch%d_seed%07d.pth.tar' % (args.results, args.dataset, args.experiment_ID, epoch, args.seed)\n    try:\n        print('saving the model to %s' % file_path)\n        state = {'epoch': epoch, 'args': args, 'state_dict': model.state_dict(), 'scheduler': scheduler.state_dict(), 'optimizer': optimizer.state_dict()}\n        if os.path.isfile(file_path):\n            print('WARNING: file %s exists and will be overwritten' % file_path)\n        torch.save(state, file_path)\n    except Exception as e:\n        print('error saving the model', e)\n\ndef load_checkpoint(model, optimizer, scheduler, file_path):\n    print('loading the model from %s' % file_path)\n    state = torch.load(file_path)\n    model.load_state_dict(state['state_dict'])\n    optimizer.load_state_dict(state['optimizer'])\n    scheduler.load_state_dict(state['scheduler'])\n    print('loading from epoch %d done' % state['epoch'])\n    return state['epoch'] + 1\n\ndef create_model_optimizer(in_features, out_features, pool, kl_weight, args, scale=None, init=None, n_hidden_attn=None):\n    set_seed(args.seed, seed_data=None)\n    model = ChebyGIN(in_features=in_features, out_features=out_features, filters=args.filters, K=args.filter_scale, n_hidden=args.n_hidden, aggregation=args.aggregation, dropout=args.dropout, readout=args.readout, pool=pool, pool_arch=args.pool_arch if n_hidden_attn in [None, 0] else args.pool_arch[:2] + ['%d' % n_hidden_attn], large_graph=args.dataset.lower() == 'mnist', kl_weight=float(kl_weight), init=args.init if init is None else init, scale=args.scale if scale is None else scale, debug=args.debug)\n    print(model)\n    print('model capacity: %d' % np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay, betas=(0.5, 0.999))\n    scheduler = lr_scheduler.MultiStepLR(optimizer, args.lr_decay_step, gamma=0.1)\n    epoch = 1\n    if args.resume not in [None, 'None']:\n        epoch = load_checkpoint(model, optimizer, scheduler, args.resume)\n        if epoch < args.epochs + 1:\n            print('resuming training for epoch %d' % epoch)\n    model.to(args.device)\n    return (epoch, model, optimizer, scheduler)\n\ndef single_job(fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=None, init=None, n_hidden_attn=None):\n    set_seed(args.seed, seed_data=None)\n    wsup = args.pool[1] == 'sup'\n    train_loader = DataLoader(GraphData(datareader, fold, 'train'), batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n    val_loader = DataLoader(GraphData(datareader, fold, 'val'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    start_epoch, model, optimizer, scheduler = create_model_optimizer(train_loader.dataset.num_features, train_loader.dataset.num_classes, None if wsup else pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n    for epoch in range(start_epoch, args.epochs + 1):\n        scheduler.step()\n        train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats, log=False)\n    if wsup:\n        train_loader_test = DataLoader(GraphData(datareader, fold, 'train'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n        train_loss, train_acc, attn_WS = test(model, train_loader_test, epoch, loss_fn, 'train', args, feature_stats, eval_attn=True)[:3]\n        train_loader = DataLoader(GraphData(datareader, fold, 'train', attn_labels=attn_WS), batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n        val_loader = DataLoader(GraphData(datareader, fold, 'val'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n        start_epoch, model, optimizer, scheduler = create_model_optimizer(train_loader.dataset.num_features, train_loader.dataset.num_classes, pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n        for epoch in range(start_epoch, args.epochs + 1):\n            scheduler.step()\n            train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats, log=False)\n    acc = test(model, val_loader, epoch, loss_fn, 'val', args, feature_stats)[1]\n    val_acc[fold] = acc\n\ndef cross_validation(datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, n_hidden_attn=None, folds=10, threads=5):\n    print('%d-fold cross-validation' % folds)\n    manager = multiprocessing.Manager()\n    val_acc = manager.dict()\n    assert threads <= folds, (threads, folds)\n    n_it = int(np.ceil(float(folds) / threads))\n    for i in range(n_it):\n        processes = []\n        if threads <= 1:\n            single_job(i * threads, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=args.scale, init=args.init, n_hidden_attn=n_hidden_attn)\n        else:\n            for fold in range(threads):\n                p = mp.Process(target=single_job, args=(i * threads + fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, args.scale, args.init, n_hidden_attn))\n                p.start()\n                processes.append(p)\n            for p in processes:\n                p.join()\n    print(val_acc)\n    val_acc = list(val_acc.values())\n    print('average and std over {} folds: {} +- {}'.format(folds, np.mean(val_acc), np.std(val_acc)))\n    metric = np.mean(val_acc) - np.std(val_acc)\n    print('metric: avg acc - std: {}'.format(metric))\n    return metric\n\n# File: chebygin.py\nimport numpy as np\nimport torch\nimport torch.sparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom attention_pooling import *\nfrom utils import *\n\nclass ChebyGINLayer(nn.Module):\n    \"\"\"\n    General Graph Neural Network layer that depending on arguments can be:\n    1. Graph Convolution Layer (T. Kipf and M. Welling, ICLR 2017)\n    2. Chebyshev Graph Convolution Layer (M. Defferrard et al., NeurIPS 2017)\n    3. GIN Layer (K. Xu et al., ICLR 2019)\n    4. ChebyGIN Layer (B. Knyazev et al., ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds)\n    The first three types (1-3) of layers are particular cases of the fourth (4) case.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, K, n_hidden=0, aggregation='mean', activation=nn.ReLU(True), n_relations=1):\n        super(ChebyGINLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_relations = n_relations\n        assert K > 0, 'order is assumed to be > 0'\n        self.K = K\n        assert n_hidden >= 0, ('invalid n_hidden value', n_hidden)\n        self.n_hidden = n_hidden\n        assert aggregation in ['mean', 'sum'], ('invalid aggregation', aggregation)\n        self.aggregation = aggregation\n        self.activation = activation\n        n_in = self.in_features * self.K * n_relations\n        if self.n_hidden == 0:\n            fc = [nn.Linear(n_in, self.out_features)]\n        else:\n            fc = [nn.Linear(n_in, n_hidden), nn.ReLU(True), nn.Linear(n_hidden, self.out_features)]\n        if activation is not None:\n            fc.append(activation)\n        self.fc = nn.Sequential(*fc)\n        print('ChebyGINLayer', list(self.fc.children())[0].weight.shape, torch.norm(list(self.fc.children())[0].weight, dim=1)[:10])\n\n    def __repr__(self):\n        return 'ChebyGINLayer(in_features={}, out_features={}, K={}, n_hidden={}, aggregation={})\\nfc={}'.format(self.in_features, self.out_features, self.K, self.n_hidden, self.aggregation, str(self.fc))\n\n    def chebyshev_basis(self, L, X, K):\n        \"\"\"\n        Return T_k X where T_k are the Chebyshev polynomials of order up to K.\n        :param L: graph Laplacian, batch (B), nodes (N), nodes (N)\n        :param X: input of size batch (B), nodes (N), features (F)\n        :param K: Chebyshev polynomial order, i.e. filter size (number of hopes)\n        :return: Tensor of size (B,N,K,F) as a result of multiplying T_k(L) by X for each order\n        \"\"\"\n        if K > 1:\n            Xt = [X]\n            Xt.append(torch.bmm(L, X))\n            for k in range(2, K):\n                Xt.append(2 * torch.bmm(L, Xt[k - 1]) - Xt[k - 2])\n            Xt = torch.stack(Xt, 2)\n            return Xt\n        else:\n            assert K == 1, K\n            return torch.bmm(L, X).unsqueeze(2)\n\n    def laplacian_batch(self, A, add_identity=False):\n        \"\"\"\n        Computes normalized Laplacian transformed so that its eigenvalues are in range [-1, 1].\n        Note that sum of all eigenvalues = trace(L) = 0.\n        :param A: Tensor of size (B,N,N) containing batch (B) of adjacency matrices of shape N,N\n        :return: Normalized Laplacian of size (B,N,N)\n        \"\"\"\n        B, N = A.shape[:2]\n        if add_identity:\n            A = A + torch.eye(N, device=A.get_device() if A.is_cuda else 'cpu').unsqueeze(0)\n        D = torch.sum(A, 1)\n        D_hat = (D + 1e-05) ** (-0.5)\n        L = D_hat.view(B, N, 1) * A * D_hat.view(B, 1, N)\n        if not add_identity:\n            L = -L\n        return (D, L)\n\n    def forward(self, data):\n        x, A, mask = data[:3]\n        B, N, F = x.shape\n        assert N == A.shape[1] == A.shape[2], ('invalid shape', N, x.shape, A.shape)\n        if len(A.shape) == 3:\n            A = A.unsqueeze(3)\n        y_out = []\n        for rel in range(A.shape[3]):\n            D, L = self.laplacian_batch(A[:, :, :, rel], add_identity=self.K == 1)\n            y = self.chebyshev_basis(L, x, self.K)\n            if self.aggregation == 'sum':\n                if self.K == 1:\n                    y = y * D.view(B, N, 1, 1)\n                else:\n                    D_GIN = torch.ones(B, N, self.K, device=x.get_device() if x.is_cuda else 'cpu')\n                    D_GIN[:, :, 1:] = D.view(B, N, 1).expand(-1, -1, self.K - 1)\n                    y = y * D_GIN.view(B, N, self.K, 1)\n            y_out.append(y)\n        y = torch.cat(y_out, dim=2)\n        y = self.fc(y.view(B, N, -1))\n        if len(mask.shape) == 2:\n            mask = mask.unsqueeze(2)\n        y = y * mask.float()\n        output = [y, A, mask]\n        output.extend(data[3:] + [x])\n        return output\n\nclass GraphReadout(nn.Module):\n    \"\"\"\n    Global pooling layer applied after the last graph layer.\n    \"\"\"\n\n    def __init__(self, pool_type):\n        super(GraphReadout, self).__init__()\n        self.pool_type = pool_type\n        dim = 1\n        if pool_type == 'max':\n            self.readout_layer = lambda x, mask: torch.max(x, dim=dim)[0]\n        elif pool_type in ['avg', 'mean']:\n            self.readout_layer = lambda x, mask: torch.sum(x, dim=dim) / torch.sum(mask, dim=dim).float()\n        elif pool_type in ['sum']:\n            self.readout_layer = lambda x, mask: torch.sum(x, dim=dim)\n        else:\n            raise NotImplementedError(pool_type)\n\n    def __repr__(self):\n        return 'GraphReadout({})'.format(self.pool_type)\n\n    def forward(self, data):\n        x, A, mask = data[:3]\n        B, N = x.shape[:2]\n        x = self.readout_layer(x, mask.view(B, N, 1))\n        output = [x]\n        output.extend(data[1:])\n        return output\n\nclass ChebyGIN(nn.Module):\n    \"\"\"\n    Graph Neural Network class.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, filters, K=1, n_hidden=0, aggregation='mean', dropout=0, readout='max', pool=None, pool_arch='fc_prev'.split('_'), large_graph=False, kl_weight=None, graph_layer_fn=None, init='normal', scale=None, debug=False):\n        super(ChebyGIN, self).__init__()\n        self.out_features = out_features\n        assert len(filters) > 0, 'filters must be an iterable object with at least one element'\n        assert K > 0, 'filter scale must be a positive integer'\n        self.pool = pool\n        self.pool_arch = pool_arch\n        self.debug = debug\n        n_prev = None\n        attn_gnn = None\n        if graph_layer_fn is None:\n            graph_layer_fn = lambda n_in, n_out, K_, n_hidden_, activation: ChebyGINLayer(in_features=n_in, out_features=n_out, K=K_, n_hidden=n_hidden_, aggregation=aggregation, activation=activation)\n            if self.pool_arch is not None and self.pool_arch[0] == 'gnn':\n                attn_gnn = lambda n_in: ChebyGIN(in_features=n_in, out_features=0, filters=[32, 32, 1], K=np.min((K, 2)), n_hidden=0, graph_layer_fn=graph_layer_fn)\n        graph_layers = []\n        for layer, f in enumerate(filters + [None]):\n            n_in = in_features if layer == 0 else filters[layer - 1]\n            if self.pool is not None and len(self.pool) > len(filters) + layer and (self.pool[layer + 3] != 'skip'):\n                graph_layers.append(AttentionPooling(in_features=n_in, in_features_prev=n_prev, pool_type=self.pool[:3] + [self.pool[layer + 3]], pool_arch=self.pool_arch, large_graph=large_graph, kl_weight=kl_weight, attn_gnn=attn_gnn, init=init, scale=scale, debug=debug))\n            if f is not None:\n                graph_layers.append(graph_layer_fn(n_in, f, K, n_hidden, None if self.out_features == 0 and layer == len(filters) - 1 else nn.ReLU(True)))\n                n_prev = n_in\n        if self.out_features > 0:\n            graph_layers.append(GraphReadout(readout))\n        self.graph_layers = nn.Sequential(*graph_layers)\n        if self.out_features > 0:\n            self.fc = nn.Sequential(*([nn.Dropout(p=dropout)] if dropout > 0 else []) + [nn.Linear(filters[-1], out_features)])\n\n    def forward(self, data):\n        data = self.graph_layers(data)\n        if self.out_features > 0:\n            y = self.fc(data[0])\n        else:\n            y = data[0]\n        return (y, data[4])\n\n# File: generate_data.py\nimport os\nimport numpy as np\nimport pickle\nimport argparse\nimport networkx as nx\nimport datetime\nimport random\nimport multiprocessing as mp\nfrom utils import *\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Generate synthetic graph datasets')\n    parser.add_argument('-D', '--dataset', type=str, default='colors', choices=['colors', 'triangles'])\n    parser.add_argument('-o', '--out_dir', type=str, default='./data', help='path where to save superpixels')\n    parser.add_argument('--N_train', type=int, default=500, help='number of training graphs (500 for colors and 30000 for triangles)')\n    parser.add_argument('--N_val', type=int, default=2500, help='number of graphs in the validation set (2500 for colors and 5000 for triangles)')\n    parser.add_argument('--N_test', type=int, default=2500, help='number of graphs in each test subset (2500 for colors and 5000 for triangles)')\n    parser.add_argument('--label_min', type=int, default=0, help='smallest label value for a graph (i.e. smallest number of green nodes); 1 for triangles')\n    parser.add_argument('--label_max', type=int, default=10, help='largest label value for a graph (i.e. largest number of green nodes)')\n    parser.add_argument('--N_min', type=int, default=4, help='minimum number of nodes')\n    parser.add_argument('--N_max', type=int, default=200, help='maximum number of nodes (default: 200 for colors and 100 for triangles')\n    parser.add_argument('--N_max_train', type=int, default=25, help='maximum number of nodes in the training set')\n    parser.add_argument('--dim', type=int, default=3, help='node feature dimensionality')\n    parser.add_argument('--green_ch_index', type=int, default=1, help='index of non-zero value in a one-hot node feature vector, i.e. [0, 1, 0] in case green_channel_index=1 and dim=3')\n    parser.add_argument('--seed', type=int, default=111, help='seed for shuffling nodes')\n    parser.add_argument('--threads', type=int, default=0, help='only for triangles')\n    args = parser.parse_args()\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n    return args\n\ndef check_graph_duplicates(Adj_matrices, node_features=None):\n    n_graphs = len(Adj_matrices)\n    print('check for duplicates for %d graphs' % n_graphs)\n    n_duplicates = 0\n    for i in range(n_graphs):\n        if node_features is not None:\n            assert Adj_matrices[i].shape[0] == node_features[i].shape[0], ('invalid data', i, Adj_matrices[i].shape[0], node_features[i].shape[0])\n        for j in range(i + 1, n_graphs):\n            if Adj_matrices[i].shape[0] == Adj_matrices[j].shape[0]:\n                if np.allclose(Adj_matrices[i], Adj_matrices[j]):\n                    if node_features is None or np.allclose(node_features[i], node_features[j]):\n                        n_duplicates += 1\n                        print('duplicates %d/%d' % (n_duplicates, n_graphs * (n_graphs - 1) / 2))\n    if n_duplicates > 0:\n        raise ValueError('%d duplicates found in the dataset' % n_duplicates)\n    print('no duplicated graphs')\n\ndef get_node_features_Colors(N_nodes, N_green, dim, green_ch_index=1, new_colors=False):\n    node_features = np.zeros((N_nodes, dim))\n    idx_not_green = rnd.randint(0, dim - 1, size=N_nodes - N_green)\n    idx_non_zero = np.concatenate((idx_not_green, np.zeros(N_green, np.int) + dim - 1))\n    idx_non_zero_cp = idx_non_zero.copy()\n    idx_non_zero[idx_non_zero_cp == dim - 1] = green_ch_index\n    idx_non_zero[idx_non_zero_cp == green_ch_index] = dim - 1\n    rnd.shuffle(idx_non_zero)\n    node_features[np.arange(N_nodes), idx_non_zero] = 1\n    if new_colors:\n        for ind in np.where(idx_non_zero != green_ch_index)[0]:\n            node_features[ind] = rnd.randint(0, 2, size=dim)\n            node_features[ind, green_ch_index] = 0\n    label = np.sum((np.sum(node_features, 1) == node_features[:, green_ch_index]) & (node_features[:, green_ch_index] == 1))\n    gt_attn = (idx_non_zero == green_ch_index).reshape(-1, 1)\n    label2 = np.sum(gt_attn)\n    assert N_green == label == label2, ('invalid node features', N_green, label, label2)\n    return (node_features, idx_non_zero, gt_attn)\n\ndef generate_graphs_Colors(N_graphs, N_min, N_max, dim, args, rnd, new_colors=False):\n    Adj_matrices, node_features, GT_attn, graph_labels, N_edges = ([], [], [], [], [])\n    n_labels = args.label_max - args.label_min + 1\n    n_graphs_per_shape = int(np.ceil(N_graphs / (N_max - N_min + 1) / n_labels) * n_labels)\n    for n_nodes in np.array(range(N_min, N_max + 1)):\n        c = 0\n        while True:\n            labels = np.arange(args.label_min, n_labels)\n            labels = labels[labels <= n_nodes]\n            rnd.shuffle(labels)\n            for lbl in labels:\n                features, idx_non_zero, gt_attn = get_node_features_Colors(N_nodes=n_nodes, N_green=lbl, dim=dim, green_ch_index=args.green_ch_index, new_colors=new_colors)\n                n_edges = int((rnd.rand() + 1) * n_nodes)\n                A = nx.to_numpy_array(nx.gnm_random_graph(n_nodes, n_edges))\n                add = True\n                for k in range(len(Adj_matrices)):\n                    if A.shape[0] == Adj_matrices[k].shape[0] and np.allclose(A, Adj_matrices[k]):\n                        if np.allclose(node_features[k], features):\n                            add = False\n                            break\n                if add:\n                    Adj_matrices.append(A.astype(np.bool))\n                    graph_labels.append(lbl)\n                    node_features.append(features.astype(np.bool))\n                    GT_attn.append(gt_attn)\n                    N_edges.append(n_edges)\n                    c += 1\n                    if c >= n_graphs_per_shape:\n                        break\n            if c >= n_graphs_per_shape:\n                break\n    graph_labels = np.array(graph_labels, np.int32)\n    N_edges = np.array(N_edges, np.int32)\n    print(N_graphs, len(graph_labels))\n    return {'Adj_matrices': Adj_matrices, 'GT_attn': GT_attn, 'graph_labels': graph_labels, 'node_features': node_features, 'N_edges': N_edges}\n\ndef get_gt_atnn_triangles(args):\n    G, N = args\n    node_ids = []\n    if G is not None:\n        for clq in nx.enumerate_all_cliques(G):\n            if len(clq) == 3:\n                node_ids.extend(clq)\n    node_ids = np.array(node_ids)\n    gt_attn = np.zeros((N, 1), np.int32)\n    for i in np.unique(node_ids):\n        gt_attn[i] = int(np.sum(node_ids == i))\n    return gt_attn\n\ndef get_graph_triangles(args):\n    N_nodes, rnd = args\n    N_edges = int((rnd.rand() + 1) * N_nodes)\n    G = nx.dense_gnm_random_graph(N_nodes, N_edges, seed=None)\n    A = nx.to_numpy_array(G)\n    A_cube = A.dot(A).dot(A)\n    label = int(np.trace(A_cube) / 6.0)\n    return (A.astype(np.bool), label, N_edges, G)\n\ndef generate_graphs_Triangles(N_graphs, N_min, N_max, args, rnd):\n    N_nodes = rnd.randint(N_min, N_max + 1, size=int(N_graphs * 10))\n    print('generating %d graphs with %d-%d nodes' % (N_graphs * 10, N_min, N_max))\n    if args.threads > 0:\n        with mp.Pool(processes=args.threads) as pool:\n            data = pool.map(get_graph_triangles, [(N_nodes[i], rnd) for i in range(len(N_nodes))])\n    else:\n        data = [get_graph_triangles((N_nodes[i], rnd)) for i in range(len(N_nodes))]\n    labels = np.array([data[i][1] for i in range(len(data))], np.int32)\n    Adj_matrices, node_features, G, graph_labels, N_edges, node_degrees = ([], [], [], [], [], [])\n    for lbl in range(args.label_min, args.label_max + 1):\n        idx = np.where(labels == lbl)[0]\n        c = 0\n        for i in idx:\n            add = True\n            for k in range(len(Adj_matrices)):\n                if data[i][0].shape[0] == Adj_matrices[k].shape[0] and labels[i] == graph_labels[k] and np.allclose(data[i][0], Adj_matrices[k]):\n                    add = False\n                    break\n            if add:\n                Adj_matrices.append(data[i][0])\n                graph_labels.append(labels[i])\n                G.append(data[i][3])\n                N_edges.append(data[i][2])\n                node_degrees.append(data[i][0].astype(np.int32).sum(1).max())\n                c += 1\n                if c >= int(N_graphs / (args.label_max - args.label_min + 1)):\n                    break\n        print('label={}, number of graphs={}/{}, total number of generated graphs={}'.format(lbl, c, len(idx), len(Adj_matrices)))\n        assert c == int(N_graphs / (args.label_max - args.label_min + 1)), ('invalid data', c, int(N_graphs / (args.label_max - args.label_min + 1)))\n    print('computing GT attention for %d graphs' % len(Adj_matrices))\n    if args.threads > 0:\n        with mp.Pool(processes=args.threads) as pool:\n            GT_attn = pool.map(get_gt_atnn_triangles, [(G[i], Adj_matrices[i].shape[0]) for i in range(len(Adj_matrices))])\n    else:\n        GT_attn = [get_gt_atnn_triangles((G[i], Adj_matrices[i].shape[0])) for i in range(len(Adj_matrices))]\n    graph_labels = np.array(graph_labels, np.int32)\n    N_edges = np.array(N_edges, np.int32)\n    return {'Adj_matrices': Adj_matrices, 'GT_attn': GT_attn, 'graph_labels': graph_labels, 'N_edges': N_edges, 'Max_degree': np.max(node_degrees)}\nif __name__ == '__main__':\n    dt = datetime.datetime.now()\n    print('start time:', dt)\n    args = parse_args()\n    if not os.path.isdir(args.out_dir):\n        os.mkdir(args.out_dir)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    rnd = np.random.RandomState(args.seed)\n\n    def print_stats(data, split_name):\n        print('%s: %d graphs' % (split_name, len(data['graph_labels'])))\n        for lbl in np.unique(data['graph_labels']):\n            print('%s: label=%d, %d graphs' % (split_name, lbl, np.sum(data['graph_labels'] == lbl)))\n    if args.dataset.lower() == 'colors':\n        data_test_combined, Adj_matrices, node_features = ([], [], [])\n        for N_graphs, N_nodes_min, N_nodes_max, dim, name in zip([args.N_train + args.N_val + args.N_test, args.N_test, args.N_test], [args.N_min, args.N_max_train + 1, args.N_max_train + 1], [args.N_max_train, args.N_max, args.N_max], [args.dim, args.dim, args.dim + 1], ['test orig', 'test large', 'test large-c']):\n            data = generate_graphs_Colors(N_graphs, N_nodes_min, N_nodes_max, dim, args, rnd, new_colors=dim == args.dim + 1)\n            if name.find('orig') >= 0:\n                idx = rnd.permutation(len(data['graph_labels']))\n                data_train = copy_data(data, idx[:args.N_train])\n                print_stats(data_train, name.replace('test', 'train'))\n                node_features += data_train['node_features']\n                Adj_matrices += data_train['Adj_matrices']\n                data_val = copy_data(data, idx[args.N_train:args.N_train + args.N_val])\n                print_stats(data_val, name.replace('test', 'val'))\n                node_features += data_val['node_features']\n                Adj_matrices += data_val['Adj_matrices']\n                data_test = copy_data(data, idx[args.N_train + args.N_val:args.N_train + args.N_val + args.N_test])\n            else:\n                data_test = copy_data(data, rnd.permutation(len(data['graph_labels']))[:args.N_test])\n            Adj_matrices += data_test['Adj_matrices']\n            node_features += data_test['node_features']\n            data_test_combined.append(data_test)\n            print_stats(data_test, name)\n        check_graph_duplicates(Adj_matrices, node_features)\n        with open('%s/random_graphs_colors_dim%d_train.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(data_train, f, protocol=2)\n        with open('%s/random_graphs_colors_dim%d_val.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(data_val, f, protocol=2)\n        with open('%s/random_graphs_colors_dim%d_test.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(concat_data(data_test_combined), f, protocol=2)\n    elif args.dataset.lower() == 'triangles':\n        data = generate_graphs_Triangles(args.N_train + args.N_val + args.N_test, args.N_min, args.N_max_train, args, rnd)\n        idx_train, idx_val, idx_test = ([], [], [])\n        classes = np.unique(data['graph_labels'])\n        n_classes = len(classes)\n        for lbl in classes:\n            idx = np.where(data['graph_labels'] == lbl)[0]\n            rnd.shuffle(idx)\n            n_train = int(args.N_train / n_classes)\n            n_val = int(args.N_val / n_classes)\n            n_test = int(args.N_test / n_classes)\n            idx_train.append(idx[:n_train])\n            idx_val.append(idx[n_train:n_train + n_val])\n            idx_test.append(idx[n_train + n_val:n_train + n_val + n_test])\n        data_train = copy_data(data, np.concatenate(idx_train))\n        print_stats(data_train, 'train orig')\n        data_val = copy_data(data, np.concatenate(idx_val))\n        print_stats(data_val, 'val orig')\n        data_test = copy_data(data, np.concatenate(idx_test))\n        print_stats(data_test, 'test orig')\n        data = generate_graphs_Triangles(args.N_test, args.N_max_train + 1, args.N_max, args, rnd)\n        data_test_large = copy_data(data, rnd.permutation(len(data['graph_labels']))[:args.N_test])\n        print_stats(data_test_large, 'test large')\n        check_graph_duplicates(data_train['Adj_matrices'] + data_val['Adj_matrices'] + data_test['Adj_matrices'] + data_test_large['Adj_matrices'])\n        max_degree = np.max(np.array([d['Max_degree'] for d in (data_train, data_val, data_test, data_test_large)]))\n        data_train['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_train.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_train, f, protocol=2)\n        data_val['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_val.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_val, f, protocol=2)\n        data_test = concat_data((data_test, data_test_large))\n        data_test['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_test.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_test, f, protocol=2)\n    else:\n        raise NotImplementedError('unsupported dataset: ' + args.dataset)\n    print('done in {}'.format(datetime.datetime.now() - dt))\n\n# File: graphdata.py\nimport numpy as np\nimport os\nfrom os.path import join as pjoin\nimport pickle\nimport copy\nimport torch\nimport torch.utils\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nfrom scipy.spatial.distance import cdist\nfrom utils import *\n\ndef compute_adjacency_matrix_images(coord, sigma=0.1):\n    coord = coord.reshape(-1, 2)\n    dist = cdist(coord, coord)\n    A = np.exp(-dist / (sigma * np.pi) ** 2)\n    A[np.diag_indices_from(A)] = 0\n    return A\n\ndef precompute_graph_images(img_size):\n    col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))\n    coord = np.stack((col, row), axis=2) / img_size\n    A = torch.from_numpy(compute_adjacency_matrix_images(coord)).float().unsqueeze(0)\n    coord = torch.from_numpy(coord).float().unsqueeze(0).view(1, -1, 2)\n    mask = torch.ones(1, img_size * img_size, dtype=torch.uint8)\n    return (A, coord, mask)\n\ndef collate_batch_images(batch, A, mask, use_mean_px=True, coord=None, gt_attn_threshold=0, replicate_features=True):\n    B = len(batch)\n    C, H, W = batch[0][0].shape\n    N_nodes = H * W\n    params_dict = {'N_nodes': torch.zeros(B, dtype=torch.long) + N_nodes, 'node_attn_eval': None}\n    has_WS_attn = len(batch[0]) > 2\n    if has_WS_attn:\n        WS_attn = torch.from_numpy(np.stack([batch[b][2].reshape(N_nodes) for b in range(B)]).astype(np.float32)).view(B, N_nodes)\n        WS_attn = normalize_batch(WS_attn)\n        params_dict.update({'node_attn': WS_attn})\n    if use_mean_px:\n        x = torch.stack([batch[b][0].view(C, N_nodes).t() for b in range(B)]).float()\n        if gt_attn_threshold == 0:\n            GT_attn = (x > 0).view(B, N_nodes).float()\n        else:\n            GT_attn = x.view(B, N_nodes).float().clone()\n            GT_attn[GT_attn < gt_attn_threshold] = 0\n        GT_attn = normalize_batch(GT_attn)\n        params_dict.update({'node_attn_eval': GT_attn})\n        if not has_WS_attn:\n            params_dict.update({'node_attn': GT_attn})\n    else:\n        raise NotImplementedError('this case is not well supported')\n    if coord is not None:\n        if use_mean_px:\n            x = torch.cat((x, coord.expand(B, -1, -1)), dim=2)\n        else:\n            x = coord.expand(B, -1, -1)\n    if x is None:\n        x = torch.ones(B, N_nodes, 1)\n    if replicate_features:\n        x = F.pad(x, (2, 0), 'replicate')\n    try:\n        labels = torch.Tensor([batch[b][1] for b in range(B)]).long()\n    except:\n        labels = torch.stack([batch[b][1] for b in range(B)]).long()\n    return [x, A.expand(B, -1, -1), mask.expand(B, -1), labels, params_dict]\n\ndef collate_batch(batch):\n    \"\"\"\n    Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to\n    the maximum number of nodes in the current batch rather than in the entire dataset.\n    \"\"\"\n    B = len(batch)\n    N_nodes = [batch[b][2] for b in range(B)]\n    C = batch[0][0].shape[1]\n    N_nodes_max = int(np.max(N_nodes))\n    mask = torch.zeros(B, N_nodes_max, dtype=torch.bool)\n    A = torch.zeros(B, N_nodes_max, N_nodes_max)\n    x = torch.zeros(B, N_nodes_max, C)\n    has_GT_attn = len(batch[0]) > 4 and batch[0][4] is not None\n    if has_GT_attn:\n        GT_attn = torch.zeros(B, N_nodes_max)\n    has_WS_attn = len(batch[0]) > 5 and batch[0][5] is not None\n    if has_WS_attn:\n        WS_attn = torch.zeros(B, N_nodes_max)\n    for b in range(B):\n        x[b, :N_nodes[b]] = batch[b][0]\n        A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]\n        mask[b][:N_nodes[b]] = 1\n        if has_GT_attn:\n            GT_attn[b, :N_nodes[b]] = batch[b][4].squeeze()\n        if has_WS_attn:\n            WS_attn[b, :N_nodes[b]] = batch[b][5].squeeze()\n    N_nodes = torch.from_numpy(np.array(N_nodes)).long()\n    params_dict = {'N_nodes': N_nodes}\n    if has_WS_attn:\n        params_dict.update({'node_attn': WS_attn})\n    if has_GT_attn:\n        params_dict.update({'node_attn_eval': GT_attn})\n        if not has_WS_attn:\n            params_dict.update({'node_attn': GT_attn})\n    elif has_WS_attn:\n        params_dict.update({'node_attn_eval': WS_attn})\n    labels = torch.from_numpy(np.array([batch[b][3] for b in range(B)])).long()\n    return [x, A, mask, labels, params_dict]\n\nclass MNIST(torchvision.datasets.MNIST):\n    \"\"\"\n    Wrapper around MNIST to use predefined attention coefficients\n    \"\"\"\n\n    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, attn_coef=None):\n        super(MNIST, self).__init__(root, train, transform, target_transform, download)\n        self.alpha_WS = None\n        if attn_coef is not None and train:\n            print('loading weakly-supervised labels from %s' % attn_coef)\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print(train, len(self.alpha_WS))\n\n    def __getitem__(self, index):\n        img, target = super(MNIST, self).__getitem__(index)\n        if self.alpha_WS is None:\n            return (img, target)\n        else:\n            return (img, target, self.alpha_WS[index])\n\nclass MNIST75sp(torch.utils.data.Dataset):\n\n    def __init__(self, data_dir, split, use_mean_px=True, use_coord=True, gt_attn_threshold=0, attn_coef=None):\n        self.data_dir = data_dir\n        self.split = split\n        self.is_test = split.lower() in ['test', 'val']\n        with open(pjoin(data_dir, 'mnist_75sp_%s.pkl' % split), 'rb') as f:\n            self.labels, self.sp_data = pickle.load(f)\n        self.use_mean_px = use_mean_px\n        self.use_coord = use_coord\n        self.n_samples = len(self.labels)\n        self.img_size = 28\n        self.gt_attn_threshold = gt_attn_threshold\n        self.alpha_WS = None\n        if attn_coef is not None and (not self.is_test):\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print('using weakly-supervised labels from %s (%d samples)' % (attn_coef, len(self.alpha_WS)))\n\n    def train_val_split(self, samples_idx):\n        self.sp_data = [self.sp_data[i] for i in samples_idx]\n        self.labels = self.labels[samples_idx]\n        self.n_samples = len(self.labels)\n\n    def precompute_graph_data(self, replicate_features, threads=0):\n        print('precompute all data for the %s set...' % self.split.upper())\n        self.Adj_matrices, self.node_features, self.GT_attn, self.WS_attn = ([], [], [], [])\n        for index, sample in enumerate(self.sp_data):\n            mean_px, coord = sample[:2]\n            coord = coord / self.img_size\n            A = compute_adjacency_matrix_images(coord)\n            N_nodes = A.shape[0]\n            x = None\n            if self.use_mean_px:\n                x = mean_px.reshape(N_nodes, -1)\n            if self.use_coord:\n                coord = coord.reshape(N_nodes, 2)\n                if self.use_mean_px:\n                    x = np.concatenate((x, coord), axis=1)\n                else:\n                    x = coord\n            if x is None:\n                x = np.ones(N_nodes, 1)\n            if replicate_features:\n                x = np.pad(x, ((0, 0), (2, 0)), 'edge')\n            if self.gt_attn_threshold == 0:\n                gt_attn = (mean_px > 0).astype(np.float32)\n            else:\n                gt_attn = mean_px.copy()\n                gt_attn[gt_attn < self.gt_attn_threshold] = 0\n            self.GT_attn.append(normalize(gt_attn))\n            if self.alpha_WS is not None:\n                self.WS_attn.append(normalize(self.alpha_WS[index]))\n            self.node_features.append(x)\n            self.Adj_matrices.append(A)\n\n    def __len__(self):\n        return self.n_samples\n\n    def __getitem__(self, index):\n        data = [self.node_features[index], self.Adj_matrices[index], self.Adj_matrices[index].shape[0], self.labels[index], self.GT_attn[index]]\n        if self.alpha_WS is not None:\n            data.append(self.WS_attn[index])\n        data = list_to_torch(data)\n        return data\n\nclass SyntheticGraphs(torch.utils.data.Dataset):\n\n    def __init__(self, data_dir, dataset, split, degree_feature=True, attn_coef=None, threads=12):\n        self.is_test = split.lower() in ['test', 'val']\n        self.split = split\n        self.degree_feature = degree_feature\n        if dataset.find('colors') >= 0:\n            dim = int(dataset.split('-')[1])\n            data_file = 'random_graphs_colors_dim%d_%s.pkl' % (dim, split)\n            is_triangles = False\n            self.feature_dim = dim + 1\n        if dataset.find('triangles') >= 0:\n            data_file = 'random_graphs_triangles_%s.pkl' % split\n            is_triangles = True\n        else:\n            NotImplementedError(dataset)\n        with open(pjoin(data_dir, data_file), 'rb') as f:\n            data = pickle.load(f)\n        for key in data:\n            if not isinstance(data[key], list) and (not isinstance(data[key], np.ndarray)):\n                print(split, key, data[key])\n            else:\n                print(split, key, len(data[key]))\n        self.Node_degrees = [np.sum(A, 1).astype(np.int32) for A in data['Adj_matrices']]\n        if is_triangles:\n            self.feature_dim = data['Max_degree'] + 1\n            self.node_features = []\n            for i in range(len(data['Adj_matrices'])):\n                N = data['Adj_matrices'][i].shape[0]\n                if degree_feature:\n                    D_onehot = np.zeros((N, self.feature_dim))\n                    D_onehot[np.arange(N), self.Node_degrees[i]] = 1\n                else:\n                    D_onehot = np.zeros((N, 1))\n                self.node_features.append(D_onehot)\n            if not degree_feature:\n                self.feature_dim = 1\n        else:\n            self.node_features = []\n            for i in range(len(data['node_features'])):\n                features = data['node_features'][i]\n                if features.shape[1] < self.feature_dim:\n                    features = np.pad(features, ((0, 0), (0, 1)), 'constant')\n                self.node_features.append(features)\n        self.alpha_WS = None\n        if attn_coef is not None and (not self.is_test):\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print('using weakly-supervised labels from %s (%d samples)' % (attn_coef, len(self.alpha_WS)))\n            self.WS_attn = []\n            for index in range(len(self.alpha_WS)):\n                self.WS_attn.append(normalize(self.alpha_WS[index]))\n        N_nodes = np.array([A.shape[0] for A in data['Adj_matrices']])\n        self.Adj_matrices = data['Adj_matrices']\n        self.GT_attn = data['GT_attn']\n        for i in range(len(self.GT_attn)):\n            self.GT_attn[i] = normalize(self.GT_attn[i])\n        self.labels = data['graph_labels'].astype(np.int32)\n        self.classes = np.unique(self.labels)\n        self.n_classes = len(self.classes)\n        R = np.corrcoef(self.labels, N_nodes)[0, 1]\n        degrees = []\n        for i in range(len(self.Node_degrees)):\n            degrees.extend(list(self.Node_degrees[i]))\n        degrees = np.array(degrees, np.int32)\n        print('N nodes avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(N_nodes)))\n        print('N edges avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(data['N_edges'])))\n        print('Node degree avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(degrees)))\n        print('Node features dim: \\t\\t%d' % self.feature_dim)\n        print('N classes: \\t\\t\\t%d' % self.n_classes)\n        print('Correlation of labels with graph size: \\t%.2f' % R)\n        print('Classes: \\t\\t\\t%s' % str(self.classes))\n        for lbl in self.classes:\n            idx = self.labels == lbl\n            print('Class {}: \\t\\t\\t{} samples, N_nodes: avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(lbl, np.sum(idx), *stats(N_nodes[idx])))\n\n    def __len__(self):\n        return len(self.Adj_matrices)\n\n    def __getitem__(self, index):\n        data = [self.node_features[index], self.Adj_matrices[index], self.Adj_matrices[index].shape[0], self.labels[index], self.GT_attn[index]]\n        if self.alpha_WS is not None:\n            data.append(self.WS_attn[index])\n        data = list_to_torch(data)\n        return data\n\nclass GraphData(torch.utils.data.Dataset):\n\n    def __init__(self, datareader, fold_id, split, degree_feature=True, attn_labels=None):\n        self.fold_id = fold_id\n        self.split = split\n        self.w_sup_signal_attn = None\n        print('The degree_feature argument is ignored for this dataset. \\n        It will automatically be set to True if nodes do not have any features. Otherwise it will be set to False')\n        if attn_labels is not None:\n            if isinstance(attn_labels, str) and os.path.isfile(attn_labels):\n                with open(attn_labels, 'rb') as f:\n                    self.w_sup_signal_attn = pickle.load(f)\n            else:\n                self.w_sup_signal_attn = attn_labels\n            for i in range(len(self.w_sup_signal_attn)):\n                alpha = self.w_sup_signal_attn[i]\n                alpha[alpha < 0.001] = 0\n                self.w_sup_signal_attn[i] = normalize(alpha)\n            print(('!!!using weakly supervised labels (%d samples)!!!' % len(self.w_sup_signal_attn)).upper())\n        self.set_fold(datareader.data, fold_id)\n\n    def set_fold(self, data, fold_id):\n        self.total = len(data['targets'])\n        self.N_nodes_max = data['N_nodes_max']\n        self.num_classes = data['num_classes']\n        self.num_features = data['num_features']\n        if self.split in ['train', 'val']:\n            self.idx = data['splits'][self.split][fold_id]\n        else:\n            assert self.split in ['train_val', 'test'], ('unexpected split', self.split)\n            self.idx = data['splits'][self.split]\n        self.labels = np.array(copy.deepcopy([data['targets'][i] for i in self.idx]))\n        self.adj_list = copy.deepcopy([data['adj_list'][i] for i in self.idx])\n        self.features_onehot = copy.deepcopy([data['features_onehot'][i] for i in self.idx])\n        self.N_edges = np.array([A.sum() // 2 for A in self.adj_list])\n        print('%s: %d/%d' % (self.split.upper(), len(self.labels), len(data['targets'])))\n        classes = np.unique(self.labels)\n        for lbl in classes:\n            print('Class %d: \\t\\t\\t%d samples' % (lbl, np.sum(self.labels == lbl)))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        if isinstance(index, str):\n            if index == 'Adj_matrices':\n                return self.adj_list\n            elif index == 'GT_attn':\n                print('Ground truth attention is unavailable for this dataset: weakly-supervised labels will be returned')\n                return self.w_sup_signal_attn\n            elif index == 'graph_labels':\n                return self.labels\n            elif index == 'node_features':\n                return self.features_onehot\n            elif index == 'N_edges':\n                return self.N_edges\n            else:\n                raise KeyError(index)\n        else:\n            data = [self.features_onehot[index], self.adj_list[index], self.adj_list[index].shape[0], self.labels[index], None]\n            if self.w_sup_signal_attn is not None:\n                data.append(self.w_sup_signal_attn[index])\n            data = list_to_torch(data)\n            return data\n\nclass DataReader:\n    \"\"\"\n    Class to read the txt files containing all data of the dataset\n    Should work for any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n    \"\"\"\n\n    def __init__(self, data_dir, N_nodes=None, rnd_state=None, use_cont_node_attr=False, folds=10, fold_id=None):\n        self.data_dir = data_dir\n        self.rnd_state = np.random.RandomState() if rnd_state is None else rnd_state\n        self.use_cont_node_attr = use_cont_node_attr\n        self.N_nodes = N_nodes\n        if os.path.isfile('%s/data.pkl' % data_dir):\n            print('loading data from %s/data.pkl' % data_dir)\n            with open('%s/data.pkl' % data_dir, 'rb') as f:\n                data = pickle.load(f)\n        else:\n            files = os.listdir(self.data_dir)\n            data = {}\n            nodes, graphs = self.read_graph_nodes_relations(list(filter(lambda f: f.find('graph_indicator') >= 0, files))[0])\n            lst = list(filter(lambda f: f.find('node_labels') >= 0, files))\n            if len(lst) > 0:\n                data['features'] = self.read_node_features(lst[0], nodes, graphs, fn=lambda s: int(s.strip()))\n            else:\n                data['features'] = None\n            data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find('_A') >= 0, files))[0], nodes, graphs)\n            data['targets'] = np.array(self.parse_txt_file(list(filter(lambda f: f.find('graph_labels') >= 0, files))[0], line_parse_fn=lambda s: int(float(s.strip()))))\n            if self.use_cont_node_attr:\n                data['attr'] = self.read_node_features(list(filter(lambda f: f.find('node_attributes') >= 0, files))[0], nodes, graphs, fn=lambda s: np.array(list(map(float, s.strip().split(',')))))\n            features, n_edges, degrees = ([], [], [])\n            for sample_id, adj in enumerate(data['adj_list']):\n                N = len(adj)\n                if data['features'] is not None:\n                    assert N == len(data['features'][sample_id]), (N, len(data['features'][sample_id]))\n                n = np.sum(adj)\n                n_edges.append(int(n / 2))\n                if not np.allclose(adj, adj.T):\n                    print(sample_id, 'not symmetric')\n                degrees.extend(list(np.sum(adj, 1)))\n                if data['features'] is not None:\n                    features.append(np.array(data['features'][sample_id]))\n            if data['features'] is not None:\n                features_all = np.concatenate(features)\n                features_min = features_all.min()\n                num_features = int(features_all.max() - features_min + 1)\n                features_onehot = []\n                for i, x in enumerate(features):\n                    feature_onehot = np.zeros((len(x), num_features))\n                    for node, value in enumerate(x):\n                        feature_onehot[node, value - features_min] = 1\n                    if self.use_cont_node_attr:\n                        feature_onehot = np.concatenate((feature_onehot, np.array(data['attr'][i])), axis=1)\n                    features_onehot.append(feature_onehot)\n                if self.use_cont_node_attr:\n                    num_features = features_onehot[0].shape[1]\n            else:\n                degree_max = int(np.max([np.sum(A, 1).max() for A in data['adj_list']]))\n                num_features = degree_max + 1\n                features_onehot = []\n                for A in data['adj_list']:\n                    n = A.shape[0]\n                    D = np.sum(A, 1).astype(np.int)\n                    D_onehot = np.zeros((n, num_features))\n                    D_onehot[np.arange(n), D] = 1\n                    features_onehot.append(D_onehot)\n            shapes = [len(adj) for adj in data['adj_list']]\n            labels = data['targets']\n            labels -= np.min(labels)\n            classes = np.unique(labels)\n            num_classes = len(classes)\n            if not np.all(np.diff(classes) == 1):\n                print('making labels sequential, otherwise pytorch might crash')\n                labels_new = np.zeros(labels.shape, dtype=labels.dtype) - 1\n                for lbl in range(num_classes):\n                    labels_new[labels == classes[lbl]] = lbl\n                labels = labels_new\n                classes = np.unique(labels)\n                assert len(np.unique(labels)) == num_classes, np.unique(labels)\n\n            def stats(x):\n                return (np.mean(x), np.std(x), np.min(x), np.max(x))\n            print('N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(shapes))\n            print('N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(n_edges))\n            print('Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(degrees))\n            print('Node features dim: \\t\\t%d' % num_features)\n            print('N classes: \\t\\t\\t%d' % num_classes)\n            print('Classes: \\t\\t\\t%s' % str(classes))\n            for lbl in classes:\n                print('Class %d: \\t\\t\\t%d samples' % (lbl, np.sum(labels == lbl)))\n            if data['features'] is not None:\n                for u in np.unique(features_all):\n                    print('feature {}, count {}/{}'.format(u, np.count_nonzero(features_all == u), len(features_all)))\n            N_graphs = len(labels)\n            assert N_graphs == len(data['adj_list']) == len(features_onehot), 'invalid data'\n            data['features_onehot'] = features_onehot\n            data['targets'] = labels\n            data['N_nodes_max'] = np.max(shapes)\n            data['num_features'] = num_features\n            data['num_classes'] = num_classes\n            with open('%s/data.pkl' % data_dir, 'wb') as f:\n                pickle.dump(data, f, protocol=2)\n        labels = data['targets']\n        N_graphs = len(labels)\n        shapes = np.array([len(adj) for adj in data['adj_list']])\n        train_ids, val_ids, train_val_ids, test_ids = self.split_ids_shape(np.arange(N_graphs), shapes, N_nodes, folds=folds)\n        splits = {'train': [], 'val': [], 'train_val': train_val_ids, 'test': test_ids}\n        for fold in range(folds):\n            splits['train'].append(train_ids[fold])\n            splits['val'].append(val_ids[fold])\n        data['splits'] = splits\n        self.data = data\n\n    def split_ids_shape(self, ids_all, shapes, N_nodes, folds=1, fold_id=0):\n        if N_nodes > 0:\n            small_graphs_ind = np.where(shapes <= N_nodes)[0]\n            print('{}/{} graphs with at least {} nodes'.format(len(small_graphs_ind), len(shapes), N_nodes))\n            idx = self.rnd_state.permutation(len(small_graphs_ind))\n            if len(idx) > 1000:\n                n = 1000\n            else:\n                n = 500\n            train_val_ids = small_graphs_ind[idx[:n]]\n            test_ids = small_graphs_ind[idx[n:]]\n            large_graphs_ind = np.where(shapes > N_nodes)[0]\n            test_ids = np.concatenate((test_ids, large_graphs_ind))\n        else:\n            idx = self.rnd_state.permutation(len(ids_all))\n            n = len(ids_all) // folds\n            test_ids = ids_all[idx[fold_id * n:(fold_id + 1) * n if fold_id < folds - 1 else -1]]\n            train_val_ids = []\n            for i in ids_all:\n                if i not in test_ids:\n                    train_val_ids.append(i)\n            train_val_ids = np.array(train_val_ids)\n        assert np.all(np.unique(np.concatenate((train_val_ids, test_ids))) == sorted(ids_all)), 'some graphs are missing in the test sets'\n        if folds > 0:\n            print('generating %d-fold cross-validation splits' % folds)\n            train_ids, val_ids = self.split_ids(train_val_ids, folds=folds)\n            for fold in range(folds):\n                ind = np.concatenate((train_ids[fold], val_ids[fold]))\n                print(fold, len(train_ids[fold]), len(val_ids[fold]))\n                assert len(train_ids[fold]) + len(val_ids[fold]) == len(np.unique(ind)) == len(ind) == len(train_val_ids), 'invalid splits'\n        else:\n            train_ids, val_ids = ([], [])\n        return (train_ids, val_ids, train_val_ids, test_ids)\n\n    def split_ids(self, ids, folds=10):\n        n = len(ids)\n        stride = int(np.ceil(n / float(folds)))\n        test_ids = [ids[i:i + stride] for i in range(0, n, stride)]\n        assert np.all(np.unique(np.concatenate(test_ids)) == sorted(ids)), 'some graphs are missing in the test sets'\n        assert len(test_ids) == folds, 'invalid test sets'\n        train_ids = []\n        for fold in range(folds):\n            train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\n            assert len(train_ids[fold]) + len(test_ids[fold]) == len(np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, 'invalid splits'\n        return (train_ids, test_ids)\n\n    def parse_txt_file(self, fpath, line_parse_fn=None):\n        with open(pjoin(self.data_dir, fpath), 'r') as f:\n            lines = f.readlines()\n        data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\n        return data\n\n    def read_graph_adj(self, fpath, nodes, graphs):\n        edges = self.parse_txt_file(fpath, line_parse_fn=lambda s: s.split(','))\n        adj_dict = {}\n        for edge in edges:\n            node1 = int(edge[0].strip()) - 1\n            node2 = int(edge[1].strip()) - 1\n            graph_id = nodes[node1]\n            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])\n            if graph_id not in adj_dict:\n                n = len(graphs[graph_id])\n                adj_dict[graph_id] = np.zeros((n, n))\n            ind1 = np.where(graphs[graph_id] == node1)[0]\n            ind2 = np.where(graphs[graph_id] == node2)[0]\n            assert len(ind1) == len(ind2) == 1, (ind1, ind2)\n            adj_dict[graph_id][ind1, ind2] = 1\n        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]\n        return adj_list\n\n    def read_graph_nodes_relations(self, fpath):\n        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\n        nodes, graphs = ({}, {})\n        for node_id, graph_id in enumerate(graph_ids):\n            if graph_id not in graphs:\n                graphs[graph_id] = []\n            graphs[graph_id].append(node_id)\n            nodes[node_id] = graph_id\n        graph_ids = np.unique(list(graphs.keys()))\n        for graph_id in graph_ids:\n            graphs[graph_id] = np.array(graphs[graph_id])\n        return (nodes, graphs)\n\n    def read_node_features(self, fpath, nodes, graphs, fn):\n        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn)\n        node_features = {}\n        for node_id, x in enumerate(node_features_all):\n            graph_id = nodes[node_id]\n            if graph_id not in node_features:\n                node_features[graph_id] = [None] * len(graphs[graph_id])\n            ind = np.where(graphs[graph_id] == node_id)[0]\n            assert len(ind) == 1, ind\n            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\n            node_features[graph_id][ind[0]] = x\n        node_features_lst = [node_features[graph_id] for graph_id in sorted(list(graphs.keys()))]\n        return node_features_lst\n\n# File: utils.py\nimport numpy as np\nimport os\nimport torch\nimport copy\nfrom graphdata import *\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import roc_auc_score\nimport numbers\nimport random\n\ndef load_save_noise(f, noise_shape):\n    if os.path.isfile(f):\n        print('loading noise from %s' % f)\n        noises = torch.load(f)\n    else:\n        noises = torch.randn(noise_shape, dtype=torch.float)\n        torch.save(noises, f)\n    return noises\n\ndef list_to_torch(data):\n    for i in range(len(data)):\n        if data[i] is None:\n            continue\n        elif isinstance(data[i], np.ndarray):\n            if data[i].dtype == np.bool:\n                data[i] = data[i].astype(np.float32)\n            data[i] = torch.from_numpy(data[i]).float()\n        elif isinstance(data[i], list):\n            data[i] = list_to_torch(data[i])\n    return data\n\ndef data_to_device(data, device):\n    if isinstance(data, dict):\n        keys = list(data.keys())\n    else:\n        keys = range(len(data))\n    for i in keys:\n        if isinstance(data[i], list) or isinstance(data[i], dict):\n            data[i] = data_to_device(data[i], device)\n        elif isinstance(data[i], torch.Tensor):\n            try:\n                data[i] = data[i].to(device)\n            except:\n                print('error', i, data[i], type(data[i]))\n                raise\n    return data\n\ndef count_correct(output, target, N_nodes=None, N_nodes_min=0, N_nodes_max=25):\n    if output.shape[1] == 1:\n        pred = output.round().long()\n    else:\n        pred = output.max(1, keepdim=True)[1]\n    target = target.long().squeeze().cpu()\n    pred = pred.squeeze().cpu()\n    if N_nodes is not None:\n        idx = (N_nodes >= N_nodes_min) & (N_nodes <= N_nodes_max)\n        if idx.sum() > 0:\n            correct = pred[idx].eq(target[idx]).sum().item()\n            for lbl in torch.unique(target, sorted=True):\n                idx_lbl = target[idx] == lbl\n                eq = (pred[idx][idx_lbl] == target[idx][idx_lbl]).float()\n                print('lbl: {}, avg acc: {:2.2f}% ({}/{})'.format(lbl, 100 * eq.mean(), int(eq.sum()), int(idx_lbl.float().sum())))\n            eq = (pred[idx] == target[idx]).float()\n            print('{} <= N_nodes <= {} (min={}, max={}), avg acc: {:2.2f}% ({}/{})'.format(N_nodes_min, N_nodes_max, N_nodes[idx].min(), N_nodes[idx].max(), 100 * eq.mean(), int(eq.sum()), int(idx.sum())))\n        else:\n            correct = 0\n            print('no graphs with nodes >= {} and <= {}'.format(N_nodes_min, N_nodes_max))\n    else:\n        correct = pred.eq(target).sum().item()\n    return correct\n\ndef attn_AUC(alpha_GT, alpha):\n    auc = []\n    if len(alpha) > 0 and alpha_GT is not None and (len(alpha_GT) > 0):\n        for layer in alpha:\n            alpha_gt = np.concatenate([a.flatten() for a in alpha_GT[layer]]) > 0\n            if len(np.unique(alpha_gt)) <= 1:\n                print('Only one class ({}) present in y_true. ROC AUC score is not defined in that case.'.format(np.unique(alpha_gt)))\n                auc.append(np.nan)\n            else:\n                auc.append(100 * roc_auc_score(y_true=alpha_gt, y_score=np.concatenate([a.flatten() for a in alpha[layer]])))\n    return auc\n\ndef stats(arr):\n    return (np.mean(arr), np.std(arr), np.min(arr), np.max(arr))\n\ndef normalize(x, eps=1e-07):\n    return x / (x.sum() + eps)\n\ndef normalize_batch(x, dim=1, eps=1e-07):\n    return x / (x.sum(dim=dim, keepdim=True) + eps)\n\ndef normalize_zero_one(im, eps=1e-07):\n    m1 = im.min()\n    m2 = im.max()\n    return (im - m1) / (m2 - m1 + eps)\n\ndef mse_loss(target, output, reduction='mean', reduce=None):\n    loss = (target.float().squeeze() - output.float().squeeze()) ** 2\n    if reduce is None:\n        if reduction == 'mean':\n            return torch.mean(loss)\n        elif reduction == 'sum':\n            return torch.sum(loss)\n        elif reduction == 'none':\n            return loss\n        else:\n            NotImplementedError(reduction)\n    elif not reduce:\n        return loss\n    else:\n        NotImplementedError('use reduction if reduce=True')\n\ndef shuffle_nodes(batch):\n    x, A, mask, labels, params_dict = batch\n    for b in range(x.shape[0]):\n        idx = np.random.permutation(x.shape[1])\n        x[b] = x[b, idx]\n        A[b] = A[b, :, idx][idx, :]\n        mask[b] = mask[b, idx]\n        if 'node_attn' in params_dict:\n            params_dict['node_attn'][b] = params_dict['node_attn'][b, idx]\n    return [x, A, mask, labels, params_dict]\n\ndef copy_batch(data):\n    data_cp = []\n    for i in range(len(data)):\n        if isinstance(data[i], dict):\n            data_cp.append({key: data[i][key].clone() for key in data[i]})\n        else:\n            data_cp.append(data[i].clone())\n    return data_cp\n\ndef sanity_check(model, data):\n    with torch.no_grad():\n        output1 = model(copy_batch(data))[0]\n        output2 = model(shuffle_nodes(copy_batch(data)))[0]\n        if not torch.allclose(output1, output2, rtol=0.01, atol=0.001):\n            print('WARNING: model outputs different depending on the nodes order', (torch.norm(output1 - output2), torch.max(output1 - output2), torch.max(output1), torch.max(output2)))\n    print('model is checked for nodes shuffling')\n\ndef set_seed(seed, seed_data=None):\n    random.seed(seed)\n    rnd = np.random.RandomState(seed)\n    if seed_data is not None:\n        rnd_data = np.random.RandomState(seed_data)\n    else:\n        rnd_data = rnd\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    return (rnd, rnd_data)\n\ndef compute_feature_stats(model, train_loader, device, n_batches=100):\n    print('computing mean and std of input features')\n    model.eval()\n    x = []\n    with torch.no_grad():\n        for batch_idx, data in enumerate(train_loader):\n            x.append(data[0].data.cpu().numpy())\n            if batch_idx > n_batches:\n                break\n    x = np.concatenate(x, axis=1).reshape(-1, x[0].shape[-1])\n    print('features shape loaded', x.shape)\n    mn = x.mean(axis=0, keepdims=True)\n    sd = x.std(axis=0, keepdims=True)\n    print('mn', mn)\n    print('std', sd)\n    sd[sd < 0.01] = 1\n    print('corrected (non zeros) std', sd)\n    mn = torch.from_numpy(mn).float().to(device).unsqueeze(0)\n    sd = torch.from_numpy(sd).float().to(device).unsqueeze(0)\n    return (mn, sd)\n\ndef copy_data(data, idx):\n    data_new = {}\n    for key in data:\n        if key == 'Max_degree':\n            data_new[key] = data[key]\n            print(key, data_new[key])\n        else:\n            data_new[key] = copy.deepcopy([data[key][i] for i in idx])\n            if key in ['graph_labels', 'N_edges']:\n                data_new[key] = np.array(data_new[key], np.int32)\n            print(key, len(data_new[key]))\n    return data_new\n\ndef concat_data(data):\n    data_new = {}\n    for key in data[0]:\n        if key == 'Max_degree':\n            data_new[key] = np.max(np.array([d[key] for d in data]))\n            print(key, data_new[key])\n        else:\n            if key in ['graph_labels', 'N_edges']:\n                data_new[key] = np.concatenate([d[key] for d in data])\n            else:\n                lst = []\n                for d in data:\n                    lst.extend(d[key])\n                data_new[key] = lst\n            print(key, len(data_new[key]))\n    return data_new",
        "experimental_info": ""
      }
    },
    {
      "title": "Classification-Based Anomaly Detection for General Data",
      "full_text": "Published as a conference paper at ICLR 2020 CLASSIFICATION -BASED ANOMALY DETECTION FOR GENERAL DATA Liron Bergman Yedid Hoshen School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel ABSTRACT Anomaly detection, ﬁnding patterns that substantially deviate from those seen pre- viously, is one of the fundamental problems of artiﬁcial intelligence. Recently, classiﬁcation-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the appli- cability of transformation-based methods to non-image data using random afﬁne transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is exten- sively validated on multiple datasets from different domains. 1 I NTRODUCTION Detecting anomalies in perceived data is a key ability for humans and for artiﬁcial intelligence. Hu- mans often detect anomalies to give early indications of danger or to discover unique opportunities. Anomaly detection systems are being used by artiﬁcial intelligence to discover credit card fraud, for detecting cyber intrusion, alert predictive maintenance of industrial equipment and for discovering attractive stock market opportunities. The typical anomaly detection setting is a one class classi- ﬁcation task, where the objective is to classify data as normal or anomalous. The importance of the task stems from being able to raise an alarm when detecting a different pattern from those seen in the past, therefore triggering further inspection. This is fundamentally different from supervised learning tasks, in which examples of all data classes are observed. There are different possible scenarios for anomaly detection methods. In supervised anomaly de- tection, we are given training examples of normal and anomalous patterns. This scenario can be quite well speciﬁed, however obtaining such supervision may not be possible. For example in cyber security settings, we will not have supervised examples of new, unknown computer viruses making supervised training difﬁcult. On the other extreme, fully unsupervised anomaly detection, obtains a stream of data containing normal and anomalous patterns and attempts to detect the anomalous data. In this work we deal with the semi-supervised scenario. In this setting, we have a training set of normal examples (which contains no anomalies). After training the anomaly detector, we detect anomalies in the test data, containing both normal and anomalous examples. This supervision is easy to obtain in many practical settings and is less difﬁcult than the fully-unsupervised case. Many anomaly detection methods have been proposed over the last few decades. They can be broadly classiﬁed into reconstruction and statistically based methods.Recently, deep learning meth- ods based on classiﬁcation have achieved superior results. Most semi-supervised classiﬁcation- based methods attempt to solve anomaly detection directly, despite only having normal training data. One example is: Deep-SVDD (Ruff et al., 2018) - one-class classiﬁcation using a learned deep space. Another type of classiﬁcation-based methods is self-supervised i.e. methods that solve one or more classiﬁcation-based auxiliary tasks on the normal training data, and this is shown to be useful for solving anomaly detection, the task of interest e.g. (Golan & El-Yaniv, 2018). Self-supervised classiﬁcation-based methods have been proposed with the object of image anomaly detection, but we show that by generalizing the class of transformations they can apply to all data types. In this paper, we introduce a novel technique, GOAD, for anomaly detection which uniﬁes current state-of-the-art methods that use normal training data only and are based on classiﬁcation. Our method ﬁrst transforms the data into M subspaces, and learns a feature space such that inter-class 1 arXiv:2005.02359v1  [cs.LG]  5 May 2020Published as a conference paper at ICLR 2020 separation is larger than intra-class separation. For the learned features, the distance from the cluster center is correlated with the likelihood of anomaly. We use this criterion to determine if a new data point is normal or anomalous. We also generalize the class of transformation functions to include afﬁne transformation which allows our method to generalize to non-image data. This is signiﬁcant as tabular data is probably the most important for applications of anomaly detection. Our method is evaluated on anomaly detection on image and tabular datasets (cyber security and medical) and is shown to signiﬁcantly improve over the state-of-the-art. 1.1 P REVIOUS WORKS Anomaly detection methods can be generally divided into the following categories: Reconstruction Methods: Some of the most common anomaly detection methods are reconstruction- based. The general idea behind such methods is that every normal sample should be reconstructed accurately using a limited set of basis functions, whereas anomalous data should suffer from larger reconstruction costs. The choice of features, basis and loss functions differentiates between the different methods. Some of the earliest methods use: nearest neighbors (Eskin et al., 2002), low-rank PCA (Jolliffe, 2011; Cand`es et al., 2011) or K-means (Hartigan & Wong, 1979) as the reconstruction basis. Most recently, neural networks were used (Sakurada & Yairi, 2014; Xia et al., 2015) for learning deep basis functions for reconstruction. Another set of recent methods (Schlegl et al., 2017; Deecke et al., 2018) use GANs to learn a reconstruction basis function. GANs suffer from mode-collapse and are difﬁcult to invert, which limits the performance of such methods. Distributional Methods: Another set of commonly used methods are distribution-based. The main theme in such methods is to model the distribution of normal data. The expectation is that anomalous test data will have low likelihood under the probabilistic model while normal data will have higher likelihoods. Methods differ in the features used to describe the data and the probabilistic model used to estimate the normal distribution. Some early methods used Gaussian or Gaussian mixture models. Such models will only work if the data under the selected feature space satisﬁes the prob- abilistic assumptions implicied by the model. Another set of methods used non-parametric density estimate methods such as kernel density estimate (Parzen, 1962). Recently, deep learning methods (autoencoders or variational autoencoders) were used to learn deep features which are sometimes easier to model than raw features (Yang et al., 2017). DAGMM introduced by Zong et al. (2018) learn the probabilistic model jointly with the deep features therefore shaping the features space to better conform with the probabilistic assumption. Classiﬁcation-Based Methods: Another paradigm for anomaly detection is separation between space regions containing normal data from all other regions. An example of such approach is One-Class SVM (Scholkopf et al., 2000), which trains a classiﬁer to perform this separation. Learning a good feature space for performing such separation is performed both by the classic kernel methods as well as by the recent deep learning approach (Ruff et al., 2018). One of the main challenges in unsupervised (or semi-supervised) learning is providing an objective for learning features that are relevant to the task of interest. One method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. Auxiliary tasks for learning high-quality image features include: video frame prediction (Mathieu et al., 2016), image colorization (Zhang et al., 2016; Larsson et al., 2016), puzzle solving (Noroozi & Favaro, 2016) - predicting the correct order of random permuted image patches. Recently, Gidaris et al. (2018) used a set of image processing transformations (rotation by 0,90,180,270 degrees around the image axis, and predicted the true image orientation has been used to learn high-quality image features. Golan & El-Yaniv (2018), have used similar image-processing task prediction for detecting anomalies in images. This method has shown good performance on detecting images from anomalous classes. In this work, we overcome some of the limitations of previous classiﬁcation-based methods and extend their applicability of self-supervised methods to general data types. We also show that our method is more robust to adversarial attacks. 2 C LASSIFICATION -BASED ANOMALY DETECTION Classiﬁcation-based methods have dominated supervised anomaly detection. In this section we will analyse semi-supervised classiﬁcation-based methods: 2Published as a conference paper at ICLR 2020 Let us assume all data lies in spaceRL (where Lis the data dimension). Normal data lie in subspace X ⊂RL. We assume that all anomalies lie outsideX. To detect anomalies, we would therefore like to build a classiﬁer C, such that C(x) = 1if x∈X and C(x) = 0if x∈RL\\X. One-class classiﬁcation methods attempt to learn C directly as P(x ∈X). Classical approaches have learned a classiﬁer either in input space or in a kernel space. Recently, Deep-SVDD (Ruff et al., 2018) learned end-to-end to i) transform the data to an isotropic feature space f(x) ii) ﬁt the minimal hypersphere of radius Rand center c0 around the features of the normal training data. Test data is classiﬁed as anomalous if the following normality score is positive: ∥f(x) −c0∥2 −R2. Learning an effective feature space is not a simple task, as the trivial solution of f(x) = 0 ∀ x results in the smallest hypersphere, various tricks are used to avoid this possibility. Geometric-transformation classiﬁcation (GEOM), proposed by Golan & El-Yaniv (2018) ﬁrst trans- forms the normal data subspace X into M subspaces X1..XM . This is done by transforming each image x ∈X using M different geometric transformations (rotation, reﬂection, translation) into T(x,1)..T(x,M). Although these transformations are image speciﬁc, we will later extend the class of transformations to all afﬁne transformations making this applicable to non-image data. They set an auxiliary task of learning a classiﬁer able to predict the transformation labelmgiven transformed data point T(x,m). As the training set consists of normal data only, each sample is x ∈X and the transformed sample is in ∪mXm. The method attempts to estimate the following conditional probability: P(m′|T(x,m)) = P(T(x,m) ∈Xm′ )P(m′)∑ ˜m P(T(x,m) ∈X˜m)P( ˜m) = P(T(x,m) ∈Xm′ )∑ ˜m P(T(x,m) ∈X˜m) (1) Where the second equality follows by design of the training set, and where every training sample is transformed exactly once by each transformation leading to equal priors. For anomalous data x∈RL\\X, by construction of the subspace, if the transformations T are one- to-one, it follows that the transformed sample does not fall in the appropriate subspace: T(x,m) ∈ RL\\Xm. GEOM uses P(m|T(x,m)) as a score for determining if x is anomalous i.e. that x ∈ RL\\X. GEOM gives samples with low probabilities P(m|T(x,m)) high anomaly scores. A signiﬁcant issue with this methodology, is that the learned classiﬁer P(m′|T(x,m)) is only valid for samples x ∈X which were found in the training set. For x ∈RL\\X we should in fact have P(T(x,m) ∈Xm′ ) = 0for all m= 1..M (as the transformed xis not in any of the subsets). This makes the anomaly score P(m′|T(x,m)) have very high variance for anomalies. One way to overcome this issue is by using examples of anomaliesxa and training P(m|T(x,m)) = 1 M on anomalous data. This corresponds to the supervised scenario and was recently introduced as Outlier Exposure (Hendrycks et al., 2018). Although getting such supervision is possible for some image tasks (where large external datasets can be used) this is not possible in the general case e.g. for tabular data which exhibits much more variation between datasets. 3 D ISTANCE -BASED MULTIPLE TRANSFORMATION CLASSIFICATION We propose a novel method to overcome the generalization issues highlighted in the previous section by using ideas from open-set classiﬁcation (Bendale & Boult, 2016). Our approach uniﬁes one-class and transformation-based classiﬁcation methods. Similarly to GEOM, we transform Xto X1..XM . We learn a feature extractor f(x) using a neural network, which maps the original input data into a feature representation. Similarly to deep OC methods, we model each subspace Xm mapped to the feature space {f(x)|x∈Xm}as a sphere with center cm. The probability of data point xafter transformation mis parameterized by P(T(x,m) ∈X′ m) = 1 Z e−(f(T(x,m))−c′ m)2 . The classiﬁer predicting transformation mgiven a transformed point is therefore: P(m′|T(x,m)) = e−∥f(T(x,m))−cm′ ∥2 ∑ ˜m e−∥f(T(x,m))−c ˜m∥2 (2) The centers cm are given by the average feature over the training set for every transformation i.e. cm = 1 N ∑ x∈X f(T(x,m)). One option is to directly learn f by optimizing cross-entropy between 3Published as a conference paper at ICLR 2020 P(m′|T(x,m)) and the correct label on the normal training set. In practice we obtained better results by training f using the center triplet loss (He et al., 2018), which learns supervised clus- ters with low intra-class variation, and high-inter-class variation by optimizing the following loss function (where sis a margin regularizing the distance between clusters): L= ∑ i max(∥f(T(xi,m)) −cm∥2 + s−minm′̸=m∥f(T(xi,m)) −cm′ ∥2,0) (3) Having learned a feature space in which the different transformation subspaces are well separated, we use the probability in Eq. 2 as a normality score. However, for data far away from the normal distributions, the distances from the means will be large. A small difference in distance will make the classiﬁer unreasonably certain of a particular transformation. To add a general prior for uncer- tainty far from the training set, we add a small regularizing constant ϵ to the probability of each transformation. This ensures equal probabilities for uncertain regions: ˜P(m′|T(x,m)) = e−∥f(T(x,m))−cm′ ∥2 + ϵ∑ ˜m e−∥f(T(x,m))−c ˜m∥2 + M ·ϵ (4) At test time we transform each sample by the M transformations. By assuming independence be- tween transformations, the probability thatxis normal (i.e. x∈X) is the product of the probabilities that all transformed samples are in their respective subspace. For log-probabilities the total score is given by: Score(x) =−log P(x∈X) =− ∑ m log ˜P(T(x,m) ∈Xm) =− ∑ m log ˜P(m|T(x,m)) (5) The score computes the degree of anomaly of each sample. Higher scores indicate a more anomalous sample. Algorithm 1:GOAD: Training Algorithm Input: Normal training data x1,x2...xN Transformations T(,1),T(,2)...T(,M) Output: Feature extractor f, centers c1,c2...cM T(xi,1),T(xi,2)...T(xi,M) ←xi // Transform each sample by all transformations 1 to M Find f,c1,c2...cM that optimize the triplet loss in Eq. 3 Algorithm 2:GOAD: Evaluation Algorithm Input: Test sample: x, feature extractor: f, centers: c1,c2...cM , transformations: T(,1),T(,2)...T(,M) Output: Score(x) T(x,1),T(x,2)...T(x,M) ←x // Transform test sample by all transformations 1 to M P(m|T(x,m)) ←f(T(x,m)),c1,c2...cM // Likelihood of predicting the correct transformation (Eq. 4) Score(x) ←P(1|T(x,1)),P(2|T(x,2))...P(M|T(x,M)) // Aggregate probabilities to compute anomaly score (Eq. 5) 4 P ARAMETERIZING THE SET OF TRANSFORMATIONS Geometric transformations have been used previously for unsupervised feature learning by Gidaris et al. (2018) as well as by GEOM (Golan & El-Yaniv, 2018) for classiﬁcation-based anomaly de- tection. This set of transformations is hand-crafted to work well with convolutional neural networks (CNNs) which greatly beneﬁt from preserving neighborhood between pixels. This is however not a requirement for fully-connected networks. 4Published as a conference paper at ICLR 2020 Anomaly detection often deals with non-image datasets e.g. tabular data. Tabular data is very commonly used on the internet e.g. for cyber security or online advertising. Such data consists of both discrete and continuous attributes with no particular neighborhoods or order. The data is one- dimensional and rotations do not naturally generalize to it. To allow transformation-based methods to work on general data types, we therefore need to extend the class of transformations. We propose to generalize the set of transformations to the class of afﬁne transformations (where we have a total of M transformations): T(x,m) =Wmx+ bm (6) It is easy to verify that all geometric transformations in Golan & El-Yaniv (2018) (rotation by a multiple of 90 degrees, ﬂips and translations) are a special case of this class ( xin this case is the set of image pixels written as a vector). The afﬁne class is however much more general than mere permutations, and allows for dimensionality reduction, non-distance preservation and random trans- formation by sampling W, bfrom a random distribution. Apart from reduced variance across different dataset types where no apriori knowledge on the cor- rect transformation classes exists, random transformations are important for avoiding adversarial examples. Assume an adversary wishes to change the label of a particular sample from anomalous to normal or vice versa. This is the same as requiring that ˜P(m′|T(x,m)) has low or high proba- bility for m′= m. If T is chosen deterministically, the adversary may create adversarial examples against the known class of transformations (even if the exact network parameters are unknown). Conversely, if T is unknown, the adversary must create adversarial examples that generalize across different transformations, which reduces the effectiveness of the attack. To summarize, generalizing the set of transformations to the afﬁne class allows us to: generalize to non-image data, use an unlimited number of transformations and choose transformations randomly which reduces variance and defends against adversarial examples. 5 E XPERIMENTS We perform experiments to validate the effectiveness of our distance-based approach and the per- formance of the general class of transformations we introduced for non-image data. 5.1 I MAGE EXPERIMENTS Cifar10: To evaluate the performance of our method, we perform experiments on the Cifar10 dataset. We use the same architecture and parameter choices of Golan & El-Yaniv (2018), with our distance-based approach. We use the standard protocol of training on all training images of a single digit and testing on all test images. Results are reported in terms of AUC. In our method, we used a margin of s = 0.1 (we also run GOAD with s = 1, shown in the appendix). Similarly to He et al. (2018), to stabilize training, we added a softmax + cross entropy loss, as well as L2 norm regularization for the extracted features f(x). We compare our method with the deep one- class method of Ruff et al. (2018) as well as Golan & El-Yaniv (2018) without and with Dirichlet weighting. We believe the correct comparison is without Dirichlet post-processing, as we also do not use it in our method. Our distance based approach outperforms the SOTA approach by Golan & El-Yaniv (2018), both with and without Dirichlet (which seems to improve performance on a few classes). This gives evidence for the importance of considering the generalization behavior outside the normal region used in training. Note that we used the same geometric transformations as Golan & El-Yaniv (2018). Random afﬁne matrices did not perform competitively as they are not pixel order preserving, this information is effectively used by CNNs and removing this information hurts performance. This is a special property of CNN architectures and image/time series data. As a rule of thumb, fully-connected networks are not pixel order preserving and can fully utilize random afﬁne matrices. FasionMNIST: In Tab. 2, we present a comparison between our method (GOAD) and the strongest baseline methods (Deep SVDD and GEOM) on the FashionMNIST dataset. We used exactly the same setting as Golan & El-Yaniv (2018). GOAD was run with s = 1. OCSVM and GEOM 5Published as a conference paper at ICLR 2020 Table 1: Anomaly Detection Accuracy on Cifar10 (ROC-AUC%) Class Method Deep-SVDD GEOM (no Dirichlet) GEOM (w. Dirichlet) Ours 0 61.7 ±1.3 76.0 ±0.8 74.7 ±0.4 77.2 ±0.6 1 65.9 ±0.7 83.0 ±1.6 95.7 ±0.0 96.7 ±0.2 2 50.8 ±0.3 79.5 ±0.7 78.1 ±0.4 83.3 ±1.4 3 59.1 ±0.4 71.4 ±0.9 72.4 ±0.5 77.7 ±0.7 4 60.9 ±0.3 83.5 ±1.0 87.8 ±0.2 87.8 ±0.7 5 65.7 ±0.8 84.0 ±0.3 87.8 ±0.1 87.8 ±0.6 6 67.7 ±0.8 78.4 ±0.7 83.4 ±0.5 90.0 ±0.6 7 67.3 ±0.3 89.3 ±0.5 95.5 ±0.1 96.1 ±0.3 8 75.9 ±0.4 88.6 ±0.6 93.3 ±0.0 93.8 ±0.9 9 73.1 ±0.4 82.4 ±0.7 91.3 ±0.1 92.0 ±0.6 Average 64.8 81.6 86.0 88.2 Table 2: Anomaly Detection Accuracy on FashionMNIST (ROC-AUC%) Class Method Deep-SVDD GEOM (no Dirichlet) GEOM (w. Dirichlet) Ours 0 98.2 77.8 ±5.9 99.4 ±0.0 94.1 ±0.9 1 90.3 79.1 ±16.3 97.6 ±0.1 98.5 ±0.3 2 90.7 80.8 ±6.9 91.1 ±0.2 90.8 ±0.4 3 94.2 79.2 ±9.1 89.9 ±0.4 91.6 ±0.9 4 89.4 77.8 ±3.3 92.1 ±0.0 91.4 ±0.3 5 91.8 58.0 ±29.4 93.4 ±0.9 94.8 ±0.5 6 83.4 73.6 ±8.7 83.3 ±0.1 83.4 ±0.4 7 98.8 87.4 ±11.4 98.9 ±0.1 97.9 ±0.4 8 91.9 84.6 ±5.6 90.8 ±0.1 98.9 ±0.1 9 99.0 99.5 ±0.0 99.2 ±0.0 99.2 ±0.3 Average 92.8 79.8 93.5 94.1 with Dirichlet were copied from their paper. We run their method without Dirichlet and presented it in the table (we veriﬁed the implementation by running their code with Dirichlet and replicated the numbers in the paper). It appears that GEOM is quite dependent on Dirichlet for this dataset, whereas we do not use it at all. GOAD outperforms all the baseline methods. Adversarial Robustness:Let us assume an attack model where the attacker knows the architecture and the normal training data and is trying to minimally modify anomalies to look normal. We exam- ine the merits of two settings i) the adversary knows the transformations used (non-random) ii) the adversary uses another set of transformations. To measure the beneﬁt of the randomized transfor- mations, we train three networks A, B, C. Networks A and B use exactly the same transformations but random parameter initialization prior to training. Network C is trained using other randomly se- lected transformations. The adversary creates adversarial examples using PGD (Madry et al., 2017) based on network A (making anomalies appear like normal data). On Cifar10, we randomly selected 8 transformations from the full set of 72 for Aand B, another randomly selected 8 transformations are used for C. We measure the increase of false classiﬁcation rate on the adversarial examples us- ing the three networks. The average increase in performance of classifying transformation correctly on anomalies (causing lower anomaly scores) on the original network A was 12.8%, the transfer performance for B causes an increase by 5.0% on network Bwhich shared the same set of transfor- mation, and 3% on network C that used other rotations. This shows the beneﬁts of using random transformations. 6Published as a conference paper at ICLR 2020 Table 3: Anomaly Detection Accuracy (%) Method Dataset Arrhythmia Thyroid KDD KDDRev F1 Score σ F 1 Score σ F 1 Score σ F 1 Score σ OC-SVM 45.8 38.9 79.5 83.2 E2E-AE 45.9 11.8 0.3 74.5 LOF 50.0 0.0 52.7 0.0 83.8 5.2 81.6 3.6 DAGMM 49.8 47.8 93.7 93.8 FB-AE 51.5 1.6 75.0 0.8 92.7 0.3 95.9 0.4 GOAD(Ours) 52.0 2.3 74.5 1.1 98.4 0.2 98.9 0.3 5.2 T ABULAR DATA EXPERIMENTS Datasets: We evaluate on small-scale medical datasets Arrhythmia, Thyroid as well as large-scale cyber intrusion detection datasets KDD and KDDRev. Our conﬁguration follows that of Zong et al. (2018). Categorical attributes are encoded as one-hot vectors. For completeness the datasets are described in the appendix A.2. We train all compared methods on 50% of the normal data. The methods are evaluated on 50% of the normal data as well as all the anomalies. Baseline methods: The baseline methods evaluated are: One-Class SVM (OC-SVM, Scholkopf et al. (2000)), End-to-End Autoencoder (E2E-AE), Local Outlier Factor (LOF, Breunig et al. (2000)). We also evaluated deep distributional method DAGMM (Zong et al., 2018), choosing their strongest variant. To compare against ensemble methods e.g. Chen et al. (2017), we implemented the Fea- ture Bagging Autoencoder (FB-AE) with autoencoders as the base classiﬁer, feature bagging as the source of randomization, and average reconstruction error as the anomaly score. OC-SVM, E2E-AE and DAGMM results are directly taken from those reported by Zong et al. (2018). LOF and FB-AE were computed by us. Implementation of GOAD: We randomly sampled transformation matrices using the normal distri- bution for each element. Each matrix has dimensionality L×r, where Lis the data dimension and ris a reduced dimension. For Arryhthmia and Thyroid we used r = 32, for KDD and KDDrev we used r = 128and r = 64respectively, the latter due to high memory requirements. We used 256 tasks for all datasets apart from KDD ( 64) due to high memory requirements. We set the bias term to 0. For C we used fully-connected hidden layers and leaky-ReLU activations (8 hidden nodes for the small datasets, 128 and 32 for KDDRev and KDD). We optimized using ADAM with a learning rate of 0.001. Similarly to He et al. (2018), to stabilize the triplet center loss training, we added a softmax + cross entropy loss. We repeated the large-scale experiments 5 times, and the small scale GOAD experiments500 times (due to the high variance). We report the mean and standard deviation (σ). Following the protocol in Zong et al. (2018), the decision threshold value is chosen to result in the correct number of anomalies e.g. if the test set contains Na anomalies, the threshold is selected so that the highest Na scoring examples are classiﬁed as anomalies. True positives and negatives are evaluated in the usual way. Some experiments copied from other papers did not measure standard variation and we kept the relevant cell blank. Results Arrhythmia: The Arrhythmia dataset was the smallest examined. A quantitative comparison on this dataset can be seen in Tab. 3. OC-SVM and DAGMM performed reasonably well. Our method is comparable to FB-AE. A linear classiﬁer C performed better than deeper networks (which suffered from overﬁtting). Early stopping after a single epoch generated the best results. Thyroid: Thyroid is a small dataset, with a low anomaly to normal ratio and low feature dimension- ality. A quantitative comparison on this dataset can be seen in Tab. 3. Most baselines performed about equally well, probably due to the low dimensionality. On this dataset, we also found that early stopping after a single epoch gave the best results. The best results on this dataset, were obtained with a linear classiﬁer. Our method is comparable to FB-AE and beat all other baselines by a wide margin. 7Published as a conference paper at ICLR 2020 Figure 1: Left: Classiﬁcation error for our method and DAGMM as a function of percentage of the anomalous examples in the training set (on the KDDCUP99 dataset). Our method consistently outperforms the baseline. Right: Classiﬁcation error as a function of the number of transforma- tions (on the KDDRev dataset). The error and instability decrease as a function of the number of transformations. For both, lower is better. KDDCUP99: The UCI KDD10% dataset is the largest dataset examined. A quantitative comparison on this dataset can be seen in Tab. 3. The strongest baselines are FB-AE and DAGMM. Our method signiﬁcantly outperformed all baselines. We found that large datasets have different dynamics from very small datasets. On this dataset, deep networks performed the best. We also, did not need early stopping. The results are reported after 25 epochs. KDD-Rev: The KDD-Rev dataset is a large dataset, but smaller than KDDCUP99 dataset. A quanti- tative comparison on this dataset can be seen in Tab. 3. Similarly to KDDCUP99, the best baselines are FB-AE and DAGMM, where FB-AE signiﬁcantly outperforms DAGMM. Our method signiﬁ- cantly outperformed all baselines. Due to the size of the dataset, we did not need early stopping. The results are reported after 25 epochs. Adversarial Robustness: Due to the large number of transformations and relatively small networks, adversarial examples are less of a problem for tabular data. PGD generally failed to obtain adversar- ial examples on these datasets. On KDD, transformation classiﬁcation accuracy on anomalies was increased by 3.7% for the network the adversarial examples were trained on, 1.3% when transfer- ring to the network with the same transformation and only0.2% on the network with other randomly selected transformations. This again shows increased adversarial robustness due to random transfor- mations. Further Analysis Contaminated Data: This paper deals with the semi-supervised scenario i.e. when the training dataset contains only normal data. In some scenarios, such data might not be available but instead we might have a training dataset that contains a small percentage of anomalies. To evaluate the robustness of our method to this unsupervised scenario, we analysed the KDDCUP99 dataset, when X% of the training data is anomalous. To prepare the data, we used the same normal training data as before and added further anomalous examples. The test data consists of the same proportions as before. The results are shown in Fig. 1. Our method signiﬁcantly outperforms DAGMM for all impurity values, and degrades more graceful than the baseline. This attests to the effectiveness of our approach. Results for the other datasets are presented in Fig. 3, showing similar robustness to contamination. Number of Tasks: One of the advantages of GOAD, is the ability to generate any number of tasks. We present the anomaly detection performance on the KDD-Rev dataset with different numbers of tasks in Fig. 1. We note that a small number of tasks (less than 16) leads to poor results. From 16 tasks, the accuracy remains stable. We found that on the smaller datasets (Thyroid, Arrhythmia) using a larger number of transformations continued to reduce F1 score variance between differently initialized runs (Fig. 2). 8Published as a conference paper at ICLR 2020 6 D ISCUSSION Openset vs. Softmax: The openset-based classiﬁcation presented by GOAD resulted in performance improvement over the closed-set softmax approach on Cifar10 and FasionMNIST. In our experi- ments, it has also improved performance in KDDRev. Arrhythmia and Thyroid were comparable. As a negative result, performance of softmax was better on KDD (F1 = 0.99). Choosing the margin parameter s: GOAD is not particularly sensitive to the choice of margin parameter s, although choosing sthat is too small might cause some instability. We used a ﬁxed value of s= 1in our experiments, and recommend this value as a starting point. Other transformations: GOAD can also work with other types of transformations such as rotations or permutations for tabular data. In our experiments, we observed that these transformation types perform comparably but a little worse than afﬁne transformations. Unsupervised training: Although most of our results are semi-supervised i.e. assume that no anoma- lies exist in the training set, we presented results showing that our method is more robust than strong baselines to a small percentage of anomalies in the training set. We further presented results in other datasets showing that our method degrades gracefully with a small amount of contamination. Our method might therefore be considered in the unsupervised settings. Deep vs. shallow classiﬁers: Our experiments show that for large datasets deep networks are ben- eﬁcial (particularly for the full KDDCUP99), but are not needed for smaller datasets (indicating that deep learning has not beneﬁted the smaller datasets). For performance critical operations, our approach may be used in a linear setting. This may also aid future theoretical analysis of our method. 7 C ONCLUSION In this paper, we presented a method for detecting anomalies for general data. This was achieved by training a classiﬁer on a set of random auxiliary tasks. Our method does not require knowledge of the data domain, and we are able to generate an arbitrary number of random tasks. Our method signiﬁcantly improve over the state-of-the-art. REFERENCES Arthur Asuncion and David Newman. Uci machine learning repository, 2007. Abhijit Bendale and Terrance E Boult. Towards open set deep networks. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 1563–1572, 2016. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J¨org Sander. Lof: identifying density- based local outliers. In ACM sigmod record, volume 29, pp. 93–104. ACM, 2000. Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? JACM, 2011. Jinghui Chen, Saket Sathe, Charu Aggarwal, and Deepak Turaga. Outlier detection with autoencoder ensembles. In ICDM, 2017. Lucas Deecke, Robert Vandermeulen, Lukas Ruff, Stephan Mandt, and Marius Kloft. Anomaly detection with generative adversarial networks. In ICLR, 2018. Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid Portnoy, and Sal Stolfo. A geometric frame- work for unsupervised anomaly detection. In Applications of data mining in computer security , pp. 77–101. Springer, 2002. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. ICLR, 2018. Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. In NeurIPS, 2018. 9Published as a conference paper at ICLR 2020 John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm.Journal of the Royal Statistical Society. Series C (Applied Statistics), 1979. Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai. Triplet-center loss for multi- view 3d object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1945–1954, 2018. Dan Hendrycks, Mantas Mazeika, and Thomas G Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. Ian Jolliffe. Principal component analysis. Springer, 2011. Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for auto- matic colorization. In ECCV, 2016. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. ICLR, 2016. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe- matical statistics, 1962. Shebuti Rayana. ODDS library http://odds.cs.stonybrook.edu, 2016. Lukas Ruff, Nico Gornitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert Vandermeulen, Alexan- der Binder, Emmanuel M¨uller, and Marius Kloft. Deep one-class classiﬁcation. In ICML, 2018. Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimen- sionality reduction. In MLSD. ACM, 2014. Thomas Schlegl, Philipp Seeb ¨ock, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International Conference on Information Processing in Medical Imaging, 2017. Bernhard Scholkopf, Robert C Williamson, Alex J Smola, John Shawe-Taylor, and John C Platt. Support vector method for novelty detection. In NIPS, 2000. Yan Xia, Xudong Cao, Fang Wen, Gang Hua, and Jian Sun. Learning discriminative reconstructions for unsupervised outlier removal. In ECCV, 2015. Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. In ICML, 2017. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. ICLR, 2018. A A PPENDIX A.1 I MAGE EXPERIMENTS Sensitive to margins: We run Cifar10 experiments with s = 0.1 and s = 1 and presented the results in Fig. 4. The results were not affected much by the margin parameter. This is in-line with the rest of our empirical observations that GOAD is not very sensitive to the margin parameter. 10Published as a conference paper at ICLR 2020 Table 4: Anomaly Detection Accuracy on Cifar10 (%) Class Method GEOM (w. Dirichlet) GOAD( s= 0.1) GOAD( 1.0) 0 74.7 ±0.4 77.2 ±0.6 77.9 ±0.7 1 95.7 ±0.0 96.7 ±0.2 96.4 ±0.9 2 78.1 ±0.4 83.3 ±1.4 81.8 ±0.8 3 72.4 ±0.5 77.7 ±0.7 77.0 ±0.7 4 87.8 ±0.2 87.8 ±0.7 87.7 ±0.5 5 87.8 ±0.1 87.8 ±0.6 87.8 ±0.7 6 83.4 ±0.5 90.0 ±0.6 90.9 ±0.5 7 95.5 ±0.1 96.1 ±0.3 96.1 ±0.2 8 93.3 ±0.0 93.8 ±0.9 93.3 ±0.1 9 91.3 ±0.1 92.0 ±0.6 92.4 ±0.3 Average 86.0 88.2 88.1 A.2 T ABULAR DATASETS Following the evaluation protocol of Zong et al. (2018), 4 datasets are used in this comparison: Arrhythmia: A cardiology dataset from the UCI repository (Asuncion & Newman, 2007) contain- ing attributes related to the diagnosis of cardiac arrhythmia in patients. The datasets consists of 16 classes: class 1 are normal patients, 2-15 contain different arrhythmia conditions, and class 16 con- tains undiagnosed cases. Following the protocol established by ODDS (Rayana, 2016), the smallest classes: 3,4,5,7,8,9,14,15 are taken to be anomalous and the rest normal. Also following ODDS, the categorical attributes are dropped, the ﬁnal attributes total 274. Thyroid: A medical dataset from the UCI repository (Asuncion & Newman, 2007), containing at- tributes related to whether a patient is hyperthyroid. Following ODDS (Rayana, 2016), from the 3 classes of the dataset, we designate hyperfunction as the anomalous class and the rest as normal. Also following ODDS only the 6 continuous attributes are used. KDD: The KDD Intrusion Detection dataset was created by an extensive simulation of a US Air Force LAN network. The dataset consists of the normal and 4 simulated attack types: denial of service, unauthorized access from a remote machine, unauthorized access from local superuser and probing. The dataset consists of around 5 million TCP connection records. Following the evaluation protocol in Zong et al. (2018), we use the UCI KDD 10% dataset, which is a subsampled version of the original dataset. The dataset contains 41 different attributes. 34 are continuous and 7 are categorical. Following Zong et al. (2018), we encode the categorical attributes using1-hot encoding. Following Zong et al. (2018), we evaluate two different settings for the KDD dataset: KDDCUP99: In this conﬁguration we use the entire UCI 10% dataset. As the non-attack class consists of only 20% of the dataset, it is treated as the anomaly in this case, while attacks are treated as normal. KDDCUP99-Rev: To better correspond to the actual use-case, in which the non-attack scenario is normal and attacks are anomalous, Zong et al. (2018) also evaluate on the reverse conﬁguration, in which the attack data is sub-sampled to consist of 25% of the number of non-attack samples. The attack data is in this case designated as anomalous (the reverse of the KDDCUP99 dataset). In all the above datasets, the methods are trained on 50% of the normal data. The methods are evaluated on 50% of the normal data as well as all the anomalies. A.3 N UMBER OF TASKS We provide plots of the number of auxiliary tasks vs. the anomaly detection accuracy (measured by F1) for all datasets. The results are presented in Fig. 2. Performance increases rapidly up to a certain number of tasks (around 16). Afterwards more tasks reduce the variance of F1 scores between runs. 11Published as a conference paper at ICLR 2020 a) ‘ b) c) ‘ d) Figure 2: Plots of the number of auxiliary tasks vs. the anomaly detection accuracy (measured by F1) a) Arrhythmia b) Thyroid c) KDDRev d) KDDCup99 Accuracy often increases with the number of tasks, although the rate diminishes with the number of tasks. Figure 3: Plots of the degree of contamination vs. the anomaly detection accuracy (measured byF1) (left) KDDRev (center) KDDCup99 (right) Arrhythmia. GOAD is generally robust to the degree of contamination. A.4 C ONTAMINATION EXPERIMENTS We conduct contamination experiments for 3 datasets. Thyroid was omitted due to not having a sufﬁcient number of anomalies. The protocol is different than that of KDDRev as we do not have unused anomalies for contamination. Instead, we split the anomalies into train and test. Train anomalies are used for contamination, test anomalies are used for evaluation. As DAGMM did not present results for the other datasets, we only present GOAD. GOAD was reasonably robust to contamination on KDD, KDDRev and Arrhythmia. The results are presented in Fig. 3 12",
      "references": [
        "Towards open set deep networks",
        "Lof: identifying density- based local outliers",
        "Robust principal component analysis?",
        "Outlier detection with autoencoder ensembles",
        "Anomaly detection with generative adversarial networks",
        "A geometric frame- work for unsupervised anomaly detection",
        "Unsupervised representation learning by predicting image rotations",
        "Deep anomaly detection using geometric transformations",
        "Algorithm as 136: A k-means clustering algorithm",
        "Triplet-center loss for multi- view 3d object retrieval",
        "Deep anomaly detection with outlier exposure",
        "Principal component analysis",
        "Learning representations for auto- matic colorization",
        "Towards deep learning models resistant to adversarial attacks",
        "Deep multi-scale video prediction beyond mean square error",
        "Unsupervised learning of visual representations by solving jigsaw puzzles",
        "On estimation of a probability density function and mode",
        "Deep one-class classiﬁcation",
        "Anomaly detection using autoencoders with nonlinear dimen- sionality reduction",
        "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery",
        "Support vector method for novelty detection",
        "Learning discriminative reconstructions for unsupervised outlier removal",
        "Towards k-means-friendly spaces: Simultaneous deep learning and clustering",
        "Colorful image colorization",
        "Deep autoencoding gaussian mixture model for unsupervised anomaly detection"
      ],
      "meta_data": {
        "arxiv_id": "2005.02359v1",
        "authors": [
          "Liron Bergman",
          "Yedid Hoshen"
        ],
        "published_date": "2020-05-05T17:44:40Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces GOAD, an open-set, classification-based anomaly detection method that unifies and extends previous methods. It relaxes generalization assumptions by using distance-based metrics and extends transformation-based anomaly detection to non-image data via random affine transformations.",
        "methodology": "GOAD transforms each data sample into multiple subspaces using M random affine (or geometric) transformations and learns a feature space using a neural network trained with a center triplet loss. It models each transformed subspace as a sphere with a computed center and estimates anomaly scores based on the aggregated probability across all transformations, which reflects the distance from the cluster center.",
        "experimental_setup": "The method is validated on multiple datasets from different domains, including image datasets (Cifar10, FashionMNIST) and tabular datasets (Arrhythmia, Thyroid, KDD, KDDRev). It is compared against state-of-the-art baselines such as Deep SVDD, GEOM, DAGMM, and FB-AE. Experiments include standard training/test splits, evaluations using ROC-AUC and F1 score metrics, as well as robustness tests against adversarial attacks and data contamination.",
        "limitations": "The approach may require careful tuning of parameters such as the margin in the triplet loss and the number and type of transformations. The method's performance can be architecture-dependent (e.g., CNNs versus fully-connected networks) and may suffer if non-pixel order preserving transformations are used with CNNs. It also assumes the availability of purely normal training data, although it shows some robustness to data contamination.",
        "future_research_directions": "Future work could explore optimal transformation selection or learning transformations in a data-driven manner, extend the framework to fully unsupervised settings, improve robustness to adversarial attacks, and further analyze the theoretical properties of the model. Additionally, research could focus on scaling the method to diverse data types and further integrating it with ensemble and hybrid anomaly detection techniques.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Improved Techniques for Training Consistency Models",
      "full_text": "IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS Yang Song & Prafulla Dhariwal OpenAI {songyang,prafulla}@openai.com ABSTRACT Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distil- lation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency mod- els learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consis- tency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64 ˆ64 respectively in a single sampling step. These scores mark a 3.5 ˆand 4ˆimprovement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models. 1 I NTRODUCTION Consistency models (Song et al., 2023) are an emerging family of generative models that produce high-quality samples using a single network evaluation. Unlike GANs (Goodfellow et al., 2014), consistency models are not trained with adversarial optimization and thus sidestep the associated training difficulty. Compared to score-based diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; 2020; Ho et al., 2020; Song et al., 2021), consistency models do not require numerous sampling steps to generate high-quality samples. They are trained to generate samples in a single step, but still retain important advantages of diffusion models, such as the flexibility to exchange compute for sample quality through multistep sampling, and the ability to perform zero-shot data editing. We can train consistency models using either consistency distillation (CD) or consistency training (CT). The former requires pre-training a diffusion model and distilling the knowledge therein into a consistency model. The latter allows us to train consistency models directly from data, establishing them as an independent family of generative models. Previous work (Song et al., 2023) demonstrates that CD significantly outperforms CT. However, CD adds computational overhead to the training process since it requires learning a separate diffusion model. Additionally, distillation limits the sample quality of the consistency model to that of the diffusion model. To avoid the downsides of CD and to position consistency models as an independent family of generative models, we aim to improve CT to either match or exceed the performance of CD. For optimal sample quality, both CD and CT rely on learned metrics like the Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018) in previous work (Song et al., 2023). However, depending on LPIPS has two primary downsides. Firstly, there could be potential bias in evaluation arXiv:2310.14189v1  [cs.LG]  22 Oct 2023IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS since the same ImageNet dataset (Deng et al., 2009) trains both LPIPS and the Inception network in Fréchet Inception Distance (FID) (Heusel et al., 2017), which is the predominant metric for image quality. As analyzed in Kynkäänniemi et al. (2023), improvements of FIDs can come from accidental leakage of ImageNet features from LPIPS, causing inflated FID scores. Secondly, learned metrics require pre-training auxiliary networks for feature extraction. Training with these metrics requires backpropagating through extra neural networks, which increases the demand for compute. To tackle these challenges, we introduce improved techniques for CT that not only surpass CD in sample quality but also eliminate the dependence on learned metrics like LPIPS. Our techniques are motivated from both theoretical analysis, and comprehensive experiments on the CIFAR-10 dataset (Krizhevsky et al., 2014). Specifically, we perform an in-depth study on the empirical impact of weighting functions, noise embeddings, and dropout in CT. Additionally, we identify an overlooked flaw in prior theoretical analysis for CT and propose a simple fix by removing the Exponential Moving Average (EMA) from the teacher network. We adopt Pseudo-Huber losses from robust statistics to replace LPIPS. Furthermore, we study how sample quality improves as the number of discretization steps increases, and utilize the insights to propose a simple but effective curriculum for total discretization steps. Finally, we propose a new schedule for sampling noise levels in the CT objective based on lognormal distributions. Taken together, these techniques allow CT to attain FID scores of 2.51 and 3.25 for CIFAR-10 and ImageNet 64 ˆ64 in one sampling step, respectively. These scores not only surpass CD but also represent improvements of 3.5ˆand 4ˆover previous CT methods. Furthermore, they significantly outperform the best few-step diffusion distillation techniques for diffusion models even without the need for distillation. By two-step generation, we achieve improved FID scores of 2.24 and 2.77 on CIFAR-10 and ImageNet 64 ˆ64, surpassing the scores from CD in both one-step and two-step settings. These results rival many top-tier diffusion models and GANs, showcasing the strong promise of consistency models as a new independent family of generative models. 2 C ONSISTENCY MODELS Central to the formulation of consistency models is the probability flow ordinary differential equation (ODE) from Song et al. (2021). Let us denote the data distribution bypdatapxq. When we add Gaussian noise with mean zero and standard deviation σ to this data, the resulting perturbed distribution is given by pσpxq“ ş pdatapyqNpx |y, σ2Iqdy. The probability flow ODE, as presented in Karras et al. (2022), takes the form of dx dσ “´σ∇x log pσpxq σ Prσmin, σmaxs, (1) where the term ∇x log pσpxqis known as the score function of pσpxq(Song et al., 2019; Song & Ermon, 2019; 2020; Song et al., 2021). Here σmin is a small positive value such that pσmin pxq « pdatapxq, introduced to avoid numerical issues in ODE solving. Meanwhile, σmax is sufficiently large so that pσpxq« Np0, σ2 maxIq. Following Karras et al. (2022); Song et al. (2023), we adopt σmin “0.002, and σmax “80 throughout the paper. Crucially, solving the probability flow ODE from noise level σ1 to σ2 allows us to transform a sample xσ1 „pσ1 pxqinto xσ2 „pσ2 pxq. The ODE in Eq. (1) establishes a bijective mapping between a noisy data sample xσ „ pσpxq and xσmin „pσmin pxq «pdatapxq. This mapping, denoted as f˚ : pxσ, σq ÞÑxσmin , is termed the consistency function. By its very definition, the consistency function satisfies the boundary condition f˚px, σminq“ x. A consistency model, which we denote by fθpx, σq, is a neural network trained to approximate the consistency function f˚px, σq. To meet the boundary condition, we follow Song et al. (2023) to parameterize the consistency model as fθpx, σq“ cskippσqx `coutpσqFθpx, σq, (2) where Fθpx, σqis a free-form neural network, while cskippσqand coutpσqare differentiable functions such that cskippσminq“ 1 and coutpσminq“ 0. To train the consistency model, we discretize the probability flow ODE using a sequence of noise levels σmin “σ1 ăσ2 ă¨¨¨ă σN “σmax, where we follow Karras et al. (2022); Song et al. (2023) in setting σi “pσ1{ρ min ` i´1 N´1 pσ1{ρ max ´σ1{ρ min qqρ for i PJ1, NK, and ρ “7, where Ja, bK denotes the set 2IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS of integers ta, a`1, ¨¨¨ , bu. The model is trained by minimizing the following consistency matching (CM) loss over θ: LN pθ, θ´q“ E “ λpσiqdpfθpxσi`1 , σi`1q, fθ´p˘xσi, σiqq ‰ , (3) where ˘xσi “ xσi`1 ´pσi ´σi`1qσi`1∇x log pσi`1 pxq|x“xσi`1 . In Eq. (3), dpx, yqis a metric function comparing vectors x and y, and λpσqą 0 is a weighting function. Typical metric functions include the squared ℓ2 metric dpx, yq“ ∥x ´y∥2 2, and the Learned Perceptual Image Patch Similar- ity (LPIPS) metric introduced in Zhang et al. (2018). The expectation in Eq. (3) is taken over the following sampling process: i „UJ1, N´1K where UJ1, N´1K represents the uniform distribution over t1, 2, ¨¨¨ , N´1u, and xσi`1 „pσi`1 pxq. Note that ˘xσi is derived from xσi`1 by solving the probability flow ODE in the reverse direction for a single step. In Eq. (3), fθ and fθ´ are referred to as the student network and the teacher network, respectively. The teacher’s parameter θ´is obtained by applying Exponential Moving Average (EMA) to the student’s parameterθ during the course of training as follows: θ´ Ðstopgradpµθ´`p1 ´µqθq, (4) with 0 ďµ ă1 representing the EMA decay rate. Here we explicitly employ the stopgrad operator to highlight that the teacher network remains fixed for each optimization step of the student network. However, in subsequent discussions, we will omit thestopgrad operator when its presence is clear and unambiguous. In practice, we also maintain EMA parameters for the student network to achieve better sample quality at inference time. It is clear that asN increases, the consistency model optimized using Eq. (3) approaches the true consistency function. For faster training, Song et al. (2023) propose a curriculum learning strategy where N is progressively increased and the EMA decay rate µ is adjusted accordingly. This curriculum for N and µ is denoted by Npkqand µpkq, where k PN is a non-negative integer indicating the current training step. Given that ˘xσi relies on the unknown score function ∇x log pσi`1 pxq, directly optimizing the consis- tency matching objective in Eq. (3) is infeasible. To circumvent this challenge, Song et al. (2023) propose two training algorithms: consistency distillation (CD) and consistency training (CT). For consistency distillation, we first train a diffusion model sϕpx, σqto estimate ∇x log pσpxqvia score matching (Hyvärinen, 2005; Vincent, 2011; Song et al., 2019; Song & Ermon, 2019), then approximate ˘xσi with ˆxσi “ xσi`1 ´pσi ´σi`1qσi`1sϕpxσi`1 , σi`1q. On the other hand, con- sistency training employs a different approximation method. Recall that xσi`1 “x `σi`1z with x „pdatapxqand z „Np0, Iq. Using the same x and z, Song et al. (2023) define ˇxσi “x `σiz as an approximation to ˘xσi, which leads to the consistency training objective below: LN CTpθ, θ´q“ Erλpσiqdpfθpx `σi`1z, σi`1q, fθ´px `σiz, σiqqs. (5) As analyzed in Song et al. (2023), this objective is asymptotically equivalent to consistency matching in the limit of N Ñ8. We will revisit this analysis in Section 3.2. After training a consistency model fθpx, σqthrough CD or CT, we can directly generate a sample x by starting with z „Np0, σ2 maxIqand computing x “fθpz, σmaxq. Notably, these models also enable multistep generation. For a sequence of indices 1 “i1 ăi2 ă¨¨¨ă iK “N, we start by sampling xK „Np0, σ2 maxIqand then iteratively computexk Ðfθpxk`1, σik`1 q` b σ2 ik ´σ2 minzk for k “K ´1, K´2, ¨¨¨ , 1, where zk „Np0, Iq. The resulting sample x1 approximates the distribution pdatapxq. In our experiments, setting K “3 (two-step generation) often enhances the quality of one-step generation considerably, though increasing the number of sampling steps further provides diminishing benefits. 3 I MPROVED TECHNIQUES FOR CONSISTENCY TRAINING Below we re-examine the design choices of CT in Song et al. (2023) and pinpoint modifications that improve its performance, which we summarize in Table 1. We focus on CT without learned metric functions. For our experiments, we employ the Score SDE architecture in Song et al. (2021) and train the consistency models for 400,000 iterations on the CIFAR-10 dataset (Krizhevsky et al., 2014) without class labels. While our primary focus remains on CIFAR-10 in this section, we observe similar improvements on other datasets, including ImageNet64ˆ64 (Deng et al., 2009). We measure sample quality using Fréchet Inception Distance (FID) (Heusel et al., 2017). 3IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS Table 1: Comparing the design choices for CT in Song et al. (2023) versus our modifications. Design choice in Song et al. (2023) Our modifications EMA decay rate for the teacher network µpkq“expps0logµ0 Npkq q µpkq“0 Metric in consistency lossdpx,yq“LPIPSpx,yq dpx,yq“ b ∥x´y∥2 2 `c2 ´c Discretization curriculumNpkq“Qb k Kpps1 `1q2 ´s20q`s20 ´1 U `1 Npkq“minps02t k K1u, s1q`1, whereK1“ Y K log2ts1{s0u`1 ] Noise schedule σi, wherei„UJ1, Npkq´1K σi, wherei„ppiq, andppiq9 erf`logpσi`1q´Pmean?2Pstd ˘´erf`logpσiq´Pmean?2Pstd ˘ Weighting function λpσiq“1 λpσiq“ 1 σi`1´σi Parameters s0 “2, s1 “150, µ0 “0.9on CIFAR-10 s0 “10, s1 “1280 s0 “2, s1 “200, µ0 “0.95on ImageNet64ˆ64 c“0.00054?d, dis data dimensionality Pmean“´1.1, Pstd“2.0 kPJ0, KK, whereKis the total training iterations σi “pσ1{ρ min` i´1 Npkq´1pσ1{ρmax´σ1{ρ minqqρ, whereiPJ1, NpkqK, ρ“7, σmin“0.002, σmax“80 3.1 W EIGHTING FUNCTIONS , NOISE EMBEDDINGS , AND DROPOUT We start by exploring several hyperparameters that are known to be important for diffusion models, including the weighting function λpσq, the embedding layer for noise levels, and dropout (Ho et al., 2020; Song et al., 2021; Dhariwal & Nichol, 2021; Karras et al., 2022). We find that proper selection of these hyperparameters greatly improve CT when using the squared ℓ2 metric. The default weighting function in Song et al. (2023) is uniform, i.e., λpσq” 1. This assigns equal weights to consistency losses at all noise levels, which we find to be suboptimal. We propose to modify the weighting function so that it reduces as noise levels increase. The rationale is that errors from minimizing consistency losses in smaller noise levels can influence larger ones and therefore should be weighted more heavily. Specifically, our weighting function (cf ., Table 1) is defined as λpσiq “ 1 σi`1´σi . The default choice for σi, given in Section 2, ensures that λpσiq “ 1 σi`1´σi reduces monotonically as σi increases, thus assigning smaller weights to higher noise levels. As shown in Fig. 1c, this refined weighting function notably improves the sample quality in CT with the squared ℓ2 metric. In Song et al. (2023), Fourier embedding layers (Tancik et al., 2020) and positional embedding layers (Vaswani et al., 2017) are used to embed noise levels for CIFAR-10 and ImageNet64 ˆ64 respectively. It is essential that noise embeddings are sufficiently sensitive to minute differences to offer training signals, yet too much sensitivity can lead to training instability. As shown in Fig. 1b, high sensitivity can lead to the divergence of continuous-time CT (Song et al., 2023). This is a known challenge in Song et al. (2023), which they circumvent by initializing the consistency model with parameters from a pre-trained diffusion model. In Fig. 1b, we show continuous-time CT on CIFAR-10 converges with random initial parameters, provided we use a less sensitive noise embedding layer with a reduced Fourier scale parameter, as visualized in Fig. 1a. For discrete-time CT, models are less affected by the sensitivity of the noise embedding layers, but as shown in Fig. 1c, reducing the scale parameter in Fourier embedding layers from the default value of 16.0 to a smaller value of 0.02 still leads to slight improvement of FIDs on CIFAR-10. For ImageNet models, we employ the default positional embedding, as it has similar sensitivity to Fourier embedding with scale 0.02 (see Fig. 1a). Previous experiments with consistency models in Song et al. (2023) always employ zero dropout, motivated by the fact that consistency models generate samples in a single step, unlike diffusion models that do so in multiple steps. Therefore, it is intuitive that consistency models, facing a more challenging task, would be less prone to overfitting and need less regularization than their diffusion counterparts. Contrary to our expectations, we discovered that using larger dropout than diffusion models improves the sample quality of consistency models. Specifically, as shown in Fig. 1c, a dropout rate of 0.3 for consistency models on CIFAR-10 obtains better FID scores. For ImageNet 64 ˆ64, we find it beneficial to apply dropout of 0.2 to layers with resolution less than or equal to 16 ˆ16, following Hoogeboom et al. (2023). We additionally ensure that the random number 4IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) Sensitivity of noise embeddings.  (b) Continuous-time CT.  (c) Ablation study. Figure 1: (a) As the Fourier scale parameter decreases, Fourier noise embeddings become less sensitive to minute noise differences. This sensitivity is closest to that of positional embeddings when the Fourier scale is set to 0.02. (b) Continuous-time CT diverges when noise embeddings are overly sensitive to minor noise differences. (c) An ablation study examines the effects of our selections for weighting function ( 1 σi`1´σi ), noise embedding (Fourier scale “0.02), and dropout (“0.3) on CT using the squared ℓ2 metric. Here baseline models for both metrics follow configurations in Song et al. (2023). All models are trained on CIFAR-10 without class labels. generators for dropout share the same states across the student and teacher networks when optimizing the CT objective in Eq. (5). By choosing the appropriate weighting function, noise embedding layers, and dropout, we signifi- cantly improve the sample quality of consistency models using the squared ℓ2 metric, closing the gap with the original CT in Song et al. (2023) that relies on LPIPS (see Fig. 1c). Although our modifications do not immediately improve the sample quality of CT with LPIPS, combining with additional techniques in Section 3.2 will yield significant improvements for both metrics. 3.2 R EMOVING EMA FOR THE TEACHER NETWORK When training consistency models, we minimize the discrepancy between models evaluated at adjacent noise levels. Recall from Section 2 that the model with the lower noise level is termed the teacher network, and its counterpart the student network. While Song et al. (2023) maintains EMA parameters for both networks with potentially varying decay rates, we present a theoretical argument indicating that the EMA decay rate for the teacher network should always be zero for CT, although it can be nonzero for CD. We revisit the theoretical analysis in Song et al. (2023) to support our assertion and provide empirical evidence that omitting EMA from the teacher network in CT notably improves the sample quality of consistency models. To support the use of CT, Song et al. (2023) present two theoretical arguments linking the CT and CM objectives as N Ñ 8. The first line of reasoning, which we call Argument (i), draws upon Theorem 2 from Song et al. (2023) to show that under certain regularity conditions, LN CTpθ, θ´q“ LN pθ, θ´q` op∆σq. That is, when N Ñ8, we have ∆σ Ñ0 and hence LN CTpθ, θ´qconverges to LN pθ, θ´qasymptotically. The second argument, called Argument (ii), is grounded in Theorem 6 from Song et al. (2023) which asserts that when θ´ “θ, both limNÑ8pN ´1q∇θLN pθ, θ´qand limNÑ8pN ´1q∇θLN CTpθ, θ´qare well-defined and identical. This suggests that after scaling by N ´1, gradients of the CT and CM objectives match in the limit of N Ñ8, leading to equivalent training dynamics. Unlike Argument (i), Argument (ii) is valid only when θ´ “θ, which can be enforced by setting the EMA decay rate µ for the teacher network to zero in Eq. (4). We show this inconsistency in requirements for Argument (i) and (ii) to hold is caused by flawed theoretical analysis of the former. Specifically, Argument (i) fails if limNÑ8LN pθ, θ´qis not a valid objective for learning consistency models, which we show can happen when θ´ ‰ θ. To give a concrete example, consider a data distribution pdatapxq“ δpx ´ξq, which leads to pσpxq“ Npx; ξ, σ2qand a ground truth consistency function f˚px, σq“ σmin σ x ` ` 1 ´σmin σ ˘ ξ. Let us define the consistency model as fθpx, σq“ σmin σ x ` ` 1 ´σmin σ ˘ θ. In addition, let σi “σmin ` i´1 N´1 pσmax ´ σminqfor i PJ1, NK be the noise levels, where we have ∆σ “ σmax´σmin N´1 . Given z „Np0, 1qand xσi`1 “ξ`σi`1z, it is straightforward to show that˘xσi “xσi`1 ´σi`1pσi´σi`1q∇x log pσpxσi`1 q 5IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) LPIPS & squared ℓ2 metrics.  (b) s0 “2, s1 “150  (c) s0 “10, s1 “1280 Figure 2: (a) Removing EMA in the teacher network leads to significant improvement in FIDs. (b, c) Pseudo-Huber metrics significantly improve the sample quality of squared ℓ2 metric, and catches up with LPIPS when using overall larger Npkq, where the Pseudo-Huber metric with c “0.03 is the optimal. All training runs here employ the improved techniques from Sections 3.1 and 3.2. simplifies to ˇxσi “ξ `σiz. As a result, the objectives for CM and CT align perfectly in this toy example. Building on top of this analysis, the following result proves that limNÑ8LN pθ, θ´qhere is not amenable for learning consistency models whenever θ´ ‰θ. Proposition 1. Given the notations introduced earlier, and using the uniform weighting function λpσq“ 1 along with the squared ℓ2 metric, we have lim NÑ8 LN pθ, θ´q“ lim NÑ8 LN CTpθ, θ´q“ E ”` 1 ´σmin σi ˘2 pθ ´θ´q2 ı if θ´ ‰θ (6) lim NÑ8 1 ∆σ dLN pθ, θ´q dθ “ $ ’& ’% d dθ E ” σmin σ2 i ´ 1 ´σmin σi ¯ pθ ´ξq2 ı , θ ´ “θ `8, θ ´ ăθ ´8, θ ´ ąθ (7) Proof. See Appendix A. Recall that typically θ´ ‰θ when µ ‰0. In this case, Eq. (6) shows that the CM/CT objective is independent of ξ, thus providing no signal of the data distribution and are therefore impossible to train correct consistency models. This directly refutes Argument (i). In contrast, when we set µ “0 to ensure θ´ “θ, Eq. (7) indicates that the gradient of the CM/CT objective, when scaled by 1{∆σ, converges to the gradient of the mean squared error between θ and ξ. Optimizing this gradient consequently yields θ “ξ, accurately learning the ground truth consistency function. This analysis is consistent with Argument (ii). As illustrated in Fig. 2a, discarding EMA from the teacher network notably improves sample quality for CT across both LPIPS and squared ℓ2 metrics. The curves labeled “Improved” correspond to CT using the improved design outlined in Section 3.1. Setting µpkq“ 0 for all training iteration k effectively counters the sample quality degradation of LPIPS caused by the modifications in Section 3.1. Combining the strategies from Section 3.1 with a zero EMA for the teacher, we are able to match the sample quality of the original CT in Song et al. (2023) that necessitates LPIPS, by using simple squared ℓ2 metrics. 3.3 P SEUDO -HUBER METRIC FUNCTIONS Using the methods from Sections 3.1 and 3.2, we are able to improve CT with squared ℓ2 metric, matching the original CT in Song et al. (2023) that utilizes LPIPS. Yet, as shown in Fig. 2a, LPIPS still maintains a significant advantage over traditional metric functions when the same improved techniques are in effect for all. To address this disparity, we adopt the Pseudo-Huber metric family (Charbonnier et al., 1997), defined as dpx, yq“ b ∥x ´y∥2 2 `c2 ´c, (8) 6IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS where c ą0 is an adjustable constant. As depicted in Fig. 6a, Pseudo-Huber metrics smoothly bridge the ℓ1 and squared ℓ2 metrics, with c determining the breadth of the parabolic section. In contrast to common metrics like ℓ0, ℓ1, and ℓ8, Pseudo-Huber metrics are continuously twice differentiable, and hence meet the theoretical requirement for CT outlined in Song et al. (2023). Compared to the squared ℓ2 metric, the Pseudo-Huber metric is more robust to outliers as it imposes a smaller penalty for large errors than the squared ℓ2 metric does, yet behaves similarly for smaller errors. We posit that this added robustness can reduce variance during training. To validate this hypothesis, we examine the ℓ2 norms of parameter updates obtained from the Adam optimizer during the course of training for both squared ℓ2 and Pseudo-Huber metric functions, and summarize results in Fig. 6b. Our observations confirm that the Pseudo-Huber metric results in reduced variance relative to the squared ℓ2 metric, aligning with our hypothesis. We evaluate the effectiveness of Pseudo-Huber metrics by training several consistency models with varying c values on CIFAR-10 and comparing their sample quality with models trained using LPIPS and squared ℓ2 metrics. We incorporate improved techniques from Sections 3.1 and 3.2 for all metrics. Fig. 2 reveals that Pseudo-Huber metrics yield notably better sample quality than the squared ℓ2 metric. By increasing the overall size of Npkq—adjusting s0 and s1 from the standard values of 2 and 150 in Song et al. (2023) to our new values of 10 and 1280 (more in Section 3.4)—we for the first time surpass the performance of CT with LPIPS on equal footing using a traditional metric function that does not rely on learned feature representations. Furthermore, Fig. 2c indicates that c “0.03 is optimal for CIFAR-10 images. We suggest that c should scale linearly with ∥x ´y∥2, and propose a heuristic of c “0.00054 ? d for images with d dimensions. Empirically, we find this recommendation to work well on both CIFAR-10 and ImageNet 64 ˆ64 datasets. 3.4 I MPROVED CURRICULUM FOR TOTAL DISCRETIZATION STEPS As mentioned in Section 3.2, CT’s theoretical foundation holds asymptotically as N Ñ 8. In practice, we have to select a finite N for training consistency models, potentially introducing bias into the learning process. To understand the influence of N on sample quality, we train a consistency model with improved techniques from Sections 3.1 to 3.3. Unlike Song et al. (2023), we use an exponentially increasing curriculum for the total discretization steps N, doubling N after a set number of training iterations. Specifically, the curriculum is described by Npkq“ minps02t k K1u, s1q` 1, K 1 “ Y K log2ts1{s0u `1 ] , (9) and its shape is labelled “Exp” in Fig. 3b. As revealed in Fig. 3a, the sample quality of consistency models improves predictably asN increases. Importantly, FID scores relative to N adhere to a precise power law until reaching saturation, after which further increases in N yield diminishing benefits. As noted by Song et al. (2023), while larger N can reduce bias in CT, they might increase variance. On the contrary, smaller N reduces variance at the cost of higher bias. Based on Fig. 3a, we cap N at 1281 in Npkq, which we empirically find to strike a good balance between bias and variance. In our experiments, we set s0 and s1 in discretization curriculums from their default values of 2 and 150 in Song et al. (2023) to 10 and 1280 respectively. Aside from the exponential curriculum above, we also explore various shapes for Npkqwith the same s0 “10 and s1 “1280, including a constant function, the square root function from Song et al. (2023), a linear function, a square function, and a cosine function. The shapes of various curriculums are illustrated in Fig. 3b. As Fig. 3c demonstrates, the exponential curriculum yields the best sample quality for consistency models. Consequently, we adopt the exponential curriculum in Eq. (9) as our standard for setting Npkqgoing forward. 3.5 I MPROVED NOISE SCHEDULES Song et al. (2023) propose to sample a randomi from UJ1, N´1K and select σi and σi`1 to compute the CT objective. Given that σi “pσ1{ρ min ` i´1 N´1 pσ1{ρ max ´σ1{ρ min qqρ, this corresponds to sampling from the distribution pplog σq“ σ σ1{ρ´1 ρpσ1{ρ max ´σ1{ρ min q as N Ñ8. As shown in Fig. 4a, this distribution exhibits a 7IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) FID scores vs. N  (b) Various curriculums for Npkq.  (c) FIDs vs. Npkqcurriculums. Figure 3: (a) FID scores improve predictably as the number of discretization steps N grows. (b) The shapes of various curriculums for total discretization steps Npkq. (c) The FID curves of various curriculums for discretization. All models are trained with improved techniques from Sections 3.1 to 3.3 with the only difference in discretization curriculums. (a) PDF of log σ  (b) Lognormal vs. default schedules. Figure 4: The PDF of log σ indicates that the default noise schedule in Song et al. (2023) assigns more weight to larger values of log σ, corrected by our lognormal schedule. We compare the FID scores of CT using both the lognormal noise schedule and the original one, where both models incorporate the improved techniques in Sections 3.1 to 3.4. higher probability density for larger values of log σ. This is at odds with the intuition that consistency losses at lower noise levels influence subsequent ones and cause error accumulation, so losses at lower noise levels should be given greater emphasis. Inspired by Karras et al. (2022), we address this by adopting a lognormal distribution to sample noise levels, setting a mean of -1.1 and a standard deviation of 2.0. As illustrated in Fig. 4a, this lognormal distribution assigns significantly less weight to high noise levels. Moreover, it also moderates the emphasis on smaller noise levels. This is helpful because learning is easier at smaller noise levels due to the inductive bias in our parameterization of the consistency model to meet the boundary condition. For practical implementation, we sample noise levels in the set tσ1, σ2, ¨¨¨ , σN uaccording to a discretized lognormal distribution defined as ppσiq9erf ˆlogpσi`1q´ Pmean? 2Pstd ˙ ´erf ˆlogpσiq´ Pmean? 2Pstd ˙ , (10) where Pmean “ ´1.1 and Pstd “ 2.0. As depicted in Fig. 4b, this lognormal noise schedule significantly improves the sample quality of consistency models. 4 P UTTING IT TOGETHER Combining all the improved techniques from Sections 3.1 to 3.5, we employ CT to train several consistency models on CIFAR-10 and ImageNet 64 ˆ64 and benchmark their performance with competing methods in the literature. We evaluate sample quality using FID (Heusel et al., 2017), Inception score (Salimans et al., 2016), and Precision/Recall (Kynkäänniemi et al., 2019). For best 8IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS Table 2: Comparing the quality of unconditional samples on CIFAR-10. METHOD NFE ( Ó) FID (Ó) IS (Ò) Fast samplers & distillation for diffusion models DDIM (Song et al., 2020) 10 13.36 DPM-solver-fast (Lu et al., 2022) 10 4.70 3-DEIS (Zhang & Chen, 2022) 10 4.17 UniPC (Zhao et al., 2023) 10 3.87 Knowledge Distillation (Luhman & Luhman, 2021) 1 9.36 DFNO (LPIPS) (Zheng et al., 2022) 1 3.78 2-Rectified Flow (+distill) (Liu et al., 2022) 1 4.85 9.01 TRACT (Berthelot et al., 2023) 1 3.78 2 3.32 Diff-Instruct (Luo et al., 2023) 1 4.53 9.89 PD˚(Salimans & Ho, 2022) 1 8.34 8.69 2 5.58 9.05 CD (LPIPS) (Song et al., 2023) 1 3.55 9.48 2 2.93 9.75 Direct Generation Score SDE (Song et al., 2021) 2000 2.38 9.83 Score SDE (deep) (Song et al., 2021) 2000 2.20 9.89 DDPM (Ho et al., 2020) 1000 3.17 9.46 LSGM (Vahdat et al., 2021) 147 2.10 PFGM (Xu et al., 2022) 110 2.35 9.68 EDM˚(Karras et al., 2022) 35 2.04 9.84 EDM-G++ (Kim et al., 2023) 35 1.77 IGEBM (Du & Mordatch, 2019) 60 40.6 6.02 NV AE (Vahdat & Kautz, 2020) 1 23.5 7.18 Glow (Kingma & Dhariwal, 2018) 1 48.9 3.92 Residual Flow (Chen et al., 2019) 1 46.4 BigGAN (Brock et al., 2019) 1 14.7 9.22 StyleGAN2 (Karras et al., 2020b) 1 8.32 9.21 StyleGAN2-ADA (Karras et al., 2020a) 1 2.92 9.83 CT (LPIPS) (Song et al., 2023) 1 8.70 8.49 2 5.83 8.85 iCT (ours) 1 2.83 9.54 2 2.46 9.80 iCT-deep (ours) 1 2.51 9.76 2 2.24 9.89 Table 3: Comparing the quality of class- conditional samples on ImageNet 64 ˆ64. METHOD NFE ( Ó) FID (Ó) Prec. (Ò) Rec. (Ò) Fast samplers & distillation for diffusion models DDIM (Song et al., 2020) 50 13.7 0.65 0.56 10 18.3 0.60 0.49 DPM solver (Lu et al., 2022) 10 7.93 20 3.42 DEIS (Zhang & Chen, 2022) 10 6.65 20 3.10 DFNO (LPIPS) (Zheng et al., 2022) 1 7.83 0.61 TRACT (Berthelot et al., 2023) 1 7.43 2 4.97 BOOT (Gu et al., 2023) 1 16.3 0.68 0.36 Diff-Instruct (Luo et al., 2023) 1 5.57 PD˚(Salimans & Ho, 2022) 1 15.39 0.59 0.62 2 8.95 0.63 0.65 4 6.77 0.66 0.65 PD (LPIPS) (Song et al., 2023) 1 7.88 0.66 0.63 2 5.74 0.67 0.65 4 4.92 0.68 0.65 CD (LPIPS) (Song et al., 2023) 1 6.20 0.68 0.63 2 4.70 0.69 0.64 3 4.32 0.70 0.64 Direct Generation RIN (Jabri et al., 2023) 1000 1.23 DDPM (Ho et al., 2020) 250 11.0 0.67 0.58 iDDPM (Nichol & Dhariwal, 2021) 250 2.92 0.74 0.62 ADM (Dhariwal & Nichol, 2021) 250 2.07 0.74 0.63 EDM (Karras et al., 2022) 511 1.36 EDM˚(Heun) (Karras et al., 2022) 79 2.44 0.71 0.67 BigGAN-deep (Brock et al., 2019) 1 4.06 0.79 0.48 CT (LPIPS) (Song et al., 2023) 1 13.0 0.71 0.47 2 11.1 0.69 0.56 iCT (ours) 1 4.02 0.70 0.63 2 3.20 0.73 0.63 iCT-deep (ours) 1 3.25 0.72 0.63 2 2.77 0.74 0.62 Most results for existing methods are taken from a previous paper, except for those marked with *, which are from our own re-implementation. (a) One-step samples on CIFAR-10. (b) Two-step samples on CIFAR-10.  (c) One-step samples on ImageNet.  (d) Two-step samples on ImageNet. Figure 5: One-step and two-step samples from iCT-deep models trained on CIFAR-10 and ImageNet 64 ˆ64 respectively. All corresponding samples are generated from the same initial noise vector. 9IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS performance, we use a larger batch size and an increased EMA decay rate for the student network in CT across all models. The model architectures are based on Score SDE (Song et al., 2021) for CIFAR-10 and ADM (Dhariwal & Nichol, 2021) for ImageNet 64 ˆ64. We also explore deeper variants of these architectures by doubling the model depth. We call our method iCT which stands for “improved consistency training”, and the deeper variants iCT-deep. We summarize our results in Tables 2 and 3 and provide uncurated samples from iCT-deep in Fig. 5. More details and results can be found in Appendix B. We summarize our results and compare them to previous methods in Tables 2 and 3. Here we exclude methods based on FastGAN (Liu et al., 2020; Sauer et al., 2021) or StyleGAN-XL (Sauer et al., 2022) from our consideration, because both utilize ImageNet pre-trained feature extractors in their discriminators. As noted by Kynkäänniemi et al. (2023), this can skew FIDs and lead to inflated sample quality. Several key observations emerge from Tables 2 and 3. First, iCT methods surpass previous diffusion distillation approaches in both one-step and two-step generationon CIFAR-10 and ImageNet64ˆ64, all while circumventing the need for training diffusion models. Secondly, iCT models demonstrate sample quality comparable to many leading generative models, including diffusion models and GANs. For instance, with one-step generation, iCT-deep obtains FIDs of 2.51 and 3.25 for CIFAR-10 and ImageNet respectively, whereas DDPMs (Ho et al., 2020) necessitate thousands of sampling steps to reach FIDs of 3.17 and 11.0 (result taken from Gu et al. (2023)) on both datasets. The one-step FID for iCT already exceeds that of StyleGAN-ADA (Karras et al., 2020b) on CIFAR-10, and that of BigGAN-deep (Brock et al., 2019) on ImageNet 64 ˆ64, let alone iCT-deep models. For two-step generation, iCT-deep records an FID of 2.24, matching Score SDE in Song et al. (2021), a diffusion model with an identical architecture but demands 2000 sampling steps for an FID of 2.20. Lastly, iCT methods show improved recall than CT (LPIPS) in Song et al. (2023) and BigGAN-deep, indicating better diversity and superior mode coverage. 5 C ONCLUSION Our improved techniques for CT have successfully addressed its previous limitations, surpassing the performance of CD in generating high-quality samples without relying on LPIPS. We examined the impact of weighting functions, noise embeddings, and dropout. By removing EMA for teacher networks, adopting Pseudo-Huber losses in lieu of LPIPS, combined with a new curriculum for discretization and noise sampling schedule, we have achieved unprecedented FID scores for consis- tency models on both CIFAR-10 and ImageNet 64 ˆ64 datasets. Remarkably, these results outpace previous CT methods by a considerable margin, surpass previous few-step diffusion distillation techniques, and challenge the sample quality of leading diffusion models and GANs. ACKNOWLEDGEMENTS We would like to thank Alex Nichol, Allan Jabri, Ishaan Gulrajani, and Jakub Pachocki for insightful technical discussions. We also appreciate Mark Chen and Ilya Sutskever for their unwavering support throughout this project. REFERENCES David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbot, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm. Pierre Charbonnier, Laure Blanc-Féraud, Gilles Aubert, and Michel Barlaud. Deterministic edge- preserving regularization in computed imaging. IEEE Transactions on image processing, 6(2): 298–311, 1997. 10IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Jörn-Henrik Jacobsen. Residual flows for invertible generative modeling. In Advances in Neural Information Processing Systems, pp. 9916–9926, 2019. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. arXiv preprint arXiv:2105.05233, 2021. Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based mod- els. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso- ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/ 2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa- tion processing systems, pp. 2672–2680, 2014. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua M Susskind. Boot: Data-free distil- lation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference tz&uGenerative Modeling, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems, 33, 2020. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. arXiv preprint arXiv:2301.11093, 2023. Aapo Hyvärinen. Estimation of Non-Normalized Statistical Models by Score Matching. Journal of Machine Learning Research, 6(Apr):695–709, 2005. Allan Jabri, David J. Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in neural information processing systems, 33:12104–12114, 2020a. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. 2020b. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion- based generative models. In Proc. NeurIPS, 2022. Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, and Il-Chul Moon. Refining gener- ative process with discriminator guidance in score-based diffusion models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 16567–16598. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/kim23i.html. Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pp. 10215–10224, 2018. Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 Dataset.online: http://www. cs. toronto. edu/kriz/cifar. html, 55, 2014. 11IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019. Tuomas Kynkäänniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of im- agenet classes in fréchet inception distance. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=4oXTQ6m_ws8. Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal. Towards faster and stabilized gan training for high-fidelity few-shot image synthesis. In International Conference on Learning Representations, 2020. Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775–5787, 2022. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff- instruct: A universal approach for transferring knowledge from pre-trained diffusion models. arXiv preprint arXiv:2305.18455, 2023. Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=TIdIXIpzhoI. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2226–2234, 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html. Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected gans converge faster. Advances in Neural Information Processing Systems, 34:17480–17492, 2021. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 1–10, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning Using Nonequilibrium Thermodynamics. In International Conference on Machine Learning, pp. 2256–2265, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pp. 11918–11930, 2019. Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 12IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced Score Matching: A Scalable Approach to Density and Score Estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 204, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=PxTIG12RRHS. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 32211–32252. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/song23a.html. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33:7537–7547, 2020. Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. Advances in neural information processing systems, 33:19667–19679, 2020. Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:11287–11302, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural Computation, 23(7):1661–1674, 2011. Yilun Xu, Ziming Liu, Max Tegmark, and Tommi S. Jaakkola. Poisson flow generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=voV_TRqcWh. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586–595, 2018. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor- corrector framework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867, 2023. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. arXiv preprint arXiv:2211.13449, 2022. 13IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS A P ROOFS Proposition 1. Given the notations introduced in Section 3.2, and using the uniform weighting function λpσq“ 1 along with the squared ℓ2 metric, we have lim NÑ8 LN pθ, θ´q“ lim NÑ8 LN CTpθ, θ´q“ E ”` 1 ´σmin σi ˘2 pθ ´θ´q2 ı if θ´ ‰θ (11) lim NÑ8 1 ∆σ dLN pθ, θ´q dθ “ $ ’& ’% d dθ E ” σmin σ2 i ´ 1 ´σmin σi ¯ pθ ´ξq2 ı , θ ´ “θ `8, θ ´ ăθ ´8, θ ´ ąθ (12) Proof. Since λpσq ”1 and dpx, yq “ px ´yq2, we can write down the CM and CT objectives as LN pθ, θ´q “Erpfθpxσi`1 , σi`1q´ fθ´p˘xσi, σiqq2sand LN CTpθ, θ´q “Erpfθpxσi`1 , σi`1q´ fθ´pˇxσi, σiqq2srespectively. Since pdatapxq “δpx ´ξq, we have pσpxq “Npx | ξ, σ2q, and therefore ∇ log pσpxq“´ x´ξ σ2 . According to the definition of ˘xσi and xσi`1 “ξ `σi`1z, we have ˘xσi “xσi`1 ´pσi ´σi`1qσi`1∇ log ppxσi`1 , σi`1q “xσi`1 `pσi ´σi`1qσi`1 xσi`1 ´ξ σ2 i`1 “xσi`1 `pσi ´σi`1qz “ξ `σi`1z `pσi ´σi`1qz “ξ `σiz “ˇxσi. As a result, the CM and CT objectives are exactly the same, that is, LN pθ, θ´q “LN CTpθ, θ´q. Recall that the consistency model fθpx, σqis defined as fθpx, σq“ σmin σ x ` ` 1 ´σmin σ ˘ θ, so we have fθpxσ, σq“ σminz `σmin σ ξ ` ` 1 ´σmin σ ˘ θ. Now, let us focus on the CM objective LN pθ, θ´q“ Erpfθpxσi`1 , σi`1q´ fθ´p˘xσi, σiqq2s “Erpfθpxσi`1 , σi`1q´ fθ´pˇxσi, σiqq2s “E „ˆσmin σi`1 ξ ` ˆ 1 ´ σmin σi`1 ˙ θ ´σmin σi ξ ´ ˆ 1 ´σmin σi ˙ θ´ ˙2ȷ “E „ˆ σmin σi `∆σ ξ ` ˆ 1 ´ σmin σi `∆σ ˙ θ ´σmin σi ξ ´ ˆ 1 ´σmin σi ˙ θ´ ˙2ȷ , where ∆σ “ σmax´σmin N´1 , because σi “σmin ` i´1 N´1 pσmax ´σminq. By taking the limit N Ñ8, we have ∆σ Ñ0, and therefore lim NÑ8 LN pθ, θ´q “ lim ∆σÑ0 E „ˆ σmin σi `∆σ ξ ` ˆ 1 ´ σmin σi `∆σ ˙ θ ´σmin σi ξ ´ ˆ 1 ´σmin σi ˙ θ´ ˙2ȷ “ lim ∆σÑ0 E „ˆσmin σi ˆ 1 ´∆σ σi ˙ ξ ` ˆ 1 ´ σmin σi `∆σ ˙ θ ´σmin σi ξ ´ ˆ 1 ´σmin σi ˙ θ´ ˙2ȷ `op∆σq “ lim ∆σÑ0 E „ˆ ´σmin∆σ σ2 i ξ ` ˆ 1 ´ σmin σi `∆σ ˙ θ ´ ˆ 1 ´σmin σi ˙ θ´ ˙2ȷ `op∆σq “ lim ∆σÑ0 E „ˆ ´σmin∆σ σ2 i ξ ` ˆ 1 ´σmin σi ˆ 1 ´∆σ σi ˙˙ θ ´ ˆ 1 ´σmin σi ˙ θ´ ˙2ȷ `op∆σq. 14IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS Suppose θ´ ‰θ, we have lim NÑ8 LN pθ, θ´q “ lim ∆σÑ0 E „ˆ ´σmin∆σ σ2 i ξ ` ˆ 1 ´σmin σi ˆ 1 ´∆σ σi ˙˙ θ ´ ˆ 1 ´σmin σi ˙ θ´ ˙2ȷ `op∆σq “ lim ∆σÑ0 E ”` 1 ´σmin σi ˘2 pθ ´θ´q2 ı `op∆σq “E ”` 1 ´σmin σi ˘2 pθ ´θ´q2 ı , which proves our first statement in the proposition. Now, let’s consider∇θLN pθ, θ´q. It has the following form ∇θLN pθ, θ´q“ 2E „ˆσmin σi`1 ξ ` ˆ 1 ´ σmin σi`1 ˙ θ ´σmin σi ξ ´ ˆ 1 ´σmin σi ˙ θ´ ˙ˆ 1 ´ σmin σi`1 ˙ȷ . As N Ñ8 and ∆σ Ñ0, we have lim NÑ8 ∇θLN pθ, θ´q “ lim ∆σÑ0 2E „ ´σmin∆σ σ2 i ξ ` ˆ 1 ´σmin σi ˆ 1 ´∆σ σi ˙˙ θ ´ ˆ 1 ´σmin σi ˙ θ´ ȷˆ 1 ´σmin σi ˙ “ $ & % lim∆σÑ0 2E ” ´σmin∆σ σ2 i ξ `σmin∆σ σ2 i θ ı´ 1 ´σmin σi ¯ , θ ´ “θ 2E ”` 1 ´σmin σ ˘2 pθ ´θ´q ı , θ ´ ‰θ “ $ & % lim∆σÑ0 2E ” σmin∆σ σ2 i pθ ´ξq ı´ 1 ´σmin σi ¯ , θ ´ “θ 2E ”` 1 ´σmin σi ˘2 pθ ´θ´q ı , θ ´ ‰θ. (13) Now it becomes obvious from Eq. (13) that when θ´ “θ, we have lim NÑ8 1 ∆σ ∇θLN pθ, θ´q“ lim ∆σÑ0 2E ”σmin σ2 i pθ ´ξq ıˆ 1 ´σmin σi ˙ “2E ”σmin σ2 i pθ ´ξq ıˆ 1 ´σmin σi ˙ “ d dθ E ”σmin σ2 i ´ 1 ´σmin σi ¯ pθ ´ξq2 ı . Moreover, we can deduce from Eq. (13) that lim NÑ8 1 ∆σ ∇θLN pθ, θ´q“ \"`8, θ ąθ´ ´8, θ ăθ´ , which concludes the proof. B A DDITIONAL EXPERIMENTAL DETAILS AND RESULTS Model architecture Unless otherwise noted, we use the NCSN++ architecture (Song et al., 2021) on CIFAR-10, and the ADM architecture (Dhariwal & Nichol, 2021) on ImageNet 64 ˆ64. For iCT-deep models in Tables 2 and 3, we double the depth of base architectures by increasing the number of residual blocks per resolution from 4 and 3 to 8 and 6 for CIFAR-10 and ImageNet64ˆ64 respectively. We use a dropout rate of 0.3 for all consistency models on CIFAR-10. For ImageNet 64 ˆ64, we use a dropout rate of 0.2, but only apply them to convolutional layers whose the feature map resolution is smaller or equal to 16ˆ16, following the configuration in Hoogeboom et al. (2023). We also found that AdaGN introduced in Dhariwal & Nichol (2021) hurts consistency training and opt to remove it for our ImageNet 64 ˆ64 experiments. All models on CIFAR-10 are unconditional, and all models on ImageNet 64 ˆ64 are conditioned on class labels. 15IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) dp0, xqas a function of x.  (b) Comparing Adam updates. Figure 6: (a) The shapes of various metric functions. (b) The ℓ2 norms of parameter updates in Adam optimizer. Curves are rescaled to have the same mean. The Pseudo-Huber metric has lower variance compared to the squared ℓ2 metric. Training We train all models with the RAdam optimizer (Liu et al., 2019) using learning rate 0.0001. All CIFAR-10 models are trained for 400,000 iterations, whereas ImageNet 64 ˆ64 models are trained for 800,000 iterations. For CIFAR-10 models in Section 3, we use batch size 512 and EMA decay rate 0.9999 for the student network. For iCT and iCT-deep models in Table 2, we use batch size 1024 and EMA decay rate of 0.99993 for CIFAR-10 models, and batch size 4096 and EMA decay rate 0.99997 for ImageNet 64 ˆ64 models. All models are trained on a cluster of Nvidia A100 GPUs. Pseudo-Huber losses and variance reduction In Fig. 6, we provide additional analysis for the Pseudo-Huber metric proposed in Section 3.3. We show the shapes of squared ℓ2 metric, as well as Pseudo-Huber losses with various values of c in Fig. 6a, illustrating that Pseudo-Huber losses smoothly interpolates between the ℓ1 and squared ℓ2 metrics. In Fig. 6b, we plot the ℓ2 norms of parameter updates retrieved from the Adam optimizer for models trained with squared ℓ2 and Pseudo-Huber metrics. We observe that the Pseudo-Huber metric has lower variance compared to the squared ℓ2 metric, which is consistent with our hypothesis in Section 3.3. Samples We provide additional uncurated samples from iCT and iCT-deep models on both CIFAR- 10 and ImageNet 64 ˆ64. See Figs. 7 to 10. For two-step sampling, the intermediate noise level σi2 is 0.821 for CIFAR-10 and 1.526 for ImageNet 64 ˆ64 when using iCT. When employing iCT-deep, σi2 is 0.661 for CIFAR-10 and 0.973 for ImageNet 64 ˆ64. 16IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) One-step samples from the iCT model on CIFAR-10 (FID = 2.83). (b) Two-step samples from the iCT model on CIFAR-10 (FID = 2.46). Figure 7: Uncurated samples from iCT models on CIFAR-10. All corresponding samples use the same initial noise. 17IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) One-step samples from the iCT-deep model on CIFAR-10 (FID = 2.51). (b) Two-step samples from the iCT-deep model on CIFAR-10 (FID = 2.24). Figure 8: Uncurated samples from iCT-deep models on CIFAR-10. All corresponding samples use the same initial noise. 18IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) One-step samples from the iCT model on ImageNet 64 ˆ64 (FID = 4.02). (b) Two-step samples from the iCT model on ImageNet 64 ˆ64 (FID = 3.20). Figure 9: Uncurated samples from iCT models on ImageNet 64 ˆ64. All corresponding samples use the same initial noise. 19IMPROVED TECHNIQUES FOR TRAINING CONSISTENCY MODELS (a) One-step samples from the iCT-deep model on ImageNet 64 ˆ64 (FID = 3.25). (b) Two-step samples from the iCT-deep model on ImageNet64 ˆ64 (FID = 2.77). Figure 10: Uncurated samples from iCT-deep models on ImageNet 64 ˆ64. All corresponding samples use the same initial noise. 20",
      "references": [
        "Tract: Denoising diffusion models with transitive closure time-distillation",
        "Large scale GAN training for high fidelity natural image synthesis",
        "Deterministic edge- preserving regularization in computed imaging",
        "Residual flows for invertible generative modeling",
        "Imagenet: A large-scale hierarchical image database",
        "Diffusion models beat GANs on image synthesis",
        "Implicit generation and modeling with energy based models",
        "Generative adversarial nets",
        "Boot: Data-free distillation of denoising diffusion models with bootstrapping",
        "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
        "Denoising Diffusion Probabilistic Models",
        "simple diffusion: End-to-end diffusion for high resolution images",
        "Estimation of Non-Normalized Statistical Models by Score Matching",
        "Scalable adaptive computation for iterative generation",
        "Training generative adversarial networks with limited data",
        "Elucidating the design space of diffusion-based generative models",
        "Refining generative process with discriminator guidance in score-based diffusion models",
        "Glow: Generative flow with invertible 1x1 convolutions",
        "The CIFAR-10 Dataset",
        "Improved precision and recall metric for assessing generative models",
        "The role of imagenet classes in fréchet inception distance",
        "Towards faster and stabilized gan training for high-fidelity few-shot image synthesis",
        "On the variance of the adaptive learning rate and beyond",
        "Flow straight and fast: Learning to generate and transfer data with rectified flow",
        "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps",
        "Knowledge distillation in iterative generative models for improved sampling speed",
        "Diff- instruct: A universal approach for transferring knowledge from pre-trained diffusion models",
        "Improved denoising diffusion probabilistic models",
        "Progressive distillation for fast sampling of diffusion models",
        "Improved techniques for training GANs",
        "Projected gans converge faster",
        "Stylegan-xl: Scaling stylegan to large diverse datasets",
        "Deep Unsupervised Learning Using Nonequilibrium Thermodynamics",
        "Denoising diffusion implicit models",
        "Generative modeling by estimating gradients of the data distribution",
        "Improved techniques for training score-based generative models",
        "Score-based generative modeling through stochastic differential equations",
        "Consistency models",
        "Fourier features let networks learn high frequency functions in low dimensional domains",
        "Nvae: A deep hierarchical variational autoencoder",
        "Score-based generative modeling in latent space",
        "Attention is all you need",
        "A Connection Between Score Matching and Denoising Autoencoders",
        "Poisson flow generative models",
        "Fast sampling of diffusion models with exponential integrator",
        "The unreasonable effectiveness of deep features as a perceptual metric",
        "Unipc: A unified predictor- corrector framework for fast sampling of diffusion models",
        "Fast sampling of diffusion models via operator learning"
      ],
      "meta_data": {
        "arxiv_id": "2310.14189v1",
        "authors": [
          "Yang Song",
          "Prafulla Dhariwal"
        ],
        "published_date": "2023-10-22T05:33:38Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a series of improved techniques for training consistency models directly from data without relying on distillation from diffusion models. Key contributions include the removal of exponential moving average (EMA) from the teacher network, adoption of Pseudo-Huber losses to replace learned metrics like LPIPS, improved weighting functions, refined noise embeddings with a lognormal noise schedule, and an exponential curriculum for discretization steps. These modifications significantly enhance sample quality, achieving state-of-the-art FID scores on CIFAR-10 and ImageNet 64x64, surpassing previous consistency training and even diffusion model techniques.",
        "methodology": "The approach is based on consistency training (CT) for generative models. The authors refine the CT objective by using a squared ℓ2 metric and propose replacing LPIPS with Pseudo-Huber loss to reduce bias. They optimize the consistency matching loss between the student and teacher networks, remove EMA for the teacher, adjust the weighting function (1/σi’ - σi), and utilize carefully tuned noise embeddings and dropout rates. Moreover, they introduce an exponential curriculum for increasing discretization steps and a lognormal noise schedule for sampling noise levels, all contributing to stable and effective training.",
        "experimental_setup": "Experiments are conducted on the CIFAR-10 dataset and ImageNet 64x64. The authors use architectures based on the Score SDE model for CIFAR-10 and ADM for ImageNet, with deeper variants (termed iCT and iCT-deep) also evaluated. They benchmark model performance using FID, Inception Score, and precision/recall measures, comparing one-step and two-step generation. The training involves 400,000 to 800,000 iterations using large batch sizes and specific EMA decay rates, dropout configurations, and discretization curricula to validate the improvements in sample quality.",
        "limitations": "While the paper demonstrates significant improvements, the methods rely heavily on precise hyperparameter tuning (e.g., dropout rates, noise embedding sensitivity, and curriculum parameters). The theoretical analysis assumes asymptotic conditions (N → ∞) which may not fully capture practical finite-step behavior. Additionally, the experiments are limited to relatively low-resolution datasets (CIFAR-10 and ImageNet 64x64), and scaling to higher resolutions may present further challenges in terms of computational demands and training stability.",
        "future_research_directions": "Future work could explore extending the improved consistency training techniques to higher resolution images and more diverse datasets. Research could also investigate alternative robust loss functions or regularization techniques to further stabilize training. Additional theoretical work may aim to tighten the analysis of finite-step behavior and generalize the approach to conditional or guided generation tasks. Integration with other generative frameworks and further exploration of the interplay between bias, variance, and model complexity are promising directions.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Techniques for Symbol Grounding with SATNet",
      "full_text": "Techniques for Symbol Grounding with SATNet Sever Topan1 David Rolnick2 Xujie Si2 Abstract Many experts argue that the future of artiﬁcial intelligence is limited by the ﬁeld’s ability to integrate symbolic logical reasoning into deep learning architectures. The recently proposed differentiable MAXSAT solver, SATNet, was a breakthrough in its capacity to integrate with a traditional neural network and solve visual reasoning problems. For instance, it can learn the rules of Sudoku purely from image examples. Despite its success, SATNet was shown to succumb to a key challenge in neurosymbolic systems known as the Symbol Grounding Problem: the inability to map visual inputs to symbolic variables without explicit supervision (“label leakage”). In this work, we present a self-supervised pre-training pipeline that enables SATNet to overcome this limitation, thus broadening the class of problems that SATNet architectures can solve to include datasets where no intermediary labels are available at all. We demonstrate that our method allows SATNet to attain full accuracy even with a harder problem setup that prevents any label leakage. We additionally introduce aproofreadingmethod that further improves the performance of SATNet architectures, beating the state-of-the-art on Visual Sudoku. 1 Introduction Recent years have seen signiﬁcant advancements in deep learning, providing breakthroughs in image, video, and audio processing [1]. Despite its success, deep learning has many known limitations, such as low interpretability, vulnerability to adversarial attacks, and difﬁculty in solving problems requiring hard logical constraints [2–5]. To overcome these limitations, experts have described the need to migrate from purely deep learning-based systems to neurosymbolic artiﬁcial intelligence systems, which integrate neural networks with logical reasoning [6]. In this work we focus on improving a promising development in this ﬁeld: the award-winning architecture known as SATNet [7]. SATNet is a differentiable MAXSAT solver based on a low-rank semideﬁnite relaxation approach. It can be integrated into traditional Deep Neural Networks (DNNs) to solve composite learning problems that require both logical reasoning and visual understanding. One such problem is Visual Sudoku, where the model must learn the rules of a Sudoku puzzle purely from visual examples. When trained end-to-end, SATNet is able to achieve 63.2% total board accuracy in this task while traditional DNN architectures are unable to exceed 0% [ 7]. This was regarded as a signiﬁcant breakthrough for neurosymbolic architectures. However, it was recently noted that SATNet training relies upon “leakage” of labels through the logical constraint layer to the DNN used to classify digits [8]. This leakage essentially means that SATNet is learning in two supervised stages, where it ﬁrst trains its digit classiﬁcation component under direct supervision, and only then trains its SAT layer to learn the logical constraints delineating Sudoku. When the leakage is removed, SATNet’s ability to solve Visual Sudoku drops to 0% [8]. This is signiﬁcant, because taken independently, these two sub-problems are signiﬁcantly easier. Digit classiﬁcation is considered a solved problem, and while SAT constraint mining is more difﬁcult, it could be argued that the differentiable aspect is no longer beneﬁcial if the system needs supervision on its inputs to learn regardless. For instance, there exist other SAT constraint miners that are not differentiable but outperform SATNet [9]. Overall, the issue 1McGill University and NVIDIA, sever.topan@mail.mcgill.ca 2McGill University and Mila – Quebec AI Institute, {drolnick,xsi}@cs.mcgill.ca arXiv:2106.11072v1  [cs.AI]  16 Jun 2021of being unable to learn to solve composite visual reasoning problems end-to-end is referred to as the Symbol Grounding Problem, and is considered one of the fundamental prerequisites for artiﬁcial intelligence to perform practical logical reasoning [8, 10]. Figure 1: The SATNet architecture used to solve Visual Sudoku. The red line shows the label leakage issue, which when removed, results in the Symbol Grounding Problem. We observe a key challenge of symbol grounding is the large gap between the compositional nature of logical reasoning and the end-to-end gradient-based nature of neural networks. The former helps to reduce a sophisticated reasoning system into simple, independent modules, each of which can be designed manually or learned, while the latter encourages fusing all components together and using gradients as a universal means for learning. Many recent approaches aim to bridge this gap by relaxing logical constraint solving through numerical optimisations [11–14]. Although such end-to- end gradient-based optimisation is appealing, it can fail to address seemingly simple tasks like Visual Sudoku. The success of SATNet is in fact due toinadvertent supervision of intermediate modules. We argue that compositionality does not have to be the opposite of the end-to-end learning design. The latter is particularly preferable because it eliminates the need for supervision of intermediate modules, which is often required by a compositional design. If compositionality can be trained using self- supervision (i.e. without manual effort), compositionality would then be at least equally preferable. This is the approach that we take in the present work, synergistically combining compositionality with end-to-end learning without any explicit intermediate supervision. We envision our methodology forming a new paradigm for tackling neurosymbolic learning. We describe a self-supervised pre-training method that can be used to bootstrap SATNet in order to overcome the Symbol Grounding Problem. Our methodology enables us to tackle a class of what we call Ungrounded MAXSAT problems, where label data are available only for the output variables of the MAXSAT problem. In the Visual Sudoku case, this formulation manifests itself as a dataset where, as before, inputs consist of images of digits describing the input cells of a Sudoku board. The labels of the dataset, however, consist of numerical representations only for the board cells that were not given as inputs. This means that there is no way of identifying what digit each input image refers to except by learning the rules of the Sudoku puzzle in parallel to predict the non-input values. We refer to this problem as Ungrounded Visual Sudoku. We show that our method improves the state of the art on this problem from 0% to 64.8%, achieving similar performance on Ungrounded Visual Sudoku as SATNet with label leakage does in the grounded version of the same problem. In short, our main contributions are the following: 1. We describe a self-supervised clustering and distillation process for training a visual classiﬁer within a SATNet architecture. 2. We introduce a Symbol Grounding Loss that makes it possible to train logical constraint layers on an ungrounded symbol representation. 3. We show empirically that our methodology allows SATNet to achieve full performance on ungrounded Visual Sudoku (where label leakage is impossible), a task where previous state-of-the-art was 0%. 4. We introduce aProofreader that improves the performance of any SATNet system (grounded or ungrounded), achieving state-of-the-art performance on Visual Sudoku. 22 Background Our contribution draws from several areas. We begin with preliminaries describing the problem, before discussing related work. 2.1 The Problem MAXSAT, the optimisation analog of SAT, represents a rich set of problems to which many program complexity classes can be reduced. A MAXSAT Solver Saims to maximally satisfy a set of n boolean clauses over m variables by modulating the values of the variables. These clauses are typically written in Conjunctive Normal Form, and represented numerically as a matrix M ∈Rn×m. We can further enrich this system by partitioning our variablesa1,...,m into a subset of ﬁxed inputs ain 1,...,k, and variable outputs aout k+1,...,m. The system can then be framed functionally: aout k+1,...,m = S(ain 1,...,k,M), for 1 ≤k≤m. (1) This formulation can be used to capture Sudoku, an example used extensively in this work, where ain represents the input cells of a given Sudoku board, aout represents the cells that we aim to solve for, and M encodes the rules of Sudoku. MAXSAT Solvers can be leveraged to solve a broader class of problems that we refer to here asVisual MAXSAT Problems. These entail a MAXSAT problem where the inputs ain must ﬁrst be derived from some other representation ain visual. This essentially results in a two-step training problem for which neurosymbolic architectures are optimised3. We have now established the preliminaries necessary to describe the Symbol Grounding Problem in the context of Visual MAXSAT solvers. It is the problem of identifyingain given only ain visual and aout. This motivates the distinction between two types of Visual MAXSAT Datasets: grounded and ungrounded. An ungrounded dataset contains ain visual as data and aout as labels, while a grounded dataset additionally contains ain in its labels (See Figure 2. Figure 2: Examples of Grounded and Ungrounded Visual MAXSAT Datasets, focusing on a3 ×3 portion of a larger Sudoku board. In previous work, SATNet is able to solve only the grounded version of the problem. We note that it is signiﬁcantly more difﬁcult to solve the ungrounded version of a Visual MAXSAT problem, as training cannot be trivially broken up into two stages. It is this the class of problems that we tackle in this work. 2.2 Logical Constraint Solvers & SATNet There has been signiﬁcant recent interest in architectures that can integrate symbolic reasoning layers within neural networks. Many approaches, however, are only capable of integrating pre-existing logical constraints into these models [ 15–19]. In the context of our formalism, this is analogous to having a ﬁxed set of clauses M for a particular problem. Conversely, there exists a family of approaches that are not differentiable, but are able to learn logical constraints by example [9, 20, 21]. SATNet, however, sits somewhere in between these approaches, as it is both differentiable and able to learn a matrix M in order to ﬁt some input data [7]. There are a few other algorithms in this class, such as OptNet and ∂-Explainer [11, 12, 22]. 3While it is also possible to train a system end-to-end to derive aout directly from ain visual, we argue that internally the system would need to have some form of representation of this two-step approach regardless. 32.3 Self-Supervised Pre-Training Self-supervised pre-training has a long history in machine learning, notably being used to navigate highly non-convex loss landscapes in Deep Belief Networks (DBNs) [23, 24]. More recently, better methods for end-to-end training have emerged and self-supervision has now been used to pre-train image tasks on large, cheap unlabeled datasets to obtain slightly better performance on supervised tasks [25, 26]. In our work we return to the insight that motivated the original use of self-supervision for DBNs. The Symbol Grounding Problem essentially represents signiﬁcant non-convexity in the problem space – both symbol meanings and the way in which symbols interact with one another must be learned in parallel, with local optima existing for many combinations of prospective groundings. Self-supervised pre-training enables us to start training from a favorable position on this loss landscape. 2.4 Clustering Algorithms & InfoGAN Data clustering is a long-standing and rich ﬁeld of computer science [27]. We leverage clustering in our method in order to conduct self-supervised pre-training. While many clustering algorithms exist, for our purposes we choose to use InfoGAN, as it is able to cluster across the semantic dimension which we are interested in for MNIST with very high accuracy [28]. InfoGAN is a Generative Adversarial Network architecture which boasts disentangled, interpretable latent encodings [29]. It maximizes the mutual information between a subset of the noise fed into its generator, and the observation which the discriminator makes. It is thus able to cluster data according to several interpretable variables. In the case of MNIST, these include handwritten digit thickness, slant, and most useful to us, the actual digit shape. This latter property is what we aim to leverage in this work. Speciﬁcally, InfoGAN can cluster MNIST digits according to their numerical value with 95% accuracy in a completely unsupervised fashion [28]. 2.5 Knowledge Distillation Knowledge Distillation is a technique for training machine learning models to reach comparable performance at inference time to a larger reference model, or an ensemble of models [30–33]. While more complex distillation techniques exist, our work leverages the concept in one of its most basic forms – simply training a smaller model from a dataset generated by a larger one in order to drastically improve inference time. 3 Method Our main contribution is a pre-training pipeline used to bootstrap the learning process such that SATNet can bypass the Symbol Grounding Problem. Overall our method entails the following steps. 1. Clustering: We ﬁrst perform unsupervised clustering of the input data, and distill the knowledge of the clusters into a digit classiﬁer. 2. Self-Grounded Training: We then employ a custom Symbol Grounding Loss to identify how clusters map to the labels we have in our training data. Once the grounding is learned, we freeze it and train the rest of the system. 3. Proofreading: We conclude with an optional proofreading step which trains an additional layer in the SATNet architecture while the rest remain frozen. This was found to slightly improve performance in all SATNet architectures tested. Before diving in to each of these steps, we will formalize the composite visual understanding/logical reasoning problem. Assume we look at a single instance of a MAXSAT problem with N variables which can fall into one of Kclasses, where each of the N variables is represented by an image of size C×H×W. Our input data is then a tensor x∈RN×C×H×W , and our desired one-hot encoded output y∈RN×K. Our digit classiﬁer Dtakes input xand returns output D(x) = ˆyin ∈RN×K. We feed this result into our SATNet layer Ssuch that S(ˆyin) = ˆyout ∈RN×K. For Ungrounded Visual Sudoku using MNIST, we have N = 81(one MAXSAT variable for each cell of the9 ×9 Sudoku board), K = 9(digits 1 through 9), and C×H×W = 1×28 ×28 (MNIST images). 4Figure 3: The architecture proposed in this work. It leverages self-supervised pre-training to solve Grounded Visual Sudoku, thereby overcoming the Symbol Grounding Problem affecting the original SATNet method. 3.1 Clustering Our ﬁrst step in solving an Ungrounded MAXSAT problem is identifying the patterns that exist in the input data. Intuitively, we do not have to start training a digit classiﬁer from scratch when training composite visual reasoning architectures. There exists some semantic aspect of the input image which is of relevance to the MAXSAT problem at hand, and it can often be at least partially identiﬁed in a self-supervised setting. In the Visual Sudoku case, this entails clustering our images into 9 groups (corresponding values 1 through 9). In our experiments, we use InfoGAN to perform the clustering, as it is capable of clustering MNIST digits with 95% accuracy [28]. Any clustering algorithm may be used here however, even ones that are not differentiable. Once the clustering is complete, we can distill the clustering knowledge back into a differentiable digit classiﬁer. In our case, we generate a dataset using the clustering algorithm, and train LeNet on the cluster allocations of the training data [34]. By doing this we implicitly map each cluster onto some one-hot representation within ˆyin. However, this one-hot encoding of the MAXSAT variables may not match with the encoding present in the labels y. We deal with this next. 3.2 Self-Grounded Training While our clustering algorithm might be able to achieve high accuracy, it doesn’t have any information about which numerical digit each cluster is actually associated with, since we don’t have access to input cell labels. This is the crux of the Symbol Grounding Problem. In an Ungrounded MAXSAT setting, the only way to learn the association between digits and numerical clusters involvesjointly learning the MAXSAT problem. In the case of Sudoku, this means that we must solve for the rules of puzzle and learn what each digit means simultaneously. To reason about this, we consider two sets of encodings for digits: the pre-trained encoding (PTE) and the (correct) label encoding (LE), which we notate using superscripts. The digit classiﬁer from the previous step outputs PTE-encoded predictions ˆyPTE in . There exists some unknown permutation matrix P ∈RK×K that translates between encodings via ˆyPTE in P = ˆyLE in . Our goal is to align the PTE encoding with the LE encoding, so that we can make use of the training labels. The question of performing this translation before or after the SATNet layer is irrelevant, however. This is because the MAXSAT CNF clauses which SATNet implements are permutation-invariant [35]. This means that as long as supervision is provided correctly, we can train the SATNet layerSon either ˆyPTE in or ˆyLE in . 4 In our approach we pass the prior through SATNet, and are left with ˆyPTE out predictions. We learn the correct permutation (without access to any of the input labels) simultaneously with training the SATNet layer, by introducing aSymbol Grounding Loss, which is intended to be a smooth function that is minimized when ˆyPTE out P ≈yLE for some permutation matrix P. Note that ˆyPTE out and yLE are N ×Kmatrices, and let ˆyPTE out (i) and yLE(i) denote the ith columns of these matrices. Then, yLE(i) is a 1-hot vector capturing the entries of the output that are labeled i (in the correct label encoding), while ˆyPTE out (i) is a vector of predicted probabilities that the output is 4While this is expected, this was not explicitly stated in the original SATNet paper. We were able to verify this empirically by applying any permutation on the one-hot encodings of the digits in the nonvisual Sudoku setting and SATNet’s performance is identical even without re-training. 5labeled i(in the pre-trained label encoding). We deﬁne the following loss L: L(ˆyPTE out ,yLE) := 1−meani(max j (exp[−BCE(yLE(j),ˆyPTE out (i))])), (2) where BCE(·,·) denotes the binary cross-entropy loss between two vectors: BCE(v,w) =−1 n ( n∑ k=1 vk log(wk) + n∑ k=1 (1 −vk) log(1−wk) ) . Proposition 3.1. Suppose Lis deﬁned as in (2). Then: 1. L(ˆyPTE out ,yLE) is minimized if and only if ˆyPTE out P = yLE for some permutation matrix P. 2. In this case, the matrix P is given by Pij := exp[−BCE(yLE(j),ˆyPTE out (i))]. This proposition, which is proven in Appendix A, shows that by minimizing L, we learn an approxi- mate permutation matrix ˆP ≈P, given by: ˆPij := exp[−BCE(yLE(j),ˆyPTE out (i))]. (3) In practice, we do not minimize L, since the max function presents an obstacle to effective training. Therefore, we relax the max operation to a function approxmax. This ﬁnally gives us our Symbol Grounding Loss LSG: LSG(ˆyPTE out ,yLE) := 1−meani(approxmaxj(exp[−BCE(yLE(j),ˆyPTE out (i))])). (4) In our experiments, we setapproxmax equal to the 2-norm; however, we did not ﬁnd that performance was sensitive to the exact choice of approxmax, and other choices are also reasonable. Having deﬁned LSG, we incorporate it into our training pipeline as follows: First, we freeze the digit classiﬁer D, and train Sunder LSG. This begins to train Swhile also learning a permutation matrix ˆP ≈P (deﬁned by (3)). Note that since we are working with the Ungrounded Visual Sudoku task, the permutation matrix is learned by means of SATNet itself, and it is impossible for labels to be leaked, since the training process does not even have access to labels for the input entries. Second, once ˆP has converged to a clear permutation matrix, we freeze this permutation and use it to align the PTE labels with the correct LE labels by multiplying the ﬁnal outputs ˆyPTE out by the learned ˆP. Now that the Symbol Grounding Loss is no longer needed, we switch to the traditional BCE loss and complete the training of S, also unfreezing Dto allow additional training. 3.3 Proofreading The performance of a SATNet architecture can be improved by the addition of aProofreader layer. This consists of a linear layer added just before the SATNet layer S, initialized to a slightly noisy identity transform RN×K →RN×K. (In the Sudoku case, N = 81and K = 9.) We freeze the layers in the original model, and train only the proofreader layer. This is an optional ﬁnal step resulting in a slight performance improvement. We ﬁnd that the Proofreader layer also improves the performance of the original SATNet (with label leakage), in both the visual and nonvisual Sudoku settings. 4 Results The above procedure allows us to achieve comparable results on an Ungrounded Visual Sudoku Dataset as the original SATNet architecture has in the grounded setting5, with results being presented in Table 1. We may thus claim to solve the Symbol Grounding Problem in the case of Visual Sudoku. All experiments were carried out on a Nvidia GTX1070 across 100 epochs, with each epoch taking roughly 2 minutes. The Adam optimiser was used with learning rate of 2 ×10−3 for the SATNet layer, and 10−5 for the digit classiﬁer [36]. Standard deviations were calculated across 5 runs. We used the Sudoku Dataset made available under an MIT License from the original SATNet work [7]. 5Note that training under a grounded dataset is equivalent to the label leakage problem described in [8] 6Model Grounded vs. Total Board Per-Cell Visual Conﬁguration Ungrounded Data Accuracy (%) Accuracy (%) Accuracy (%) Original SATNet grounded 66.5 ±1.0 98.8 ±0.1 99.0 ±0.0 Original SATNet ungrounded 0 ±0.0 11.2 ±0.1 11.6 ±0.0 Our Method ungrounded 64.8 ±3.0 98.4 ±0.2 98.9 ±0.1 Table 1: Performance of our method compared to the original SATNet architecture between grounded and ungrounded versions of the Visual Sudoku problem. Note that we distinguish the total board accuracy (how many whole boards are correct) from per-cell accuracy (how many board cells are correct) and visual accuracy (how many input board cells are correct). Our method achieves comparable performance on a signiﬁcantly more difﬁcult version of the problem, thus solving the Symbol Grounding Problem. Figure 4: Permutation matrices extracted from the Symbol Grounding Loss function. On the left is a matrix extracted given a clustering with high accuracy, and the right matrix shows the results in a case where the clustering accuracy was below the necessary threshold (see Section 4.1). During our pre-training pipeline, the clustering step achieves 95.6 ±0.4% clustering accuracy. Under the Symbol Grounding Loss, our self-grounded training achieves 22.3 ±1.0% per-cell accuracy. One thing to note is that the self-grounded training step is susceptible to overﬁtting, and one needs to employ early stopping on the basis of per-cell error in order to learn the permutation matrix ˆP. See Figure 4 for an example of a learned ˆP matrix. Note that it is expected that the ungrounded version of the dataset will produce slightly worse results since it carries less information in its labels that its grounded counterpart. Another relevant aspect is that InfoGAN itself is sensitive to random seed. 4/10 runs converge to a clustering below the threshold necessary to ground symbols. We discuss this limitation further in Section 4.1. 4.1 Effect of Clustering Accuracy Figure 5: The effect of clustering accuracy on our method’s total board accuracy (blue) and per-cell accuracy during the Symbol Grounding Loss training phase (orange). Each pair of points connected by a dashed line indicates a different experiment. We note the sharp performance drop at roughly 88% clustering accuracy. An important ablation test to deﬁne some limitations of our approach is a study on the effect of clustering accuracy on our pre-training perfor- mance. It is difﬁcult to measure this, as performance could vary based on the distribution of predictions across clusters, not only raw clustering accu- racy. In this study we run our pipeline against InfoGAN at different stages of its training. In this way the cluster assignments start out uniform (based on noisy initialization) and gradually anneal to a 89.6 ±7.7% accurate clus- tering. We ﬁnd that our system re- quires roughly at least 88% clustering accuracy in order for the rest of the pipeline to progress. This is shown in Figure 5. While this is a signiﬁ- cant limitation to our approach, solv- ing Ungrounded Visual Sudoku was 7not at all possible with SATNet prior to this work. Furthermore, even once clusters are assigned, it is highly nontrivial to obtain an accurate permutation matrix to ground the clusters. 4.2 Effect of Distillation In the case that the clustering algorithm used in our ﬁrst pre-training phase is differentiable, the distillation step becomes optional. Despite this, it is desirable to distill our clustering model if there exists some smaller architecture that can achieve similar performance in the supervised setting. This is the case with InfoGAN, which in its standard form uses an architecture with 7,307,997 parameters. We distill this into a LeNet-derived architecture, with only 1,049,080 parameters and comparable performance, as shown in Table 2 [ 34]. Training speed changes from 602 ±5 seconds/epoch to 255 ±3 seconds/epoch between the two architectures. Digit Classiﬁer Digit Clustering Accuracy (%) InfoGAN 89.6 ±7.7 Distilled LeNet 86.2 ±13.5 Table 2: The effect of distilling InfoGAN into a smaller LeNet-based convolutional architecture. InfoGAN has a tendency to plateau at different clusters. A “successful” InfoGAN run will plateau at roughly 95% accuracy. 4.3 Effect of Proofreading Proofreading improves the performance of both visual and non-visual Sudoku, as seen in Table 3. We achieve the following results by training the proofreader with ungrounded Datasets even if the original model which it augments was trained with the grounded version. Model Proofreader Total Board Per-Cell Visual Conﬁguration Present? Accuracy (%) Accuracy (%) Accuracy (%) Original Non-visual no 96.6 ±0.3 99.9 ±0.0 N/A Original Non-visual yes 97.1 ±0.3 99.9 ±0.0 N/A Original Visual no 66.5 ±1.0 98.8 ±0.1 99.0 ±0.0 Original Visual yes 67.6 ±1.2 98.6 ±0.1 99.0 ±0.0 Our Method no 62.8 ±3.2 98.6 ±0.1 98.9 ±0.1 Our Method yes 64.8 ±3.0 98.4 ±0.2 98.9 ±0.1 Table 3: The effect of adding a proofreading layer to the original versions of SATNet for both visual and non-visual Sudoku datasets, as well as the pre-training method proposed in this paper. We show that a proofreader uniformly improves the performance of SATNet. We note that the numbers above from the original architectures reﬂect our reproduction of the results in the original paper. Please see Appendix B for further details. 5 Discussion 5.1 Sensitivity of SATNet to Random Seeds It was described in [ 8] that SATNet exhibits a high sensitivity to the choice of random seed. For instance, 8 out of 10 random seeds would fail even with label leakage. While we initially reproduced this behavior, such sensitivity can in fact be circumvented with a minor correction to the PyTorch implementation, detailed further in Appendix B. We use the corrected, stable model for comparison in all our results. 85.2 Incorrect Upper Performance Bound In the original SATNet paper, it is argued that the performance of the visual Sudoku model is bound by the probability of identifying all the input cells on a particular board correctly. Thus when using LeNet, which has a classiﬁcation accuracy of 99.2%, the best performance we can expect on our dataset with 36.2 input cells on average is 0.99236.2 = 74.8% [34, 7]. This is not exactly accurate. It is not necessarily true that the SATNet layer cannot solve a board correctly if some number of input cells are wrong. Intuitively, if one ﬁnds two of the same numbers as inputs in a row of a Sudoku puzzle, one can infer that one of those inputs might have been classiﬁed incorrectly. This can then be used to make an educated guess about the correct ﬁnal board state. We are able to show that the SATNet layer is actually able to reason about incorrect input cells to a certain extent. Interestingly, SATNet’s ability to reason is affected by whether an incorrectly labeled digit results in an unsolvable board or not. It is also affected by the presence of a Proofreader layer. Details on these experiments can be found in Appendix C. While the upper bound posed originally may not be strictly correct, it is still a good guideline. Deriving a strict upper performance bound is likely quite difﬁcult as the mathematics of logical problems such as Sudoku are not fully understood. 6 Limitations & Future Work While our method is able to address a new class of Visual MAXSAT problems with SATNet, it is limited by the need to prime the digit classiﬁer with correct data clusters (see Section 4.1). This imposes a constraint on which datasets can be used as visual inputs to this pipeline. One facet of this limitation is the fact that the current Symbol Grounding Loss function only supports inferring a permutation between the pre-trained encoding and the label encoding. This means that if there are K label classes, the clustering algorithm must cluster the input data accurately in Kclusters. One might imagine allowing the Symbol Grounding Loss to support a more general surjective mapping between encoding domains, allowing for a higher number of clusters (and consequently a higher accuracy). A second limitation is the tendency of the Symbol Grounding Loss to overﬁt somewhat quickly. While we experimented with several loss function formulations, further experimentation may prove useful. Implementation of regularisers in the architecture may also be an interesting avenue of research. We believe neither of these limitations is fundamental; future investigation may help to alleviate them. 7 Societal Impacts The goal of the present work is to advance methods integrating deep learning and logical reasoning, which has long been a goal of artiﬁcial intelligence and admits a broad range of applications. We also alleviate reproducibility issues with prior SATNet systems, describing in Section 5.1 how to ﬁx previously observed training instabilities [8]. Potential negative implications from our work are largely indirect and hard to assess. We envision the possibility of work on neurosymbolic methods leading to unrealistic expectations of the power of deep learning methods, which are not yet capable of sophisticated reasoning. This could lead to inappropriate trust placed in current deep learning methods, or to backlash if expectations fall short. 8 Conclusion Our work lays out a foundation for distinguishing between grounded and ungrounded variants of Visual MAXSAT problems, and presents a self-supervised pre-training methodology which enables SATNet to solve both classes. The ability to solve the more difﬁcult Ungrounded Visual MAXSAT problems contrasts markedly with the previous state of the art, which was unable to surpass 0% accuracy on these tasks. Further, we describe a proofreading methodology which can be used to incrementally improve both our architecture and prior models. This work extends the current state of the art for logical constraint-learning neurosymbolic methods, a promising area of research which boasts the potential to dramatically broaden the range of problems which machine learning can address. 9References [1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning.Nature, 521(7553):436–444, May 2015. [2] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On Evaluating Adversarial Robustness. Preprint arXiv:1902.06705, 2019. [3] Samuel Henrique Silva and Peyman Najaﬁrad. Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey. Preprint arXiv:2007.00753, 2020. [4] Erico Tjoa and Cuntai Guan. A Survey on Explainable Artiﬁcial Intelligence (XAI): Towards Medical XAI. Preprint arXiv:1907.07374, 2019. [5] Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of Gradient-Based Deep Learning. In International Conference on Machine Learning (ICML), 2017. [6] Artur d’Avila Garcez and Luis C. Lamb. Neurosymbolic AI: The 3rd Wave. Preprint arXiv:2012.05876, 2020. [7] Po-Wei Wang, Priya L. Donti, Bryan Wilder, and Zico Kolter. SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver. In International Conference on Machine Learning (ICML), 2019. [8] Oscar Chang, Lampros Flokas, Hod Lipson, and Michael Spranger. Assessing SATNet's ability to solve the symbol grounding problem. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems (NeurIPS), 2020. [9] Tao Meng and Kai-Wei Chang. An Integer Linear Programming Framework for Mining Constraints from Data. Preprint arXiv:2006.10836, 2020. [10] Stevan Harnad. The symbol grounding problem.Physica D Nonlinear Phenomena, 42(1-3):335– 346, June 1990. [11] James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, and Bryan Wilder. End-to-End Constrained Optimization Learning: A Survey. Preprint arXiv:2103.16378, 2021. [12] Brandon Amos and J. Zico Kolter. OptNet: Differentiable Optimization as a Layer in Neural Networks. In International Conference on Machine Learning (ICML), 2017. [13] Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable sorting and ranking. In International Conference on Machine Learning (ICML), 2020. [14] Marin Vlastelica Pogan ˇci´c, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations (ICLR), 2020. [15] Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT Solver from Single-Bit Supervision. In International Conference on Learning Representations (ICLR), 2019. [16] Robin Manhaeve, Sebastijan Dumanˇci´c, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. DeepProbLog: Neural Probabilistic Logic Programming. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [17] Wang-Zhou Dai, Qiu-Ling Xu, Yang Yu, and Zhi-Hua Zhou. Tunneling Neural Perception and Logic Reasoning through Abductive Learning. Preprint arXiv:1802.01173, 2018. [18] Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent Relational Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [19] Richard Evans and Edward Grefenstette. Learning Explanatory Rules from Noisy Data. Journal of Artiﬁcial Intelligence Research, 2017. 10[20] Sebastian Nowozin and Christoph Lampert. Structured learning and prediction in computer vision. Foundations and Trends in Computer Graphics and Vision, 6:185–365, 01 2011. [21] Christian Bessière, Abderrazak Daoudi, Emmanuel Hébrard, George Katsirelos, Nadjib Lazaar, Younes Mechqrane, Nina Narodytska, Claude-Guy Quimper, and Toby Walsh. New Approaches to Constraint Acquisition. In Data Mining and Constraint Programming, volume 10101 of Lecture Notes in Computer Science, pages 51–76. Springer International Publishing AG, 2016. [22] Mokanarangan Thayaparan, Marco Valentino, Deborah Ferreira, Julia Rozanova, and An- dré Freitas. ∂-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization. Preprint arXiv:2105.03417, 2021. [23] G. E. Hinton, S. Osindero, and Y . W. Teh. A fast learning algorithm for deep belief nets.Neural Comput, 18(7):1527–1554, Jul 2006. [24] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems (NeurIPS), 2007. [25] Huanru Henry Mao. A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks. Preprint arXiv:2007.00800, 2020. [26] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised Pre- Training of Image Features on Non-Curated Data. In International Conference on Computer Vision (ICCV), 2019. [27] Dongkuan Xu and Yingjie Tian. A comprehensive survey of clustering algorithms. Annals of Data Science, 2(2):165–193, Jun 2015. [28] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Ad- versarial Nets. In Advances in Neural Information Processing Systems (NeurIPS), 2016. [29] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2014. [30] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. Preprint arXiv:1503.02531, 2015. [31] Lei Jimmy Ba and Rich Caruana. Do Deep Nets Really Need to be Deep? In Advances in Neural Information Processing Systems (NeurIPS), 2014. [32] Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, V olodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy Distillation. In International Conference on Learning Representations (ICLR), 2016. [33] Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson. Do Deep Convolutional Nets Really Need to be Deep and Convolutional? In International Conference on Learning Representations (ICLR), 2016. [34] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [35] Luis C. Lamb, Artur Garcez, Marco Gori, Marcelo Prates, Pedro Avelar, and Moshe Vardi. Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2020. [36] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations (ICLR), 2015. 11A Proof of Proposition 3.1 In this section, we prove Proposition 3.1. Proposition. Suppose Lis deﬁned as in (2). Then: 1. L(ˆyPTE out ,yLE) is minimized if and only if ˆyPTE out P = yLE for some permutation matrix P. 2. In this case, the matrix P is given by Pij := exp[−BCE(yLE(j),ˆyPTE out (i))]. Proof. We ﬁrst consider part (1), recalling that: L(ˆyPTE out ,yLE) := 1−meani(max j (exp[−BCE(yLE(j),ˆyPTE out (i))])). Since the BCE loss is minimized at 0, we have: L(ˆyPTE out ,yLE) = 1−meani(max j (exp[−BCE(yLE(j),ˆyPTE out (i))])) ≥1 −meani(max j (exp[0])) = 0. and equality holds if and only if maxj(exp[−BCE(yLE(j),ˆyPTE out (i))]) = 1 for every i. This statement is true if and only if for everyithere exists a jsuch that exp[−BCE(yLE(j),ˆyPTE out (i))] = 1, or equivalently such that BCE(yLE(j),ˆyPTE out (i)) = 0. Therefore, Lreaches its minimum at 0 if and only if for every ithere exists a jsuch that yLE(j) = ˆyPTE out (i), proving part (1). We now prove part (2). Suppose that ˆyPTE out P = yLE, and suppose that σis the permutation deﬁned by P, so that σ(iPTE ) =jLE. Then, for each i,j, we have: BCE(yLE(j),ˆyPTE out (i)) = {0 if σ(i) =j +∞ otherwise, and therefore exp[−BCE(yLE(j),ˆyPTE out (i))] = {1 if σ(i) =j 0 otherwise. This proves part (2). B Random Seed Sensitivity Fix There are two aspects of the implementation which alleviate the sensitivity of SATNet on random seed. Please note that for this section we discuss the SATNet architecture from the original paper trained on an Ungrounded Visual Sudoku dataset [7]. The ﬁrst was a minor bug in the CUDA implementation of SATNet’s backprop calculation. The second relates to how supervision is provided on the digit classiﬁer Dduring training. Recall in section 2.1 that we can partition any Ungrounded Visual MAXSAT labels into input and output variable labels. Let us also reference our notation from section 3, where we have our digit classiﬁer D(x) = ˆyPTE in and our SATNet layer S(ˆyPTE in ) = ˆyPTE out . We essentially have two options for returning the architecture’s predictions for the input variables, as they are present in bothˆyPTE in and ˆyPTE out . The choice of which of these to return results in a signiﬁcant performance difference. The original SATNet model returned the input variable predictions from ˆyPTE out , whereas we return the ones in ˆyPTE in . Note that since the input variables are held constant in the SATNet layer S, the nominal value of the input variables is the same between these two returns. The only difference is the implication of how gradients are computed. 12C Effect of Error Injection for Visual Sudoku We construct an experiment by running the nonvisual Sudoku model, and perturbing input cell labels in the test datasets. Under no change to the original architecture, a SATNet layer trained on correct inputs is able to solve some small percentage of the boards with erroneous inputs. Interestingly, SATNet’s ability to solve these puzzles depends on whether the error injection resulted in an unsolvable board or not. While adding a proofreader improves performance under normal circumstances, in the presence of injected errors it worsens performance. Number of Injected Input Total Board Accuracy (%) Cell Errors per Board Solvable Unsolvable 0 96.6 ±0.3 96.6 ±0.3 1 3.2 ±2.0 0.0 ±0.0 2 0.1 ±0.0 0.0 ±0.0 3 0.0 ±0.0 0.0 ±0.0 Table 4: Solvable/Unsolvable split under error injection for traditional nonvisual Sudoku solver. Some boards are still solved by SATNet even when not all input cells are correct. Number of Injected Input Total Board Accuracy (%) Cell Errors per Board Solvable Unsolvable 0 97.1 ±0.3 97.1 ±0.3 1 1.9 ±0.0 0.0 ±0.0 2 0.0 ±0.0 0.0 ±0.0 3 0.0 ±0.0 0.0 ±0.0 Table 5: Solvable/Unsolvable split under error injection for nonvisual Sudoku solver with proofreader. D Codebases used for Experimentation Our experiments are built on top of two codebases. We leverage the SATNet implementation provided by the original authors athttps://github.com/locuslab/SATNet, in addition to an InfoGAN im- plementation available at https://github.com/Natsu6767/InfoGAN-PyTorch. We have made our implementation public at https://github.com/SeverTopan/SATNet. 13",
      "references": [
        "Deep learning.",
        "On Evaluating Adversarial Robustness.",
        "Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey.",
        "A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI.",
        "Failures of Gradient-Based Deep Learning.",
        "Neurosymbolic AI: The 3rd Wave.",
        "SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver.",
        "Assessing SATNet's ability to solve the symbol grounding problem.",
        "An Integer Linear Programming Framework for Mining Constraints from Data.",
        "The symbol grounding problem.",
        "End-to-End Constrained Optimization Learning: A Survey.",
        "OptNet: Differentiable Optimization as a Layer in Neural Networks.",
        "Fast differentiable sorting and ranking.",
        "Differentiation of blackbox combinatorial solvers.",
        "Learning a SAT Solver from Single-Bit Supervision.",
        "DeepProbLog: Neural Probabilistic Logic Programming.",
        "Tunneling Neural Perception and Logic Reasoning through Abductive Learning.",
        "Recurrent Relational Networks.",
        "Learning Explanatory Rules from Noisy Data.",
        "Structured learning and prediction in computer vision.",
        "New Approaches to Constraint Acquisition.",
        "∂-Explainer: Abductive Natural Language Inference via Differentiable Convex Optimization.",
        "A fast learning algorithm for deep belief nets.",
        "Greedy layer-wise training of deep networks.",
        "A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks.",
        "Unsupervised Pre- Training of Image Features on Non-Curated Data.",
        "A comprehensive survey of clustering algorithms.",
        "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.",
        "Generative Adversarial Networks.",
        "Distilling the Knowledge in a Neural Network.",
        "Do Deep Nets Really Need to be Deep?",
        "Policy Distillation.",
        "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?",
        "Gradient-based learning applied to document recognition.",
        "Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective.",
        "Adam: A Method for Stochastic Optimization."
      ],
      "meta_data": {
        "arxiv_id": "2106.11072v1",
        "authors": [
          "Sever Topan",
          "David Rolnick",
          "Xujie Si"
        ],
        "published_date": "2021-06-16T18:42:12Z",
        "github_url": "https://github.com/locuslab/SATNet"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the Symbol Grounding Problem in neurosymbolic systems by developing a self-supervised pre-training pipeline that enables SATNet, a differentiable MAXSAT solver, to work without explicit intermediate supervision. It introduces a Symbol Grounding Loss for aligning pre-trained digit encodings with label encodings and introduces a proofreading module to further enhance performance, achieving state-of-the-art results in Ungrounded Visual Sudoku.",
        "methodology": "The approach involves an initial unsupervised clustering of digit images using InfoGAN, followed by distillation into a smaller digit classifier (e.g., LeNet). A custom Symbol Grounding Loss is then applied to learn a permutation matrix that aligns the pre-trained encoding with the correct label encoding within the SATNet architecture. Finally, a proofreading layer is added to incrementally improve the SATNet's final outputs.",
        "experimental_setup": "Experiments are conducted on a Visual Sudoku task using MNIST images, where each board consists of 81 cells and each image is 28x28 pixels. The setup involves both grounded and ungrounded versions of the dataset, measuring total board accuracy, per-cell accuracy, and visual configuration accuracy. The experiments use standard deep learning techniques (e.g., Adam optimizer) on a Nvidia GTX1070 across 100 epochs with ablation studies evaluating clustering accuracy, distillation effects, and the impact of the proofreading layer.",
        "limitations": "The method requires high clustering accuracy (approximately 88% or above) to successfully ground symbols, making it sensitive to the initialization and performance of the clustering algorithm (InfoGAN). Additionally, the Symbol Grounding Loss tends to overfit quickly, necessitating careful use of early stopping or regularization. The current approach assumes an exact match between the number of clusters and label classes, which may limit its applicability to datasets with more complex or variable structures.",
        "future_research_directions": "Future work could focus on developing more robust and generalizable Symbol Grounding Loss functions, potentially supporting surjective mappings between different encoding domains. Research could also explore regularization techniques to mitigate overfitting, extensions to other neurosymbolic tasks beyond Sudoku, and improvements in the clustering/distillation process to make the pipeline more resilient to variations in input data.",
        "experimental_code": "# File: exps/parity.py\nimport argparse\nimport os\nimport sys\nimport csv\nimport shutil\nimport numpy.random as npr\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport satnet\nfrom tqdm.auto import tqdm\n\nclass CSVLogger(object):\n\n    def __init__(self, fname):\n        self.f = open(fname, 'w')\n        self.logger = csv.writer(self.f)\n\n    def log(self, fields):\n        self.logger.writerow(fields)\n        self.f.flush()\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='parity')\n    parser.add_argument('--testPct', type=float, default=0.1)\n    parser.add_argument('--batchSz', type=int, default=100)\n    parser.add_argument('--testBatchSz', type=int, default=500)\n    parser.add_argument('--nEpoch', type=int, default=100)\n    parser.add_argument('--lr', type=float, default=0.1)\n    parser.add_argument('--seq', type=int, default=20)\n    parser.add_argument('--save', type=str)\n    parser.add_argument('--m', type=int, default=4)\n    parser.add_argument('--aux', type=int, default=4)\n    parser.add_argument('--no_cuda', action='store_true')\n    parser.add_argument('--adam', action='store_true')\n    args = parser.parse_args()\n    npr.seed(1)\n    torch.manual_seed(7)\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.cuda:\n        print('Using', torch.cuda.get_device_name(0))\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.init()\n    save = 'parity.aux{}-m{}-lr{}-bsz{}'.format(args.aux, args.m, args.lr, args.batchSz)\n    if args.save:\n        save = '{}-{}'.format(args.save, save)\n    save = os.path.join('logs', save)\n    if os.path.isdir(save):\n        shutil.rmtree(save)\n    os.makedirs(save)\n    L = args.seq\n    with open(os.path.join(args.data_dir, str(L), 'features.pt'), 'rb') as f:\n        X = torch.load(f).float()\n    with open(os.path.join(args.data_dir, str(L), 'labels.pt'), 'rb') as f:\n        Y = torch.load(f).float()\n    if args.cuda:\n        X, Y = (X.cuda(), Y.cuda())\n    N = X.size(0)\n    nTrain = int(N * (1 - args.testPct))\n    nTest = N - nTrain\n    assert nTrain % args.batchSz == 0\n    assert nTest % args.testBatchSz == 0\n    train_is_input = torch.IntTensor([1, 1, 0]).repeat(nTrain, 1)\n    test_is_input = torch.IntTensor([1, 1, 0]).repeat(nTest, 1)\n    if args.cuda:\n        train_is_input, test_is_input = (train_is_input.cuda(), test_is_input.cuda())\n    train_set = TensorDataset(X[:nTrain], train_is_input, Y[:nTrain])\n    test_set = TensorDataset(X[nTrain:], test_is_input, Y[nTrain:])\n    model = satnet.SATNet(3, args.m, args.aux, prox_lam=0.1)\n    if args.cuda:\n        model = model.cuda()\n    if args.adam:\n        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n    else:\n        optimizer = optim.SGD(model.parameters(), lr=args.lr)\n    train_logger = CSVLogger(os.path.join(save, 'train.csv'))\n    test_logger = CSVLogger(os.path.join(save, 'test.csv'))\n    fields = ['epoch', 'loss', 'err']\n    train_logger.log(fields)\n    test_logger.log(fields)\n    test(0, model, optimizer, test_logger, test_set, args.testBatchSz)\n    for epoch in range(1, args.nEpoch + 1):\n        train(epoch, model, optimizer, train_logger, train_set, args.batchSz)\n        test(epoch, model, optimizer, test_logger, test_set, args.testBatchSz)\n\ndef apply_seq(net, zeros, batch_data, batch_is_inputs, batch_targets):\n    y = torch.cat([batch_data[:, :2], zeros], dim=1)\n    y = net(y, batch_is_inputs)\n    L = batch_data.size(1)\n    for i in range(L - 2):\n        y = torch.cat([y[:, -1].unsqueeze(1), batch_data[:, i + 2].unsqueeze(1), zeros], dim=1)\n        y = net(((y - 0.5).sign() + 1) / 2, batch_is_inputs)\n    loss = F.binary_cross_entropy(y[:, -1], batch_targets[:, -1])\n    return (loss, y)\n\ndef run(epoch, model, optimizer, logger, dataset, batchSz, to_train):\n    loss_final, err_final = (0, 0)\n    loader = DataLoader(dataset, batch_size=batchSz)\n    tloader = tqdm(enumerate(loader), total=len(loader))\n    start = torch.zeros(batchSz, 1)\n    if next(model.parameters()).is_cuda:\n        start = start.cuda()\n    for i, (data, is_input, label) in tloader:\n        if to_train:\n            optimizer.zero_grad()\n        loss, pred = apply_seq(model, start, data, is_input, label)\n        if to_train:\n            loss.backward()\n            optimizer.step()\n        err = computeErr(pred, label)\n        tloader.set_description('Epoch {} {} Loss {:.4f} Err: {:.4f}'.format(epoch, 'Train' if to_train else 'Test ', loss.item(), err))\n        loss_final += loss.item()\n        err_final += err\n    loss_final, err_final = (loss_final / len(loader), err_final / len(loader))\n    logger.log((epoch, loss_final, err_final))\n    if not to_train:\n        print('TESTING SET RESULTS: Average loss: {:.4f} Err: {:.4f}'.format(loss_final, err_final))\n\ndef train(epoch, model, optimizer, logger, dataset, batchSz):\n    run(epoch, model, optimizer, logger, dataset, batchSz, True)\n\n@torch.no_grad()\ndef test(epoch, model, optimizer, logger, dataset, batchSz):\n    run(epoch, model, optimizer, logger, dataset, batchSz, False)\n\n@torch.no_grad()\ndef computeErr(pred, target):\n    y = pred[:, -1] - 0.5\n    t = target[:, -1] - 0.5\n    correct = ((y * t).sign() + 1.0) / 2\n    acc = correct.sum().float() / target.size(0)\n    return 1 - float(acc)\nif __name__ == '__main__':\n    main()\n\n# File: exps/sudoku.py\nimport argparse\nimport os\nimport shutil\nimport csv\nimport numpy as np\nimport numpy.random as npr\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom tqdm.auto import tqdm\nimport satnet\n\nclass SudokuSolver(nn.Module):\n\n    def __init__(self, boardSz, aux, m):\n        super(SudokuSolver, self).__init__()\n        n = boardSz ** 6\n        self.sat = satnet.SATNet(n, m, aux)\n\n    def forward(self, y_in, mask):\n        out = self.sat(y_in, mask)\n        return out\n\nclass DigitConv(nn.Module):\n    \"\"\"\n    Convolutional neural network for MNIST digit recognition. From:\n    https://github.com/pytorch/examples/blob/master/mnist/main.py\n    \"\"\"\n\n    def __init__(self):\n        super(DigitConv, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)[:, :9].contiguous()\n\nclass MNISTSudokuSolver(nn.Module):\n\n    def __init__(self, boardSz, aux, m):\n        super(MNISTSudokuSolver, self).__init__()\n        self.digit_convnet = DigitConv()\n        self.sudoku_solver = SudokuSolver(boardSz, aux, m)\n        self.boardSz = boardSz\n        self.nSq = boardSz ** 2\n\n    def forward(self, x, is_inputs):\n        nBatch = x.shape[0]\n        x = x.flatten(start_dim=0, end_dim=1)\n        digit_guess = self.digit_convnet(x)\n        puzzles = digit_guess.view(nBatch, self.nSq * self.nSq * self.nSq)\n        solution = self.sudoku_solver(puzzles, is_inputs)\n        return solution\n\nclass CSVLogger(object):\n\n    def __init__(self, fname):\n        self.f = open(fname, 'w')\n        self.logger = csv.writer(self.f)\n\n    def log(self, fields):\n        self.logger.writerow(fields)\n        self.f.flush()\n\nclass FigLogger(object):\n\n    def __init__(self, fig, base_ax, title):\n        self.colors = ['tab:red', 'tab:blue']\n        self.labels = ['Loss (entropy)', 'Error']\n        self.markers = ['d', '.']\n        self.axes = [base_ax, base_ax.twinx()]\n        base_ax.set_xlabel('Epochs')\n        base_ax.set_title(title)\n        for i, ax in enumerate(self.axes):\n            ax.set_ylabel(self.labels[i], color=self.colors[i])\n            ax.tick_params(axis='y', labelcolor=self.colors[i])\n        self.reset()\n        self.fig = fig\n\n    def log(self, args):\n        for i, arg in enumerate(args[-2:]):\n            self.curves[i].append(arg)\n            x = list(range(len(self.curves[i])))\n            self.axes[i].plot(x, self.curves[i], self.colors[i], marker=self.markers[i])\n            self.axes[i].set_ylim(0, 1.05)\n        self.fig.canvas.draw()\n\n    def reset(self):\n        for ax in self.axes:\n            for line in ax.lines:\n                line.remove()\n        self.curves = [[], []]\n\ndef print_header(msg):\n    print('===>', msg)\n\ndef find_unperm(perm):\n    unperm = torch.zeros_like(perm)\n    for i in range(perm.size(0)):\n        unperm[perm[i]] = i\n    return unperm\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='sudoku')\n    parser.add_argument('--boardSz', type=int, default=3)\n    parser.add_argument('--batchSz', type=int, default=40)\n    parser.add_argument('--testBatchSz', type=int, default=40)\n    parser.add_argument('--aux', type=int, default=300)\n    parser.add_argument('--m', type=int, default=600)\n    parser.add_argument('--nEpoch', type=int, default=100)\n    parser.add_argument('--testPct', type=float, default=0.1)\n    parser.add_argument('--lr', type=float, default=0.002)\n    parser.add_argument('--save', type=str)\n    parser.add_argument('--model', type=str)\n    parser.add_argument('--no_cuda', action='store_true')\n    parser.add_argument('--mnist', action='store_true')\n    parser.add_argument('--perm', action='store_true')\n    args = parser.parse_args()\n    npr.seed(1)\n    torch.manual_seed(7)\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.cuda:\n        print('Using', torch.cuda.get_device_name(0))\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.init()\n    save = 'sudoku{}{}.boardSz{}-aux{}-m{}-lr{}-bsz{}'.format('.perm' if args.perm else '', '.mnist' if args.mnist else '', args.boardSz, args.aux, args.m, args.lr, args.batchSz)\n    if args.save:\n        save = '{}-{}'.format(args.save, save)\n    save = os.path.join('logs', save)\n    if os.path.isdir(save):\n        shutil.rmtree(save)\n    os.makedirs(save)\n    print_header('Loading data')\n    with open(os.path.join(args.data_dir, 'features.pt'), 'rb') as f:\n        X_in = torch.load(f)\n    with open(os.path.join(args.data_dir, 'features_img.pt'), 'rb') as f:\n        Ximg_in = torch.load(f)\n    with open(os.path.join(args.data_dir, 'labels.pt'), 'rb') as f:\n        Y_in = torch.load(f)\n    with open(os.path.join(args.data_dir, 'perm.pt'), 'rb') as f:\n        perm = torch.load(f)\n    N = X_in.size(0)\n    nTrain = int(N * (1.0 - args.testPct))\n    nTest = N - nTrain\n    assert nTrain % args.batchSz == 0\n    assert nTest % args.testBatchSz == 0\n    print_header('Forming inputs')\n    X, Ximg, Y, is_input = process_inputs(X_in, Ximg_in, Y_in, args.boardSz)\n    data = Ximg if args.mnist else X\n    if args.cuda:\n        data, is_input, Y = (data.cuda(), is_input.cuda(), Y.cuda())\n    unperm = None\n    if args.perm and (not args.mnist):\n        print('Applying permutation')\n        data[:, :], Y[:, :], is_input[:, :] = (data[:, perm], Y[:, perm], is_input[:, perm])\n        unperm = find_unperm(perm)\n    train_set = TensorDataset(data[:nTrain], is_input[:nTrain], Y[:nTrain])\n    test_set = TensorDataset(data[nTrain:], is_input[nTrain:], Y[nTrain:])\n    print_header('Building model')\n    if args.mnist:\n        model = MNISTSudokuSolver(args.boardSz, args.aux, args.m)\n    else:\n        model = SudokuSolver(args.boardSz, args.aux, args.m)\n    if args.cuda:\n        model = model.cuda()\n    if args.mnist:\n        optimizer = optim.Adam([{'params': model.sudoku_solver.parameters(), 'lr': args.lr}, {'params': model.digit_convnet.parameters(), 'lr': 1e-05}])\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n    if args.model:\n        model.load_state_dict(torch.load(args.model))\n    train_logger = CSVLogger(os.path.join(save, 'train.csv'))\n    test_logger = CSVLogger(os.path.join(save, 'test.csv'))\n    fields = ['epoch', 'loss', 'err']\n    train_logger.log(fields)\n    test_logger.log(fields)\n    test(args.boardSz, 0, model, optimizer, test_logger, test_set, args.testBatchSz, unperm)\n    for epoch in range(1, args.nEpoch + 1):\n        train(args.boardSz, epoch, model, optimizer, train_logger, train_set, args.batchSz, unperm)\n        test(args.boardSz, epoch, model, optimizer, test_logger, test_set, args.testBatchSz, unperm)\n\ndef process_inputs(X, Ximg, Y, boardSz):\n    is_input = X.sum(dim=3, keepdim=True).expand_as(X).int().sign()\n    Ximg = Ximg.flatten(start_dim=1, end_dim=2)\n    Ximg = Ximg.unsqueeze(2).float()\n    X = X.view(X.size(0), -1)\n    Y = Y.view(Y.size(0), -1)\n    is_input = is_input.view(is_input.size(0), -1)\n    return (X, Ximg, Y, is_input)\n\ndef run(boardSz, epoch, model, optimizer, logger, dataset, batchSz, to_train=False, unperm=None):\n    loss_final, err_final = (0, 0)\n    loader = DataLoader(dataset, batch_size=batchSz)\n    tloader = tqdm(enumerate(loader), total=len(loader))\n    for i, (data, is_input, label) in tloader:\n        if to_train:\n            optimizer.zero_grad()\n        preds = model(data.contiguous(), is_input.contiguous())\n        loss = nn.functional.binary_cross_entropy(preds, label)\n        if to_train:\n            loss.backward()\n            optimizer.step()\n        err = computeErr(preds.data, boardSz, unperm) / batchSz\n        tloader.set_description('Epoch {} {} Loss {:.4f} Err: {:.4f}'.format(epoch, 'Train' if to_train else 'Test ', loss.item(), err))\n        loss_final += loss.item()\n        err_final += err\n    loss_final, err_final = (loss_final / len(loader), err_final / len(loader))\n    logger.log((epoch, loss_final, err_final))\n    if not to_train:\n        print('TESTING SET RESULTS: Average loss: {:.4f} Err: {:.4f}'.format(loss_final, err_final))\n    torch.cuda.empty_cache()\n\ndef train(args, epoch, model, optimizer, logger, dataset, batchSz, unperm=None):\n    run(args, epoch, model, optimizer, logger, dataset, batchSz, True, unperm)\n\n@torch.no_grad()\ndef test(args, epoch, model, optimizer, logger, dataset, batchSz, unperm=None):\n    run(args, epoch, model, optimizer, logger, dataset, batchSz, False, unperm)\n\n@torch.no_grad()\ndef computeErr(pred_flat, n, unperm):\n    if unperm is not None:\n        pred_flat[:, :] = pred_flat[:, unperm]\n    nsq = n ** 2\n    pred = pred_flat.view(-1, nsq, nsq, nsq)\n    batchSz = pred.size(0)\n    s = (nsq - 1) * nsq // 2\n    I = torch.max(pred, 3)[1].squeeze().view(batchSz, nsq, nsq)\n\n    def invalidGroups(x):\n        valid = x.min(1)[0] == 0\n        valid *= x.max(1)[0] == nsq - 1\n        valid *= x.sum(1) == s\n        return valid.bitwise_not()\n    boardCorrect = torch.ones(batchSz).type_as(pred)\n    for j in range(nsq):\n        boardCorrect[invalidGroups(I[:, j, :])] = 0\n        boardCorrect[invalidGroups(I[:, :, j])] = 0\n        row, col = (n * (j // n), n * (j % n))\n        M = invalidGroups(I[:, row:row + n, col:col + n].contiguous().view(batchSz, -1))\n        boardCorrect[M] = 0\n        if boardCorrect.sum() == 0:\n            return batchSz\n    return float(batchSz - boardCorrect.sum())\nif __name__ == '__main__':\n    main()\n\n# File: satnet/models.py\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nimport torch.optim as optim\nimport satnet._cpp\nif torch.cuda.is_available():\n    import satnet._cuda\n\ndef get_k(n):\n    return int((2 * n) ** 0.5 + 3) // 4 * 4\n\nclass MixingFunc(Function):\n    \"\"\"Apply the Mixing method to the input probabilities.\n\n    Args: see SATNet.\n\n    Impl Note: \n        The SATNet is a wrapper for the MixingFunc,\n        handling the initialization and the wrapping of auxiliary variables.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, S, z, is_input, max_iter, eps, prox_lam):\n        B, n, m, k = (z.size(0), S.size(0), S.size(1), 32)\n        ctx.prox_lam = prox_lam\n        device = 'cuda' if S.is_cuda else 'cpu'\n        ctx.g, ctx.gnrm = (torch.zeros(B, k, device=device), torch.zeros(B, n, device=device))\n        ctx.index = torch.zeros(B, n, dtype=torch.int, device=device)\n        ctx.is_input = torch.zeros(B, n, dtype=torch.int, device=device)\n        ctx.V, ctx.W = (torch.zeros(B, n, k, device=device).normal_(), torch.zeros(B, k, m, device=device))\n        ctx.z = torch.zeros(B, n, device=device)\n        ctx.niter = torch.zeros(B, dtype=torch.int, device=device)\n        ctx.S = torch.zeros(n, m, device=device)\n        ctx.Snrms = torch.zeros(n, device=device)\n        ctx.z[:] = z.data\n        ctx.S[:] = S.data\n        ctx.is_input[:] = is_input.data\n        perm = torch.randperm(n - 1, dtype=torch.int, device=device)\n        satnet_impl = satnet._cuda if S.is_cuda else satnet._cpp\n        satnet_impl.init(perm, is_input, ctx.index, ctx.z, ctx.V)\n        for b in range(B):\n            ctx.W[b] = ctx.V[b].t().mm(ctx.S)\n        ctx.Snrms[:] = S.norm(dim=1) ** 2\n        satnet_impl.forward(max_iter, eps, ctx.index, ctx.niter, ctx.S, ctx.z, ctx.V, ctx.W, ctx.gnrm, ctx.Snrms, ctx.g)\n        return ctx.z.clone()\n\n    @staticmethod\n    def backward(ctx, dz):\n        B, n, m, k = (dz.size(0), ctx.S.size(0), ctx.S.size(1), 32)\n        device = 'cuda' if ctx.S.is_cuda else 'cpu'\n        ctx.dS = torch.zeros(B, n, m, device=device)\n        ctx.U, ctx.Phi = (torch.zeros(B, n, k, device=device), torch.zeros(B, k, m, device=device))\n        ctx.dz = torch.zeros(B, n, device=device)\n        ctx.dz[:] = dz.data\n        satnet_impl = satnet._cuda if ctx.S.is_cuda else satnet._cpp\n        satnet_impl.backward(ctx.prox_lam, ctx.is_input, ctx.index, ctx.niter, ctx.S, ctx.dS, ctx.z, ctx.dz, ctx.V, ctx.U, ctx.W, ctx.Phi, ctx.gnrm, ctx.Snrms, ctx.g)\n        ctx.dS = ctx.dS.sum(dim=0)\n        return (ctx.dS, ctx.dz, None, None, None, None)\n\ndef insert_constants(x, pre, n_pre, app, n_app):\n    \"\"\" prepend and append torch tensors\n    \"\"\"\n    one = x.new(x.size()[0], 1).fill_(1)\n    seq = []\n    if n_pre != 0:\n        seq.append((pre * one).expand(-1, n_pre))\n    seq.append(x)\n    if n_app != 0:\n        seq.append((app * one).expand(-1, n_app))\n    r = torch.cat(seq, dim=1)\n    r.requires_grad = False\n    return r\n\nclass SATNet(nn.Module):\n    \"\"\"Apply a SATNet layer to complete the input probabilities.\n\n    Args:\n        n: Number of input variables.\n        m: Rank of the clause matrix.\n        aux: Number of auxiliary variables.\n\n        max_iter: Maximum number of iterations for solving\n            the inner optimization problem.\n            Default: 40\n        eps: The stopping threshold for the inner optimizaiton problem.\n            The inner Mixing method will stop when the function decrease\n            is less then eps times the initial function decrease.\n            Default: 1e-4\n        prox_lam: The diagonal increment in the backward linear system\n            to make the backward pass more stable.\n            Default: 1e-2\n        weight_normalize: Set true to perform normlization for init weights.\n            Default: True\n\n    Inputs: (z, is_input)\n        **z** of shape `(batch, n)`: \n            Float tensor containing the probabilities (must be in [0,1]).\n        **is_input** of shape `(batch, n)`: \n            Int tensor indicating which **z** is a input.\n\n    Outputs: z\n        **z** of shape `(batch, n)`:\n            The prediction probabiolities.\n\n    Attributes: S\n        **S** of shape `(n, m)`:\n            The learnable clauses matrix containing `m` clauses \n            for the `n` variables.\n\n    Examples:\n        >>> sat = satnet.SATNet(3, 4, aux=5)\n        >>> z = torch.randn(2, 3)\n        >>> is_input = torch.IntTensor([[1, 1, 0], [1,0,1]])\n        >>> pred = sat(z, is_input)\n    \"\"\"\n\n    def __init__(self, n, m, aux=0, max_iter=40, eps=0.0001, prox_lam=0.01, weight_normalize=True):\n        super(SATNet, self).__init__()\n        S_t = torch.FloatTensor(n + 1 + aux, m)\n        S_t = S_t.normal_()\n        if weight_normalize:\n            S_t = S_t * (0.5 / (n + 1 + aux + m)) ** 0.5\n        self.S = nn.Parameter(S_t)\n        self.aux = aux\n        self.max_iter, self.eps, self.prox_lam = (max_iter, eps, prox_lam)\n\n    def forward(self, z, is_input):\n        B = z.size(0)\n        device = 'cuda' if self.S.is_cuda else 'cpu'\n        m = self.S.shape[1]\n        if device == 'cpu' and m % 4 != 0:\n            raise ValueError('m is required to be a multiple of 4 on CPU for SSE acceleration. Now ' + str(m))\n        is_input = insert_constants(is_input.data, 1, 1, 0, self.aux)\n        z = torch.cat([torch.ones(z.size(0), 1, device=device), z, torch.zeros(z.size(0), self.aux, device=device)], dim=1)\n        z = MixingFunc.apply(self.S, z, is_input, self.max_iter, self.eps, self.prox_lam)\n        return z[:, 1:self.S.size(0) - self.aux]",
        "experimental_info": ""
      }
    },
    {
      "title": "Soft Augmentation for Image Classification",
      "full_text": "Soft Augmentation for Image Classification Yang Liu, Shen Yan, Laura Leal-Taixé, James Hays, Deva Ramanan Argo AI youngleoel@gmail.com, shenyan@google.com, leal.taixe@tum.de, hays@gatech.edu, deva@cs.cmu.edu Abstract Modern neural networks are over-parameterized and thus rely on strong regularization such as data augmenta- tion and weight decay to reduce overfitting and improve generalization. The dominant form of data augmentation applies invariant transforms, where the learning target of a sample is invariant to the transform applied to that sam- ple. We draw inspiration from human visual classifica- tion studies and propose generalizing augmentation with invariant transforms to soft augmentation where the learn- ing target softens non-linearly as a function of the de- gree of the transformapplied to the sample: e.g., more ag- gressive image crop augmentations produce less confident learning targets. We demonstrate that soft targets allow for more aggressive data augmentation, offer more robust performance boosts, work with other augmentation poli- cies, and interestingly, produce better calibrated models (since they are trained to be less confident on aggressively cropped/occluded examples). Combined with existing ag- gressive augmentation strategies, soft targets 1) double the top-1 accuracy boost across Cifar-10, Cifar-100, ImageNet- 1K, and ImageNet-V2, 2) improve model occlusion perfor- mance by up to 4×, and 3) half the expected calibration error (ECE). Finally, we show that soft augmentation gen- eralizes to self-supervised classification tasks. Code avail- able at https://github.com/youngleox/soft_ augmentation 1. Introduction Deep neural networks have enjoyed great success in the past decade in domains such as visual understanding [42], natural language processing [5], and protein structure pre- diction [41]. However, modern deep learning models are often over-parameterized and prone to overfitting. In addi- tion to designing models with better inductive biases, strong regularization techniques such as weight decay and data augmentation are often necessary for neural networks to achieve ideal performance. Data augmentation is often a computationally cheap and effective way to regularize mod- els and mitigate overfitting. The dominant form of data aug- mentation modifies training samples with invariant trans- forms – transformations of the data where it is assumed that the identity of the sample is invariant to the transforms. Indeed, the notion of visual invariance is supported by evidence found from biological visual systems [54]. The robustness of human visual recognition has long been docu- mented and inspired many learning methods including data augmentation and architectural improvement [19, 47]. This paper focuses on the counterpart of human visual robust- ness, namely how our vision fails. Instead of maintaining perfect invariance, human visual confidence degrades non- linearly as a function of the degree of transforms such as occlusion, likely as a result of information loss [44]. We propose modeling the transform-induced information loss for learned image classifiers and summarize the contribu- tions as follows: • We propose Soft Augmentation as a generalization of data augmentation with invariant transforms. With Soft Aug- mentation, the learning target of a transformed training sample softens. We empirically compare several soften- ing strategies and prescribe a robust non-linear softening formula. • With a frozen softening strategy, we show that replac- ing standard crop augmentation with soft crop augmenta- tion allows for more aggressive augmentation, and dou- bles the top-1 accuracy boost of RandAugment [8] across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. • Soft Augmentation improves model occlusion robustness by achieving up to more than 4× Top-1 accuracy boost on heavily occluded images. • Combined with TrivialAugment [37], Soft Augmentation further reduces top-1 error and improves model calibra- tion by reducing expected calibration error by more than half, outperforming 5-ensemble methods [25]. • In addition to supervised image classification models, Soft Augmentation also boosts the performance of self- supervised models, demonstrating its generalizability. arXiv:2211.04625v2  [cs.CV]  23 Jan 2024-32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty Top-1 Error: 20.80 Standard Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 22.99(+2.19) Aggressive Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 18.31(−2.49) Soft Augmentation 0 1 0 1 0 1 Target Confidence (p) original image 77% visible  38% visible  22% visible Figure 1. Traditional augmentation encourages invariance by requiring augmented samples to produce the same target label; we visualize the translational offset range (tx, ty) of Standard Hard Crop augmentations for 32 × 32 images from Cifar-100 on the left, reporting the top-1 error of a baseline ResNet-18. Naively increasing the augmentation range without reducing target confidence increases error (middle), but softening the target label by reducing the target confidence for extreme augmentations reduces the error ( right), allowing for training with even more aggressive augmentations that may even produce blank images. Our work also shows that soft augmentations produce models that are more robust to occlusions (since they encounter larger occlusions during training) and models that are better calibrated (since they are trained to be less-confident on such occluded examples). 2. Related Work 2.1. Neural Networks for Vision Since the seminal work from Krizhevskyet al. [24], neu- ral networks have been the dominant class of high per- forming visual classifiers. Convolutional Neural Networks (CNNs) are a popular family of high performing neural models which borrows a simple idea of spatially local com- putations from biological vision [12, 18, 26]. With the help of architectural improvements [15], auxiliary loss [42], and improved computational power [13], deeper, larger, and more efficient neural nets have been developed in the past decade. 2.2. Data Augmentation Data augmentation has been an essential regularizer for high performing neural networks in many domains includ- ing visual recognition. While many other regularization techniques such as weight decay [32] and batch normal- ization [4] are shown to be optional, we are aware of no competitive vision models that omit data augmentation. Accompanying the influential AlexNet model, Krizhevsky et al . [24] proposed horizontal flipping and random cropping transforms which became the back- bone of image data augmentation. Since the repertoire of invariant transformations has grown significantly in the past decade [42], choosing which subset to use and then finding the optimal hyperparameters for each transform has become computationally burdensome. This sparked a line of research [7, 28] which investigates optimal policies for data augmentation such as RandAugment [8] and TrivialAugment [37]. 2.3. Learning from Soft Targets While minimizing the cross entropy loss between model logits and hard one-hot targets remains the go-to recipe for supervised classification training, learning with soft targets has emerged in many lines of research. Label Smooth- ing [36, 43] is a straightforward method which applies a fixed smoothing (softening) factor α to the hard one-hot classification target. The motivation is that label smoothing prevents the model from becoming over-confident. Müller et al. [36] shows that label smoothing is related to knowl- edge distillation [17], where a student model learns the soft distribution of a (typically) larger teacher model. A related line of research [49,53] focuses on regularizing how a model interpolates between samples by linearly mix- ing two or more samples and linearly softening the result- ing learning targets. Mixing can be in the form of per-pixel blending [53] or patch-level recombination [49]. 2.4. Robustness of Human Vision Human visual classification is known to be robust against perturbations such as occlusion. In computer vision re- search, the robustness of human vision is often regarded as the gold standard for designing computer vision mod- els [34, 54]. These findings indeed inspire development of robust vision models, such as compositional, recurrent, and occlusion aware models [22,46,47]. In addition to specialty models, much of the idea of using invariant transforms toaugment training samples come from the intuition and ob- servation that human vision are robust against these trans- forms such as object translation, scaling, occlusion, photo- metric distortions, etc. Recent studies such as Tang et al. [44] indeed confirm the robustness of human visual recognition against mild to moderate perturbations. In a 5-class visual classifica- tion task, human subjects maintain high accuracy when up to approximately half of an object is occluded. However, the more interesting observation is that human performance starts to degenerate rapidly as occlusion increases and falls to chance level when the object is fully occluded (see Figure 2 right k = 2, 3, 4 for qualitative curves). 3. Soft Augmentation In a typical supervised image classification setting, each training image xi has a ground truth learning target yi asso- ciated to it thus forming tuples: (xi, yi), (1) where xi ∈ RC×W×H denotes the image and yi ∈ [0, 1]N denotes a N-dimensional one-hot vector representing the target label (Figure 2 left, “Hard Target”). As modern neural models have the capacity to memorize even large datasets [1], data augmentation mitigates the issue by hal- lucinating data points through transformations of existing training samples. (Hard) data augmentation relies on the key underlying assumption that the augmented variant of xi should main- tain the original target label yi: (xi, yi) ⇒ (tϕ∼S(xi), yi) , Hard Augmentation (2) where tϕ∼S(xi) denotes the image transform applied to sample xi, ϕ is a random sample from the fixed transform range S. Examples of image transforms include transla- tion, rotation, crop, noise injection, etc. As shown by Tang et al. [44], transforms of xi such as occlusion are approxi- mately perceptually invariant only whenϕ is mild. Hence S often has to be carefully tuned in practice, since naively in- creasing it can lead to degraded performance (Figure 1). In the extreme case of 100% occlusion, total information loss occurs, making it detrimental for learning. Label Smoothing applies a smoothing function g to the target label yi parameterized by a handcrafted, fixed smoothing factor α. Specifically, label smoothing replaces the indicator value ‘1’ (for the ground-truth class label) with p = 1 − α, distributing the remaining α probability mass across all other class labels (Figure 2 left, “Soft Target”). One can interpret label smoothing as accounting for the average loss of information resulting from averaging over transforms from the range S. From this perspective, the smoothing factor α can be written as a function of the fixed transform range S: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(S)(yi) \u0001 , Label Smoothing (3) Soft Augmentation, our proposed approach, can now be described succinctly as follows: replace the fixed smoothing factor α(S) with an adaptive smoothing factor α(ϕ), that depends on the degree of thespecific sampled augmentation ϕ applied to xi: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi) \u0001 , Soft Augmentation (Target) (4) Crucially, conditioning on the information loss from a par- ticular ϕ allows one to define far larger augmentation ranges S. We will show that such a strategy consistently produces robust performance improvements with little tun- ing across a variety of datasets, models, and augmentation strategies. Extensions to Soft Augmentation may be proposed by also considering loss reweighting [40, 48], which is an al- ternative approach for softening the impact of an augmented example by down-weighting its contribution to the loss. To formalize this, let us write the training samples of a super- vised dataset as triples including a weight factor wi (that is typically initialized to all ‘1’s). One can then re-purpose our smoothing function g to modify the weight instead of (or in addition to) the target label (Figure 2 left): (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), yi, gα(ϕ)(wi) \u0001 , Soft Augmentation (Weight) (5) (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi), gα(ϕ)(wi) \u0001 . Soft Augmentation (Target & Weight) (6) Finally, one may wish to soften targets by exploit- ing class-specific confusions when applying α(ϕ); the smoothed target label of a highly-occluded truck example could place more probability mass on other vehicle classes, as opposed to distributing the remaining probability equally across all other classes. Such extensions are discussed in Section 5. 4. Experiments 4.1. Soft Augmentation with Crop As a concrete example of the proposed Soft Augmenta- tion, we consider the crop transform t(tx,ty,w,h)(x). In the case of 32×32 pixel Cifar images [23], the cropped images typically have a constant sizew = h = 32, and t(x) is fully parameterized by tx and ty, which are translational offsets1 0 0 1 0 0 0.1 0.1 0.6 0.1 0.1 0.1 0.1 0.6 0.1 0.1 0.6 x 0 0 1 0 0 0.6 x Hard Target Soft Weight Soft Target Soft Target & Weight 1.0 x 1.0 x Weight One-Hot Target  Variants of Soft Augmentation Variant 0.0 0.5 1.0 Proportion Visible (v) 0.0 pmin 1 - α 1.0 Target Confidence (p) Softening Curves chance k=1 k=2 k=3 k=4 LS Figure 2. Variants of Soft Augmentation as prescribed by Equations 4 (Soft Target), 5 (Soft Weight), 6 (Soft Target & Weight) with example target confidence p = 0.6 (left). Soft Augmentation applies non-linear (k = 2, 3, 4, ...) softening to learning targets based on the specific degree of occlusion of a cropped image (Equation 7), which qualitatively captures the degradation of human visual recognition under occlusion [44]. Label Smoothing applies a fixed softening factor α to the one-hot classification target. between the cropped and the original image. As shown in Figure 1 (left), the standard hard crop augmentation for the Cifar-10/100 classification tasks drawstx, tyindependently from a uniform distribution of a modest range U(−4, 4). Under this distribution, the minimal visibility of an image is 77% and ResNet-18 models trained on the Cifar-100 task achieve mean top-1 validation error of 20.80% across three independent runs (Figure 1 left). Naively applying aggres- sive hard crop augmentation drawn from a more aggressive range U(−16, 16) increases top-1 error by 2.19% (Figure 1 middle). We make two changes to the standard crop aug- mentation. We first propose drawing tx, tyindependently from a scaled normal distribution S∗ ∼N(0, σL) (with clipping such that |tx| < L,|ty| < L), where L is the length of the longer edge of the image ( L = 32 for Cifar). The distri- bution has zero mean and σ controls the relative spread of the distribution hence the mean occlusion level. Following the 3σ rule of normal distribution, an intuitive tuning-free choice is to set σ ≈ 0.3, where ∼ 99% of cropped samples have visibility ≥ 0. Figure 3 (left, α = 0) shows that chang- ing the distribution alone without target softening provides a moderate ∼ 0.4% performance boost across crop strength σ. Directly borrowing the findings from human vision re- seach [44], one can define an adaptive softeningα(tx, ty, k) that softens the ground truth learning target. Similar to La- bel Smoothing [43], a hard target can be softened to confi- dence p ∈ [0, 1]. Instead of a fixed α, consider a family of power functions that produces target hardness p given crop parameters tx, tyand curve shape k: p = 1 −α(tx, ty, k) = 1 −(1 −pmin)(1 −v(tx,ty))k, (7) where v(tx,ty) ∈ [0, 1] is the image visibility which is a function of tx and ty. The power function family is a simple one-parameter formulation that allows us to test both linear (k = 1) and non-linear (k ̸= 1) softening: higher k provides flatter plateaus in high visibility regime (see Figure 2 right). 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Top-1 Error Reduction (%) Label Smoothing baseline α=0 α=0.001 α=0.01 α=0.1 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Weight baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target & Weight baseline k=1 k=2 k=3 k=4 Figure 3. Soft Augmentation reduces the top-1 validation error of ResNet-18 on Cifar-100 by up to 2.5% via combining both target and weight softening (Equation 6). Applying target softening alone (Equation 4) can boost performance by ∼ 2%. Crop parameters tx, ty are independently drawn from N(0, σL) (L = 32). Higher error reductions indicate better performance over baseline. All results are the means and standard errors across 3 independent runs.As seen in Label Smoothing, p can be interpreted as ground truth class probability of the one-hot learning target. pmin is the chance probability depending on the task prior. For example, for Cifar-100, pmin = 1 #classes = 0.01. Equation 7 has three assumptions: 1) the information loss is a function of image visibility and all information is lost only when the image is fully occluded, 2) the original label of a training image has a confidence of 100%, which suggests that there is no uncertainty in the class of the label, and 3) the information loss of all images can be approx- imated by a single confidence-visibility curve. While the first assumption is supported by observations of human vi- sual classification research [44], and empirical evidence in Sections 4.2 and 4.3 suggests that the second and the third assumptions approximately hold, the limitations to these as- sumptions will be discussed in Section 5. 4.2. How to Soften Targets As prescribed by Equations 4, 5, and 6, three versions of Soft Augmentation are compared with Label Smoothing across a range of crop strength σ. The popular ResNet-18 models [16] are trained on the 100-class classification Cifar- 100 training set. Top-1 error reductions on the validation set are reported (details in Appendix B). Consistent with prior studies, label smoothing can boost model performance by ∼ 1.3% when the smoothing factor α is properly tuned (Figure 3 left). Combining both target and weight softening (Equation 6) with k = 2 and σ ≈ 0.3 boosts model performance by 2.5% (Figure 3 right). Note that k = 2 qualitatively re- sembles the shape of the curve of human visual confidence degradation under occlusion reported by Tang et al. [44]. Interestingly, the optimal σ ≈ 0.3 fits the intuitive 3-σ rule. In the next section we freeze k = 2 and σ = 0 .3 and show robust improvements that generalize to Cifar-10 [23], ImageNet-1K [9], and ImageNet-V2 tasks [39]. 4.3. Supervised Classification 4.3.1 Comparison with Related Methods As mentioned in Section 2, many approaches similar to soft augmentation have demonstrated empirical perfor- mance gains, including additional data augmentation trans- forms [10], learning augmentation policies [8], softening learning targets [43], and modifying loss functions [29]. However, as training recipes continued to evolve over the past decade, baseline model performance has improved ac- cordingly. As seen in Table 1 (Baseline), our baseline ResNet-18 models with a 500-epoch schedule and cosine learning rate decay [33] not only outperform many recent baseline models of the same architecture, but also beat var- ious published results of Mixup and Cutout. To ensure fair comparisons, we reproduce 6 popular methods: Mixup, Table 1. Soft augmentation outperforms related methods. Optimal hyperparameters for Mixup [53], Cutout [10], and Online Label Smoothing [52] were applied. α of Focal Loss is tuned as [29] did not prescribe an optimal α for Cifar classification. It is worth not- ing that our baseline model (20.80%) not only outperforms other published baseline models by 1.5% to 4.8%, but also beat various implementations of Mixup and Cutout. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Baseline Zhanget al. [53] 25.6 DeVries and Taylor [10]22.46±0.31 Kimet al. [20] 23.59 Ours 20.80±0.11 Mixup Zhanget al. [53] 21.1 Kimet al. [20] 22.43 Ours 19.88±0.38 Cutout DeVries and Taylor [10]21.96±0.24 Ours 20.51±0.02 Label Smoothing Ours 19.47±0.18 Online Label Smoothing 20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 RandAugment 20.99±0.11 Soft Augmentation 18.31±0.17 Cutout, Label Smoothing, Online Label Smoothing, Focal Loss, and RandAugment, and report the Top-1 Error on Cifar-100 in Table 1. Additional comparisons with the self- reported results are available in Appendix Table 5. Table 1 shows that Soft Augmentation outperforms all other single methods. It is worth noting that although focal loss [29] proposed for detection tasks, it can be tuned to slightly improve classification model performance. 4.3.2 Soft Augmentation Compliments RandAugment This section investigates the robustness of Soft Augmen- tation across models and tasks, and how well it compares or complements more sophisticated augmentation policies such as RandAugment [8]. The ImageNet-1K dataset [9] has larger and variable-sized images compared to the Ci- far [23] datasets. In contrast with the fixed-sized crop augmentation for Cifar, a crop-and-resize augmentation t(tx,ty,w,h)(x) with random location tx, tyand random size w, his standard for ImageNet training recipes [7,8,42]. The resizing step is necessary to produce fixed-sized training images (e.g. 224 × 224). We follow the same σ = 0 .3 principle for drawing tx, tyand w, h(details in Appendix B). Comparing single methods, Soft Augmentation with crop only consistently outperforms the more sophisticated RandAugment with 14 transforms (Table 2). The small ResNet-18 models trained with SA on Cifar-10/100 even outperforms much larger baseline ResNet-50 [39] andTable 2. Soft Augmentation (SA) with a fixed softening curve of k = 2 doubles the top-1 error reduction of RandAugment (RA) across datasets and models. Note that the ResNet-18 models trained with SA on Cifar-10/100 even outperform larger baseline ResNet-50 and WideResNet-28 models. All results are mean ± standard error of top-1 validation error in percentage. Best results are shown in bold, runners-up are underlined, and results in parentheses indicate improvement over baseline. Statistics are computed from three runs. Dataset Model Baseline SA RA SA+RA Cifar100 EfficientNet-B0 49.70±1.55 42.13±0.45(−7.57) 46.68±1.52(−3.02) 38.72±0.71(−10.98) ResNet-18 20.80±0.11 18.31±0.17(−2.49) 20.99±0.11(+0.19) 18.10±0.20(−2.70) ResNet-50 20.18±0.30 18.06±0.24(−2.12) 18.57±0.09(−1.61) 16.72±0.06(−3.46) WideResNet-28 18.60±0.19 16.47±0.18(−2.13) 17.65±0.14(−0.95) 15.37±0.17(−3.23) PyramidNet + ShakeDrop15.77±0.17 14.03±0.05(−1.75) 14 .02±0.28(−1.76) 12.78±0.16(−2.99) Cifar10 EfficientNet-B0 17.73±0.69 12.21±0.22(−5.52) 14.54±0.47(−3.19) 11.67±0.26(−6.06) ResNet-18 4.38±0.05 3.51±0.08(−0.87) 3.89±0.06(−0.49) 3.27±0.08(−1.11) ResNet-50 4.34±0.14 3.67±0.08(−0.67) 3.91±0.14(−0.43) 3.01±0.02(−1.33) WideResNet-28 3.67±0.08 2.85±0.02(−0.82) 3.26±0.04(−0.41) 2.45±0.03(−1.20) PyramidNet + ShakeDrop 2.86±0.03 2.26±0.02(−0.60) 2.32±0.08(−0.54) 2.02±0.01(−0.84) ImageNet-1K ResNet-50 22.62±<0.01 21.66±0.02(−0.96) 22.02±0.02(−0.60) 21.27±0.05(−1.35) ResNet-101 20.91±0.04 20.63±0.03(−0.28) 20 .39±0.07(−0.52) 19.86±0.03(−1.05) ImageNet-V2 ResNet-50 34.97±0.03 33.32±0.10(−1.65) 34.16±0.21(−0.81) 32.38±0.16(−2.59) ResNet-101 32.68±0.04 31.81±0.16(−0.87) 32.08±0.19(−0.60) 31.26±0.12(−1.42) WideResNet-28 [50] models. Because RandAugment is a searched policy that is orig- inally prescribed to be applied in addition to the standard crop augmentation [8], one can easily replace the standard crop with soft crop and combine Soft Augmentation and RandAugment. As shown in Table 2, Soft Augmentation complements RandAugment by doubling its top-1 error re- duction across tasks and models. Note that for the small ResNet-18 model trained on Cifar-100, the fixed RandAugment method slightly de- grades its performance. Consistent with observations from Cubuk et al. [8], the optimal hyperparameters for RandAug- ment depend on the combination of model capacity and task complexity. Despite the loss of performance of applying RandAugment alone, adding Soft Augmentation reverses the effect and boosts performance by 2.7%. For the preceding experiments, a fixed k = 2 is used for Soft Augmentation and the official PyTorch RandomAug- ment [38] is implemented to ensure a fair comparison and to evaluate robustness. It is possible to fine-tune the hyperpa- rameters for each model and task to achieve better empirical performance. 4.3.3 Occlusion Robustness As discussed in Section 2, occlusion robustness in both hu- man vision [34,44,54] and computer vision [22,46,47] have been an important property for real world applications of vi- sion models as objects. To assess the effect of soft augmen- tation on occlusion robustness of computer vision models, ResNet-50 models are tested with occluded ImageNet vali- dation images (Figure 4 and Appendix Figure 7).224×224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of oc- clusion levels. As shown in Figure 5, both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness indepen- dently across occlusion levels. Combining RA with SA re- duces Top-1 error by up to 17%. At 80% occlusion level, SA+RA achieves more than 4× accuracy improvement over the baseline (18.98% vs 3.42%). 4.3.4 Confidence Calibration In addition to top-1 errors, reliability is yet another impor- tant aspect of model performance. It measures how close a model’s predicted probability (confidence) tracks the true correctness likelihood (accuracy). Expected Calibration Er- ror (ECE) is a popular metric [14, 25, 35] to measure con- fidence calibration by dividing model predictions into M confidence bins (Bm) and compute a weighted average er- ror between accuracy and confidence: ECE = MX m=1 |Bm| n |acc(Bm) − conf(Bm)|, (8) where n is the number of samples, acc(Bm) denotes the accuracy of bin m, and conf(Bm) denotes mean model confidence of bin m. Consistent with Guo et al. [14], we set M = 10 and compute ECE for Cifar-10 and Cifar-100 tasks. As shown in Table 3, many methods [25,30,35,45] have been proposed to improve confidence calibration, some- times at the cost of drastically increased computational overhead [25], or degraded raw performance [30]. We show in Table 3 (and Appendix Table 7) that it is possible to fur- ther reduce model top-1 error and expected calibration error simultaneously.Occlusion: 0% wreckBaselineSA+RA Class Prediction Probability wreck 1.00 wreck 0.98 Class Prediction Probability liner 0.00 liner 0.00 Class Prediction Probability dock 0.00 dock 0.00 Class Prediction Probability pirate 0.00 submarine 0.00 Class Prediction Probability submarine 0.00 pirate 0.00 Occlusion: 20% Class Prediction Probability wreck 0.95 wreck 0.95 Class Prediction Probability steel_arch_bridge 0.01 dock 0.01 Class Prediction Probability pier 0.01 submarine 0.01 Class Prediction Probability liner 0.01 liner 0.00 Class Prediction Probability crane 0.00 paddlewheel 0.00 Occlusion: 40% Class Prediction Probability crane 0.28 dock 0.20 Class Prediction Probability web_site 0.18 boathouse 0.10 Class Prediction Probability beacon 0.07 paddlewheel 0.08 Class Prediction Probability pier 0.06 pier 0.08 Class Prediction Probability envelope 0.04 wreck 0.07 Occlusion: 60% Class Prediction Probability web_site 0.78 dock 0.18 Class Prediction Probability crane 0.06 liner 0.16 Class Prediction Probability beacon 0.05 submarine 0.05 Class Prediction Probability seashore 0.01 crane 0.04 Class Prediction Probability envelope 0.01 container_ship 0.03 Occlusion: 80% Class Prediction Probability beacon 0.34 liner 0.07 Class Prediction Probability web_site 0.22 dock 0.03 Class Prediction Probability crane 0.07 aircraft_carrier 0.03 Class Prediction Probability seashore 0.06 container_ship 0.03 Class Prediction Probability liner 0.02 schooner 0.02 Figure 4. Examples of occluded ImageNet validation images and model predictions of ResNet-50. 224 × 224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of occlusion levels. 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-1 Accuracy (%) baseline RA SA RA+SA 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-5 Accuracy (%) baseline RA SA RA+SA Figure 5. Soft Augmentation improves occlusion robustness of ResNet-50 on ImageNet. Both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness independently. Combining RA with SA reduces Top-1 error by up to 17%. At 80% occlusion level, compared with baseline accuracy (3.42%), SA+RA achieves more than 4× accuracy (18.98%). Compared to previous single-model methods, our strong baseline WideResNet-28 models achieves lower top-1 er- ror at the cost of higher ECE. Combining Soft Augmen- tation with more recently developed augmentation policies such as TrivialAugment [37] (SA+TA) reduces top-1 error by 4.36% and reduces ECE by more than half on Cifar-100, outperforming the 4× more computationally expensive 5- ensemble model [25]. To the best of our knowledge, this is state of the art ECE performance for WideResNet-28 on Cifar without post-hoc calibration. 4.4. Soft Augmentation Boosts Self-Supervised Learning In contrast with supervised classification tasks where the learning target yi is usually a one-hot vector, many self-supervised methods such as SimSiam [6] and Barlow Twins [51] learn visual feature representations without class labels by encouraging augmentation invariant feature rep- resentations. This section investigates whether Soft Aug- Table 3. Soft Augmentation improves both accuracy and calibra- tion. We report mean and standard error of three WideResNet-28 runs per configuration (bottom two rows). On the more challeng- ing Cifar-100 benchmark, our Baseline already outperforms much of prior work in terms of Top-1 error, but has worse calibration er- ror (ECE). Applying Soft Augment + Trivial Augment (SA+TA) reduces Top-1 error by 4.36% and reduces ECE by more than half, outperforming even compute-heavy models such as the 5- Ensemble [25]. Similar trends hold for Cifar-10. Method Cifar-100 Cifar-10 Top-1 Error ECE Top-1 Error ECE Energy-based [31] 19.74 4.62 4.02 0.85 DUQ [45] – – 5.40 1.55 SNGP [30] 20.00 4.33 3.96 1.80 DDU [35] 19.02 4.10 4.03 0.85 5-Ensemble [25] 17.21 3.32 3.41 0.76 Our Baseline 18.60±0.16 4.86±0.10 3.67±0.07 2.22±0.03 SA+TA 14.24±0.11 1.76±0.15 2.23±0.06 0.61±0.10 mentation generalizes to learning settings where no one-hot style labeling is provided. In a typical setting, two random crops of the same image are fed into a pair of identical twin networks (e.g., ResNet- 18) with shared weights and architecture. The learning tar- get can be the maximization of similarity between the fea- ture representations of the two crops [6], or minimization of redundancy [51]. By default, all randomly cropped pairs have equal weights. We propose and test two alternative hypotheses for weight softening with SimSiam. To accom- modate self-supervised learning, Equation 7 is modified by replacing visibility vtx,ty with intersection over union IoU of two crops of an image: p = 1 −α(ϕ1, ϕ2, k) = 1 −(1 −pmin)(1 −IoUϕ1,ϕ2 )k, SA#1 (9) where ϕ1 = (tx1, ty1, w1, h1) and ϕ2 = (tx2, ty2, w2, h2) are crop parameters for the first and second sample in a pair.Table 4. Soft Augmentation (SA#1) improves self supervised learning with SimSiam (ResNet-18) on Cifar-100 by down- weighting sample pairs with small intersection over union (IoU), outperforming the opposite hypothesis (SA#2) of down-weighting pairs with large IoU. For each configuration, we report means and standard errors of 3 runs with best learning rates (LR) found for Cifar-100. The effect of SA#1 (with a fixed k = 4) generalizes to Cifar-10 without re-tuning. Task LR Baseline LR SA#1 LR SA#2 Cifar100 0.2 37.64±0.06 0.2 36.61±0.05 0.1 37.39±0.06 Cifar10 0.2 9.87±0.03 0.2 9.31±0.01 - - p is used to soften weights only as no one-hot classification vector is available in this learning setting. With this hypoth- esis (SA#1), “hard\" sample pairs with low IoUs are assigned low weights. Alternatively, one can assign lower weights to “easy\" sample pairs with higher IoUs (SA#2), as prescribed by Equation 10: p = 1 − α(ϕ1, ϕ2, k) = 1 − (1 − pmin)(IoUϕ1,ϕ2 )k. SA#2 (10) We first test all three hypotheses (baseline, SA#1, and SA#2) on Cifar-100 with the SimSiam-ResNet-18 models. Table 4 (top) shows that SA#1 outperform both baseline and SA#2 (details in Appendix B.4). Additional experiments show that models trained with the same SA#1 configuration also generalize to Cifar-10 (Table 4 bottom). 5. Discussion Other augmentations. While we focus on crop aug- mentations as an illustrative example, Soft Augmentation can be easily extended to a larger repertoire of transforms such as affine transforms and photometric distortions, as seen in the more sophisticated augmentation policies such as RandAugment. As the formulation of Equation 7 (and Figure 2 right) is directly inspired by the qualitative shape of human vision experiments from Tanget al. [44], optimal softening curves for other transforms may be discovered by similar human experiments. However, results with a sec- ond transform in Appendix Table 6 suggest that Equation 7 generalizes to additive noise augmentation as well. A po- tential challenge is determining the optimal softening strat- egy when a combination of several transforms are applied to an image since the cost of a naive grid search increases ex- ponentially with the number of hyperparameters. Perhaps reinforcement learning methods as seen in RandAugment can be used to speed up the search. Other tasks. While we limit the scope of Soft Augmen- tation to image classification as it is directly inspired by hu- man visual classification research, the idea can be general- ized to other types of tasks such as natural language mod- eling and object detection. Recent studies have shown that detection models benefit from soft learning targets in the fi- nal stages [3,27], Soft Augment has the potential to comple- ment these methods by modeling information loss of image transform in the models’ input stage. Class-dependant augmentations. As pointed out by Balestriero et al. [2], the effects of data augmentation are class-dependent. Thus assumption 3 of Equation 7 does not exactly hold. One can loosen it by adaptively determining the range of transform and softening curve on a per class or per sample basis. As shown in Equation 11, (xi, yi) ⇒ \u0000 tϕ∼S(xi,yi)(xi), gα(ϕ,xi,yi)(yi) \u0001 , (11) two adaptive improvements can be made: 1) the transforma- tion range S where ϕ is drawn from can be made a function of sample (xi, yi), 2) the softening factorα can also adapt to (xi, yi). Intuitively, the formulation recognizes the hetero- geneity of training samples of images at two levels. Firstly, the object of interest can occupy different proportions of an image. For instance, a high-resolution training image with a small object located at the center can allow more ag- gressive crop transforms without losing its class invariance. Secondly, texture and shape may contribute differently de- pending on the visual class. A heavily occluded tiger may be recognized solely by its distinctive stripes; in contrast, a minimally visible cloak can be mistaken as almost any clothing. 6. Conclusion In summary, we draw inspiration from human vision re- search, specifically how human visual classification perfor- mance degrades non-linearly as a function of image occlu- sion. We propose generalizing data augmentation with in- variant transforms to Soft Augmentation where the learning target (e.g. one-hot vector and/or sample weight) softens non-linearly as a function of the degree of the transform ap- plied to the sample. Using cropping transformations as an example, we em- pirically show that Soft Augmentation offers robust top-1 error reduction across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. With a fixed softening curve, Soft Aug- mentation doubles the top-1 accuracy boost of the popular RandAugment method across models and datasets, and im- proves performance under occlusion by up to 4×. Combin- ing Soft Augment with the more recently developed Triv- ialAugment further improves model accuracy and calibra- tion simultaneously, outperforming even compute-heavy 5- ensemble models. Finally, self-supervised learning exper- iments demonstrate that Soft Augmentation also general- izes beyond the popular supervised one-hot classification setting.References [1] Devansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning , pages 233– 242. PMLR, 2017. 3 [2] Randall Balestriero, Leon Bottou, and Yann LeCun. The effects of regularization and data augmentation are class de- pendent. arXiv preprint arXiv:2204.03632, 2022. 8 [3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. Soft-nms–improving object detection with one line of code. In Proceedings of the IEEE international conference on computer vision, pages 5561–5569, 2017. 8 [4] Andy Brock, Soham De, Samuel L Smith, and Karen Si- monyan. High-performance large-scale image recognition without normalization. In International Conference on Ma- chine Learning, pages 1059–1071. PMLR, 2021. 2 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 1 [6] Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 15750–15758, 2021. 7, 14 [7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 113–123, 2019. 2, 5 [8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. 1, 2, 5, 6, 12 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5 [10] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 5 [11] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 12 [12] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self- organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neu- ral nets, pages 267–285. Springer, 1982. 2 [13] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 2 [14] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321–1330. PMLR, 2017. 6 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 2 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. 5, 11 [17] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. 2 [18] David H Hubel and Torsten N Wiesel. Receptive fields of single neurones in the cat’s striate cortex. The Journal of physiology, 148(3):574, 1959. 2 [19] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural informa- tion processing systems, 28, 2015. 1 [20] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with super- modular diversity. arXiv preprint arXiv:2102.03065, 2021. 5, 12 [21] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puz- zle mix: Exploiting saliency and local statistics for optimal mixup. In International Conference on Machine Learning , pages 5275–5285. PMLR, 2020. 12 [22] Adam Kortylewski, Ju He, Qing Liu, and Alan L Yuille. Compositional convolutional neural networks: A deep archi- tecture with innate robustness to partial occlusion. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8940–8949, 2020. 2, 6 [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 3, 5 [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. Advances in neural information processing systems , 25, 2012. 2 [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estima- tion using deep ensembles. Advances in neural information processing systems, 30, 2017. 1, 6, 7 [26] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015. 2 [27] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002–21012, 2020. 8 [28] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019. 2 [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. 5, 12[30] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in Neural Infor- mation Processing Systems, 33:7498–7512, 2020. 6, 7 [31] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems , 33:21464–21475, 2020. 7 [32] Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue. Learning by turning: Neural architecture aware optimi- sation. In International Conference on Machine Learning , pages 6748–6758. PMLR, 2021. 2 [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas- tic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5 [34] David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pages 1150–1157. Ieee, 1999. 2, 6 [35] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, and Yarin Gal. Deep deterministic uncer- tainty: A simple baseline, 2021. 6, 7 [36] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural in- formation processing systems, 32, 2019. 2 [37] Samuel G Müller and Frank Hutter. Trivialaugment: Tuning- free yet state-of-the-art data augmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 774–782, 2021. 1, 2, 7 [38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 32, pages 8024–8035. Curran Asso- ciates, Inc., 2019. 6, 12 [39] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to im- agenet? In International Conference on Machine Learning, pages 5389–5400. PMLR, 2019. 5, 12 [40] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urta- sun. Learning to reweight examples for robust deep learn- ing. In International conference on machine learning, pages 4334–4343. PMLR, 2018. 3 [41] Andrew W Senior, Richard Evans, John Jumper, James Kirk- patrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Im- proved protein structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020. 1 [42] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1–9, 2015. 1, 2, 5 [43] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception archi- tecture for computer vision. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 2818–2826, 2016. 2, 4, 5 [44] Hanlin Tang, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35):8835–8840, 2018. 1, 3, 4, 5, 6, 8 [45] Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep de- terministic neural network. In International conference on machine learning, pages 9690–9700. PMLR, 2020. 6, 7 [46] Angtian Wang, Yihong Sun, Adam Kortylewski, and Alan L Yuille. Robust object detection under occlusion with context- aware compositionalnets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12645–12654, 2020. 2, 6 [47] Dean Wyatte, Tim Curran, and Randall O’Reilly. The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded. Journal of Cognitive Neuroscience, 24(11):2248–2261, 2012. 1, 2, 6 [48] Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Zhi-Ming Ma. Reweighting augmented samples by minimizing the maximal expected loss. arXiv preprint arXiv:2103.08933, 2021. 3 [49] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu- larization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international con- ference on computer vision, pages 6023–6032, 2019. 2 [50] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. arXiv preprint arXiv:1605.07146, 2016. 6, 11 [51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Ma- chine Learning, pages 12310–12320. PMLR, 2021. 7 [52] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, and Ming-Ming Cheng. Delving deep into label smoothing. IEEE Transactions on Image Process- ing, 30:5984–5996, 2021. 5, 12 [53] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 2, 5, 12 [54] Hongru Zhu, Peng Tang, Jeongho Park, Soojin Park, and Alan Yuille. Robustness of object recognition under ex- treme occlusion in humans and computational models.arXiv preprint arXiv:1905.04598, 2019. 1, 2, 6Appendix A. Implementation 1import torch 2 3class SoftCropAugmentation: 4def __init__(self, n_class, sigma=0.3, k=2): 5self.chance = 1/n_class 6self.sigma = sigma 7self.k = k 8 9def draw_offset(self, limit, sigma=0.3, n =100): 10# draw an integer from a (clipped) Gaussian 11for d in range(n): 12x = torch.randn((1))*sigma 13if abs(x) <= limit: 14return int(x) 15return int(0) 16 17def __call__(self, image, label): 18# typically, dim1 = dim2 = 32 for Cifar 19dim1, dim2 = image.size(1), image.size(2) 20# pad image 21image_padded = torch.zeros((3, dim1 * 3, dim2 * 3)) 22image_padded[:, dim1:2*dim1, dim2:2*dim2] = image 23# draw tx, ty 24tx = self.draw_offset(dim1, self. sigma_crop * dim1) 25ty = self.draw_offset(dim2, self. sigma_crop * dim2) 26# crop image 27left, right = tx + dim1, tx + dim1 * 2 28top, bottom = ty + dim2, ty + dim2 * 2 29new_image = image_padded[:, left: right, top: bottom] 30# compute transformed image visibility and confidence 31v = (dim1 - abs(tx)) * (dim2 - abs(ty)) / (dim1 * dim2) 32confidence = 1 - (1 - self.chance) * (1 - v) ** self.k 33return new_image, label, confidence Listing 1. Pytorch implementation of Soft Crop Augmentation for Cifar. 1import torch 2import torch.nn.functional as F 3 4def soft_target(pred, label, confidence): 5log_prob = F.log_softmax(pred, dim=1) 6n_class = pred.size(1) 7# make soft one-hot target 8one_hot = torch.ones_like(pred) * (1 - confidence) / (n_class - 1) 9one_hot.scatter_(dim=1, index=label, src= confidence) 10# compute weighted KL loss 11kl = confidence * F.kl_div(input=log_prob, 12target=one_hot, 13reduction=’none’). sum(-1) 14return kl.mean() Listing 2. Pytorch implementation of Soft Target loss function. Appendix B. Experiment Details Appendix B.1. Supervised Cifar-10/100 For Cifar-100 experiments, we train all ResNet-like models with a batch size 128 on a single Nvidia V100 16GB GPU on Amazon Web Services (AWS) and with an intial learning rate 0.1 with cosine learning rate decay over 500 epochs. EfficientNet-B0 is trained with an initial learning rate of 0.025, PyramidNet-272 is trained with 2 GPUs. We use the Conv-BatchNorm-ReLU configuration of ResNet models [16] and WideResNet-28 with a widening factor of 10 [50]. Horizontal flip is used in all experiments as it is considered a lossless transformation in the context of Ci- far images. We find decaying crop aggressiveness towards the end of training (e.g., last 20 epochs) by a large fac- tor (e.g., reducing σ by 1000×) marginally improve per- formance on Cifar-100, but slightly hurts performance on Cifar-10. Accordingly, we only apply σ decay for all Cifar- 100 experiments. A single run of ResNet-18, ResNet-50, and WideResNet-28 takes ∼ 2.5, ∼ 7, ∼ 9 GPU hours on Cifar-10/100, respectively. BL beaver 0.30 SA+TA seal 0.44 BL pear 0.43 SA+TA apple 0.79 BL television 0.12 SA+TA dinosaur 0.37 BL skyscraper 0.18 SA+TA castle 0.44 BL kangaroo 0.35 SA+TA rabbit 0.79 BL poppy 0.83 SA+TA beetle 0.27 BL poppy 0.74 SA+TA sunflower 0.89 BL clock 0.48 SA+TA ray 0.74 BL girl 0.51 SA+TA boy 0.83 BL crab 0.52 SA+TA crocodile 0.95 BL rabbit 0.56 SA+TA mouse 0.68 BL butterfly 0.28 SA+TA skunk 0.88 BL beetle 0.12 SA+TA flatfish 0.16 BL mouse 0.17 SA+TA lizard 0.98 BL forest 0.37 SA+TA pine_tree 0.56 BL skunk 0.68 SA+TA elephant 0.78 BL hamster 0.35 SA+TA raccoon 0.17 BL bee 0.84 SA+TA caterpillar 0.95 BL cattle 0.10 SA+TA kangaroo 0.31 BL cattle 0.24 SA+TA lion 0.89 Figure 6. Example images of the Cifar-100 validation set and pre- dictions of WideResNet-28. Predicted classes and confidence lev- els of models trained with Soft Augmentation + Trivial Augment (SA+TA) and baseline (BL) augmentation are reported. In many cases, SA+TA not only corrects the class prediction, but also im- proves the model confidence. For instance, BL mistakes “seal” for “beaver” (top-left, both classes belong to the same “aquatic mammal” superclass), and SA+TA makes a correct class predic- tion with higher confidence.Appendix B.2. Additional Results Table 5. Comparing SA with other methods. Recommended hyperparameters for Mixup [53], Cutout [11], and Online Label Smoothing [52]. α of Focal Loss is tuned as Lin et al. [29] did not prescribe an optimal α for Cifar classification. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Zhanget al. [53] Baseline 25.6 Mixup 21.1 Kimet al. [21] Baseline 23.67 Mixup 23.16 Manifold Mixup 20.98 Puzzle Mix 19.62 Kimet al. [20] Baseline 23.59 Mixup 22.43 Manifold Mixup 21.64 Puzzle Mix 20.62 Co-Mixup 19.87 Our Baseline 20.80±0.11 Label Smoothing 19.47±0.18 Online Label Smoothing20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 Mixup (α= 1.0) 19.88±0.38 Cutout (L= 8) 20.51±0.02 SA 18.31±0.17 RA 20.99±0.11 SA + RA 18.10±0.20 Table 6. Soft Augmentation with additive noise improves ResNet- 18 performance on Cifar-100. Given an image X and a random noise pattern Xnoise, and augmented image is given by Xaug = X + αXnoise, where α is drawn from N(0, 0.1) and pixel values of Xnoise are also independently drawn from N(0, 0.1). Apply- ing Soft Augmentation to additive noise boost performance over baseline as well as Soft Augmentation Crop + RandAugment. ResNet-18 Top-1 Error Baseline 20.80±0.11 RA 20.99±0.11 Hard Crop 20.26±0.12 SA-Crop (k=2) 18.31±0.17 Hard Noise 20.68±0.05 SA-Noise (k=1) 19.20±0.20 SA-Crop (k=2) + RA 18.10±0.20 SA-Noise (k=1) + SA-Crop (k=2) + RA17.87±0.17 Table 7. Soft Augmentation reduces expected calibration error (ECE) of ResNet-50 on ImageNet. Dataset Baseline RA SA RA+SA ImageNet-1K 5.11 4.09 3.17 2.78 ImageNet-V2 9.91 8.84 3.24 3.18 Appendix B.3. ImageNet All ImageNet-1k experiments are conducted with a batch size of 256 distributed across 4 Nvidia V100 16GB GPUs on AWS. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset (BSD 3- Clause License) is downloaded from the official website (https://www.image-net.org/). Horizontal flip is used in all experiments as an additional lossless base augmentation. The base learning rate is set to 0.1 with a 5-epoch linear warmup and cosine decay over 270 epochs. A single run of ResNet-50 training takes ∼ 4 × 4 = 16 GPU days and ImageNet experiments take a total of 600 GPU days. We use the official PyTorch [38] implementation of Ran- dAugment and ResNet-50/101 (BSD-style license) and run all experiments with the standard square input Linput = W = H = 224. Note that the original RandAugment [8] uses a larger input size of H = 224 , W = 244 , but our re-implemention improved top-1 error (22.02 vs 22.4) of ResNet-50 despite using a smaller input size. ImageNet-V2 is a validation set proposed by He et al. [39]. For training, the standard crop transform has 4 hy- perparameters: (scalemin, scalemax) define the range of the relative size of a cropped image to the original one, (ratiomin, ratiomax) determine lower and upper bound of the aspect ratio of the cropped patch before the final resize step. In practice, a scale is drawn from a uni- form distribution U(scalemin, scalemax), then the loga- rithm of the aspect ratio is drawn from a uniform dis- tribution U(log(ratiomin), log(ratiomax)). Default val- ues are scalemin = 0 .08, scalemax = 1 .0, ratiomin = 3/4, ratiomax = 4/3. Similar to our Cifar crop augmentation, we propose a simplified ImageNet crop augmentation with only 2 hy- perparameters σ, Lmin. First, we draw ∆w, ∆h from a clipped rectified normal distribution NR(0, σ(L − Lmin)) and get w = W − ∆w, h= H − ∆h Lmin is the mini- mum resolution of a cropped image and set to half of input resolution 224. tx, tyare then independently drawn from N(0, σ(W +w)), N(0, σ(H +h)). Note that we use a fixed set of intuitive values σ = 0.3, Lmin = 1/2Linput = 112 for all the experiments. For model validation, standard augmentation practice first resizes an image so that its short edge has length Linput = 256 , then a center 224 × 224 crop is applied. Note that Linput is an additional hyperparameter introduced by the test augmentation. In contrast, we simplify this by setting Linput to the final input size 224 and use this con- figuration for all ImageNet model evaluation.Occlusion: 0% Boston_bullBaselineSA+RA Class Prediction Probability Boston_bull 0.82 Boston_bull 0.65 Class Prediction Probability French_bulldog 0.09 French_bulldog 0.23 Class Prediction Probability toy_terrier 0.03 toy_terrier 0.01 Class Prediction Probability tennis_ball 0.01 Chihuahua 0.00 Class Prediction Probability Chihuahua 0.00 pug 0.00 Occlusion: 20% Class Prediction Probability Boston_bull 0.95 Boston_bull 0.86 Class Prediction Probability French_bulldog 0.05 French_bulldog 0.13 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability tennis_ball 0.00 pug 0.00 Occlusion: 40% Class Prediction Probability French_bulldog 0.83 Boston_bull 0.96 Class Prediction Probability Boston_bull 0.17 French_bulldog 0.04 Class Prediction Probability Staffordshire_bullterrier 0.00 toy_terrier 0.00 Class Prediction Probability American_Staffordshire_terrier0.00 Italian_greyhound 0.00 Class Prediction Probability Great_Dane 0.00 Chihuahua 0.00 Occlusion: 60% Class Prediction Probability Boston_bull 0.97 Boston_bull 0.80 Class Prediction Probability French_bulldog 0.01 French_bulldog 0.02 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.02 Class Prediction Probability whippet 0.00 toy_terrier 0.02 Class Prediction Probability toy_terrier 0.00 Chihuahua 0.01 Occlusion: 80% Class Prediction Probability French_bulldog 0.88 Boston_bull 0.13 Class Prediction Probability Boston_bull 0.06 French_bulldog 0.06 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.03 Class Prediction Probability pug 0.01 miniature_pinscher 0.02 Class Prediction Probability Staffordshire_bullterrier 0.01 Staffordshire_bullterrier 0.01 Occlusion: 0% papillonBaselineSA+RA Class Prediction Probability papillon 1.00 papillon 0.97 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability Japanese_spaniel 0.00 Japanese_spaniel 0.00 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Pomeranian 0.00 Pomeranian 0.00 Occlusion: 20% Class Prediction Probability papillon 0.74 papillon 0.76 Class Prediction Probability Blenheim_spaniel 0.09 Blenheim_spaniel 0.03 Class Prediction Probability clumber 0.06 Japanese_spaniel 0.03 Class Prediction Probability Japanese_spaniel 0.05 Chihuahua 0.02 Class Prediction Probability Welsh_springer_spaniel 0.02 Pomeranian 0.01 Occlusion: 40% Class Prediction Probability papillon 0.29 Blenheim_spaniel 0.15 Class Prediction Probability Blenheim_spaniel 0.19 papillon 0.13 Class Prediction Probability Welsh_springer_spaniel 0.14 Welsh_springer_spaniel 0.13 Class Prediction Probability collie 0.11 Brittany_spaniel 0.05 Class Prediction Probability Brittany_spaniel 0.05 hare 0.02 Occlusion: 60% Class Prediction Probability envelope 0.09 bustard 0.04 Class Prediction Probability Indian_cobra 0.03 hare 0.02 Class Prediction Probability hognose_snake 0.02 golf_ball 0.02 Class Prediction Probability web_site 0.02 partridge 0.02 Class Prediction Probability dhole 0.01 kit_fox 0.02 Occlusion: 80% Class Prediction Probability envelope 0.15 kit_fox 0.01 Class Prediction Probability web_site 0.08 hare 0.01 Class Prediction Probability dhole 0.04 bustard 0.01 Class Prediction Probability red_fox 0.01 partridge 0.01 Class Prediction Probability honeycomb 0.01 bittern 0.01 Occlusion: 0% vending_machineBaselineSA+RA Class Prediction Probability vending_machine 0.65 vending_machine 0.98 Class Prediction Probability streetcar 0.04 streetcar 0.00 Class Prediction Probability refrigerator 0.02 refrigerator 0.00 Class Prediction Probability minibus 0.01 pop_bottle 0.00 Class Prediction Probability grocery_store 0.01 ambulance 0.00 Occlusion: 20% Class Prediction Probability vending_machine 0.39 vending_machine 0.83 Class Prediction Probability moving_van 0.12 streetcar 0.04 Class Prediction Probability refrigerator 0.12 moving_van 0.01 Class Prediction Probability web_site 0.05 refrigerator 0.01 Class Prediction Probability monitor 0.04 trolleybus 0.01 Occlusion: 40% Class Prediction Probability vending_machine 0.88 vending_machine 0.93 Class Prediction Probability refrigerator 0.03 refrigerator 0.01 Class Prediction Probability web_site 0.01 pay-phone 0.00 Class Prediction Probability desktop_computer 0.00 slot 0.00 Class Prediction Probability monitor 0.00 streetcar 0.00 Occlusion: 60% Class Prediction Probability screen 0.26 streetcar 0.03 Class Prediction Probability monitor 0.18 garbage_truck 0.02 Class Prediction Probability home_theater 0.14 parking_meter 0.01 Class Prediction Probability television 0.04 cab 0.01 Class Prediction Probability desktop_computer 0.04 trolleybus 0.01 Occlusion: 80% Class Prediction Probability home_theater 0.59 sliding_door 0.04 Class Prediction Probability monitor 0.13 refrigerator 0.02 Class Prediction Probability entertainment_center 0.05 vending_machine 0.02 Class Prediction Probability television 0.05 pay-phone 0.01 Class Prediction Probability screen 0.04 barbershop 0.01 Occlusion: 0% pirateBaselineSA+RA Class Prediction Probability pirate 1.00 pirate 0.93 Class Prediction Probability schooner 0.00 fireboat 0.01 Class Prediction Probability dock 0.00 dock 0.01 Class Prediction Probability fireboat 0.00 schooner 0.01 Class Prediction Probability crane 0.00 amphibian 0.00 Occlusion: 20% Class Prediction Probability pirate 0.98 pirate 0.76 Class Prediction Probability schooner 0.02 schooner 0.03 Class Prediction Probability dock 0.00 cannon 0.03 Class Prediction Probability stupa 0.00 dock 0.01 Class Prediction Probability crane 0.00 fireboat 0.01 Occlusion: 40% Class Prediction Probability crane 0.28 pirate 0.72 Class Prediction Probability pirate 0.22 schooner 0.04 Class Prediction Probability moving_van 0.12 fireboat 0.03 Class Prediction Probability schooner 0.05 dock 0.01 Class Prediction Probability scoreboard 0.03 drilling_platform 0.01 Occlusion: 60% Class Prediction Probability crane 0.43 pirate 0.26 Class Prediction Probability bookshop 0.08 schooner 0.03 Class Prediction Probability scoreboard 0.07 toyshop 0.03 Class Prediction Probability schooner 0.05 suspension_bridge 0.02 Class Prediction Probability drilling_platform 0.03 bookshop 0.02 Occlusion: 80% Class Prediction Probability pole 0.10 toyshop 0.03 Class Prediction Probability book_jacket 0.09 carousel 0.02 Class Prediction Probability comic_book 0.09 shoe_shop 0.02 Class Prediction Probability envelope 0.07 bookshop 0.01 Class Prediction Probability binder 0.06 totem_pole 0.01 Occlusion: 0% military_uniformBaselineSA+RA Class Prediction Probability military_uniform 0.74 military_uniform 0.70 Class Prediction Probability mortarboard 0.07 suit 0.06 Class Prediction Probability suit 0.05 bow_tie 0.02 Class Prediction Probability academic_gown 0.03 crutch 0.02 Class Prediction Probability cornet 0.03 groom 0.01 Occlusion: 20% Class Prediction Probability suit 0.19 crutch 0.23 Class Prediction Probability mortarboard 0.09 suit 0.14 Class Prediction Probability military_uniform 0.06 groom 0.09 Class Prediction Probability notebook 0.04 military_uniform 0.04 Class Prediction Probability lab_coat 0.04 turnstile 0.02 Occlusion: 40% Class Prediction Probability military_uniform 0.22 military_uniform 0.22 Class Prediction Probability lab_coat 0.07 projectile 0.04 Class Prediction Probability file 0.07 warplane 0.02 Class Prediction Probability bearskin 0.05 missile 0.02 Class Prediction Probability suit 0.05 grand_piano 0.01 Occlusion: 60% Class Prediction Probability military_uniform 0.11 military_uniform 0.14 Class Prediction Probability suit 0.10 lab_coat 0.02 Class Prediction Probability envelope 0.06 cowboy_hat 0.02 Class Prediction Probability kimono 0.06 rifle 0.02 Class Prediction Probability abaya 0.05 trombone 0.02 Occlusion: 80% Class Prediction Probability abaya 0.15 crutch 0.04 Class Prediction Probability space_heater 0.13 mortarboard 0.01 Class Prediction Probability web_site 0.13 academic_gown 0.01 Class Prediction Probability window_shade 0.12 trombone 0.01 Class Prediction Probability shower_curtain 0.04 shoe_shop 0.01 Figure 7. Examples of occluded ImageNet validation images and model predictions of ResNet-50.Appendix B.4. Self-Supervised Cifar-10/100 Self-supervised SimSiam experiments are run on a sin- gle Nvidia A6000 GPU. We follow the standard two-step training recipe [6]. 1) We first train the Siamese network in a self-supervised manner to learn visual features for 500 epochs with a cosine decay schedule and a batch size of 512. We apply Soft Augmentation only during this step. 2) The linear layer is then tuned with ground-truth labels for 100 epochs with an initial learning rate of 10 and 10× de- cay at epochs 60 and 80. Following [6], we set scalemin = 0.2, scalemax = 1 .0, ratiomin = 3 /4, ratiomax = 4 /3. Since down-weighting training samples in a batch effec- tively reduces learning rate and SimSiam is sensitive to it, we normalized the weight in a batch so that the mean re- mains 1 and re-tuned the learning rate (Table 8). Table 8. Soft Augmentation improve self supervised learning with SimSiam. Mean ± standard error of top-1 validation errors of three runs of ResNet-18 are reported. Task lr baseline SA#1 ∆#1 SA#2 ∆#2 Cifar100 0.1 39.50±0.13 40.21±0.03 +0.71 37 .39±0.06 −2.11 0.2 37.64±0.06 36.61±0.05 −1.03 39 .20±0.42 +1.56 0.4 40.28±2.49 37.68±0.06 −2.60 Diverged - 0.5 43.26±3.03 41.94±0.04 −1.32 Diverged - 0.8 78.88±9.05 55.44±4.15 −23.44 Diverged - Cifar10 0.2 9.87±0.03 9.31±0.01 −0.56 - - Table 9. SimSiam k tuning on Cifar-100 (single run) learning rate k Top-1 Error 0.2 1 37.78 2 37.27 3 36.34 4 36.31 Appendix C. Effects of Target Smoothing and Loss Reweighting on Loss Func- tions Consider the KL divergence loss of a single learning sample with a one-hot ground truth vector ytrue, and the softmax prediction vector of a model is denoted by ypred: L(ypred, ytrue) = w ∗ DKL(ytrue||ypred) =w ∗ NX n=1 ytrue n ∗ log(ytrue n ypred n ), (12) let n∗ be the ground truth class of an N-class classifica- tion task, Equation 12 can be re-written as: L(ypred, ytrue) = −w ∗ ytrue n∗ ∗ log(ypred n∗ ) + w ∗  ytrue n∗ ∗ log(ytrue n∗ ) + X n̸=n∗ ytrue n ∗ log(ytrue n ypred n )  . (13) In the case of hard one-hot ground truth target where ytrue n∗ = 1 and ytrue n = 0, n̸= n∗, with the default weight w = 1 it degenerates to cross entropy loss: L(ypred, ytrue) = −log(ypred n∗ ), (14) Now we apply label smoothing style softening to the one-hot target ytrue so that ytrue n∗ = p and ytrue n = (1 − p)/(N − 1) = q, n̸= n∗: L(ypred, ytrue) = −p ∗ log(ypred n∗ ) +  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (15) If q is not distributed, and ytrue n = 0, n̸= n∗ (This con- figuration does not correspond to any of our experiments): L(ypred, ytrue) = −p ∗ log(ypred n∗ ) + p ∗ log(p), (16) When only weight w is softened to w = p: L(ypred, ytrue) = −p ∗ log(ypred n∗ ). (17) Note that p is not a function of model weights, so when we take the derivative w.r.t. model weights to compute gra- dient, Equations 16 and 17 yield the same gradient. When both the one-hot label and weight are softened with p: L(ypred, ytrue) = −p2 ∗ log(ypred n∗ ) + p ∗  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (18) The three types of softening in Section 4 are unique as suggested by Equations 15, 17, and 18.",
      "references": [
        "A closer look at memorization in deep networks.",
        "The effects of regularization and data augmentation are class dependent.",
        "Soft-nms–improving object detection with one line of code.",
        "High-performance large-scale image recognition without normalization.",
        "Language models are few-shot learners.",
        "Exploring simple siamese representation learning.",
        "Autoaugment: Learning augmentation strategies from data.",
        "Randaugment: Practical automated data augmentation with a reduced search space.",
        "Imagenet: A large-scale hierarchical image database.",
        "Improved regularization of convolutional neural networks with cutout.",
        "Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition.",
        "Accurate, large mini-batch sgd: Training imagenet in 1 hour.",
        "On calibration of modern neural networks.",
        "Deep residual learning for image recognition.",
        "Identity mappings in deep residual networks.",
        "Distilling the knowledge in a neural network.",
        "Receptive fields of single neurones in the cat’s striate cortex.",
        "Spatial transformer networks.",
        "Co-mixup: Saliency guided joint mixup with super modular diversity.",
        "Puzzle mix: Exploiting saliency and local statistics for optimal mixup.",
        "Compositional convolutional neural networks: A deep architecture with innate robustness to partial occlusion.",
        "Learning multiple layers of features from tiny images.",
        "Imagenet classification with deep convolutional neural networks.",
        "Simple and scalable predictive uncertainty estimation using deep ensembles.",
        "Deep learning.",
        "Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection.",
        "Fast autoaugment.",
        "Focal loss for dense object detection.",
        "Simple and principled uncertainty estimation with deterministic deep learning via distance awareness.",
        "Energy-based out-of-distribution detection.",
        "Learning by turning: Neural architecture aware optimisation.",
        "Sgdr: Stochastic gradient descent with warm restarts.",
        "Object recognition from local scale-invariant features.",
        "Deep deterministic uncertainty: A simple baseline",
        "When does label smoothing help?",
        "Trivialaugment: Tuning-free yet state-of-the-art data augmentation.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Do imagenet classifiers generalize to imagenet?",
        "Learning to reweight examples for robust deep learning.",
        "Improved protein structure prediction using potentials from deep learning.",
        "Going deeper with convolutions.",
        "Rethinking the inception architecture for computer vision.",
        "Recurrent computations for visual pattern completion.",
        "Uncertainty estimation using a single deep deterministic neural network.",
        "Robust object detection under occlusion with context-aware compositional nets.",
        "The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded.",
        "Reweighting augmented samples by minimizing the maximal expected loss.",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features.",
        "Wide residual networks.",
        "Barlow twins: Self-supervised learning via redundancy reduction.",
        "Delving deep into label smoothing.",
        "mixup: Beyond empirical risk minimization.",
        "Robustness of object recognition under extreme occlusion in humans and computational models."
      ],
      "meta_data": {
        "arxiv_id": "2211.04625v2",
        "authors": [
          "Yang Liu",
          "Shen Yan",
          "Laura Leal-Taixé",
          "James Hays",
          "Deva Ramanan"
        ],
        "published_date": "2022-11-09T01:04:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Soft Augmentation, a novel data augmentation method that adapts the learning targets based on the degree of aggressive transformations (e.g., cropping, occlusion). It models the non-linear degradation of human visual confidence and shows that softening targets (and weights) can allow for more aggressive augmentations, resulting in significant improvements in classification accuracy, occlusion robustness, and calibration across various datasets.",
        "methodology": "The proposed approach modifies traditional data augmentation by replacing hard one-hot targets with soft targets that decrease in confidence as the information loss from transformations increases. The technique uses a power function parameterized by a shape factor (k) to relate image visibility (or other metrics) to target confidence. The method is implemented in standard CNN architectures (e.g., ResNet, WideResNet) with experiments conducted using aggressive crop augmentations (and additive noise) and is also extended to self-supervised learning frameworks.",
        "experimental_setup": "Experiments were conducted on multiple benchmarks including CIFAR-10, CIFAR-100, ImageNet-1K, and ImageNet-V2. The setups involve training with aggressive data augmentation using both standard augmentation techniques (e.g., RandAugment, TrivialAugment) and the proposed Soft Augmentation. The evaluation metrics include top-1 and top-5 error rates, occlusion robustness assessed by applying varying levels of occlusion, and reliability metrics such as Expected Calibration Error (ECE). Results are reported using various architectures such as ResNet-18, ResNet-50, WideResNet-28, and others for both supervised and self-supervised learning tasks.",
        "limitations": "The method relies on several assumptions, such as the relationship between crop parameters and information loss reflecting human visual degradation. It assumes a fixed softening function (with parameters like sigma and k) that may not optimally generalize across all transformations or datasets. There is also potential complexity in optimizing multiple hyperparameters when extending the technique to a combination of several transforms, and class-dependent augmentation effects are not fully addressed in the initial formulation.",
        "future_research_directions": "Future work could explore extending Soft Augmentation to other transformations (e.g., affine, photometric distortions) and tasks such as object detection or natural language processing. Additional research could look into reinforcement learning-based hyperparameter search for optimal softening functions, adaptive per-class or per-sample augmentation strategies, and further integration with loss reweighting schemes to improve robustness and calibration.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Frozen Feature Augmentation for Few-Shot Image Classification",
      "full_text": "Frozen Feature Augmentation for Few-Shot Image Classification Andreas B¨ar1 2 * Neil Houlsby1 Mostafa Dehghani1 Manoj Kumar1 † 1Google DeepMind 2Technische Universit¨at Braunschweig andreas.baer@tu-braunschweig.de {neilhoulsby, dehghani, mechcoder}@google Abstract Vision foundation models are currently one of the main driving forces in computer vision research. Simply training a linear classifier or a lightweight model on top of model outputs or so-called ‘frozen features’ leads to impressive performance on a number of tasks. Currently, frozen fea- tures are not modified during training of such lightweight models. On the other hand, when networks are trained di- rectly on images, data augmentation is a standard recipe that improves performance with no additional overhead. In this paper, we conduct an extensive pilot study that ex- plores applying data augmentations in the frozen feature space for few-shot image classification. We dub this type of augmentation ‘frozen feature augmentation (FroFA)’. Our study demonstrates that adopting deceptively simple point- wise FroFAs, such as brightness, can improve few-shot per- formance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets. 1. Introduction A prevalent trend now is to pretrain vision models on large datasets and adapt them downstream [5, 41, 56]. Notable, even training a simple linear layer or a light-weight model on top of vision transformer (ViT) outputs, also known as frozen features, can yield remarkable performance across a number of diverse downstream tasks [13, 19, 43]. However, there is still an interest in training ViTs to achieve good performance on ImageNet-sized [36, 52] or smaller [31, 34] datasets. In this setting, a crucial ingre- dient is data augmentation — a predefined set of simple, stochastic input transformations. Simple but effective ex- amples for image augmentations include random cropping which extracts a fixed-sized region from an image of ar- bitrary resolution, or pixel-wise modifications that change brightness, saturation, or contrast. These are complemented by more advanced augmentation strategies such as mixup [58] or RandAugment [10]. *Work conducted as Research Intern at Google DeepMind. †Project lead. 1 5 10 25 shots 0.0 2.5 5.0 top-1 acc. (abs. gains) JFT-3B 1 5 10 25 shots WebLI + SigLIP MAPwd linear probe Figure 1. Few-shot results averaged across eight test sets, in- cluding ILSVRC-2012 [14, 44]. We use cached features from an L/16 model [16] pretrained on JFT-3B [56] (left) or WebLI [5] following a sigmoid language-image pretraining (SigLIP) [57] (right). Our method, i.e., a multi-head attention pooling [30] head trained with weight decay (MAPwd) and frozen feature augmenta- tion (FroFA), shows significant gains across all shots with respect to a weight-decayed MAP, i.e., MAPwd, or an L2-regularized lin- ear probe baseline, both without FroFA. In this paper, we revisit standard image augmentation techniques in a data-constrained, few-shot frozen feature setting. In particular, we first stochastically transform frozen features and then train a lightweight model on top. Our only modification before applying image augmenta- tions on top of frozen features is a point-wise scaling such that each feature value lies in [0, 1] or [0, 255]. We investigate eighteen augmentations applied to frozen features extracted from vision transformers pretrained on JFT-3B [56], ImageNet-21k [14, 44], or WebLI [5]. We train a small lightweight multi-head attention pooling (MAP) [30, 56] head using these augmented inputs and evaluate its performance across eight downstream image classification datasets, where we on average achieve signif- icant gains (see Fig. 1). Our major insights are as follows: 1. Geometric augmentations that modify the shape and structure of two-dimensional frozen features always lead to worse performance on ImageNet. On the other hand, simple stylistic (point-wise) augmentations, such as brightness, contrast, and posterize, give steady im- 1 arXiv:2403.10519v2  [cs.CV]  26 Jul 2024provements on 1-, 5-, and 10-shot settings. 2. Unlike traditional image augmentations that apply a sin- gle randomly sampled value across the entire image, we introduce per-channel stochasticity by sampling inde- pendent random values for each channel. For example, on the 5-shot setting, we improve accuracy over a well- tuned MAP and linear probe baseline by 0.5% absolute and 0.8% absolute, respectively. 3. While FroFA provides modest but significant improve- ments on ImageNet, it excels on smaller transfer datasets. Across seven downstream datasets, FroFA out- performs the mean accuracy of the MAP baseline in the 5 shot setting by 3.2% absolute and the linear probe base- line by 4.2% absolute. 2. Related Works Transfer learning on few-shot data : State-of-the-art vi- sion models [5, 13, 16, 56] are typically pretrained on large-scale datasets, e.g., ImageNet-21k [14, 44] or ver- sions of JFT [21, 56], before transferred to other middle- scale to small-scale ones,e.g., CIFAR10 [1], ILSVRC-2012 [14, 44], or SUN397 [53, 54]. Depending on the model size, efficient transfer learning becomes a challenge. Many meth- ods have been proposed for large language models (LLMs), e.g., adapters [22], low-rank adaptation (LoRA) [23], or prompt tuning [32], of which some have been successfully adapted to computer vision [4, 17, 24, 59]. CLIP-Adapter [17] builds on the power of contrastive language-image pre- training (CLIP) [43] and combines it with adapters [22]. A follow-up work [59] proposes TiP-Adapter which uses a query-key cache model [18, 42] instead of a gradient de- scent approach. Inspired by the success of prompt tuning in LLMs [32], Jia et al. propose visual prompt tuning at the model input [24]. On the other hand, AdaptFormer [4] uses additional intermediate trainable layers to finetune a frozen vision transformer [16]. In contrast, we do not introduce additional prompts [24] or intermediate parameters [4, 17] that require backprop- agating through the network. Instead, we train a small network on top of frozen features coming from a vision transformer. This aligns with linear probing [43] which is typically used to transfer vision models to other tasks [13, 19, 56] — our objective. In addition, we focus our experiments around transfer learning on few-shot data [29, 51]. Although not surprising, few-shot results obtained by Dehghani et al . [13] clearly show significant gaps between linear probing and full fine- tuning. We take these results as an incentive to improve upon linear probing. Data augmentation: One go-to method to improve per- formance while training in a low-data regime is data aug- mentation [46]. Some prominent candidates in computer vision are AutoAugment [9], AugMix [20], RandAugment [9], and TrivialAugment [39]. These methods typically combine low-level image augmentations together to aug- ment the input. Although some works propose augmen- tations in feature space [15, 28, 33, 37, 50], a large-scale empirical study on frozen features of single-modal vision models does not exist. To this end, we investigate frozen feature augmentation (FroFA) by reformulating eighteen image augmentations. In particular, we consider a subset used in AutoAugment [9], inception crop [48], mixup [50, 58], and the recently introduced patch dropout [35]. 3. Framework Overview In this section, we give an overview of our framework. 3.1. Notation Let x ∈ IH×W×3 be an RGB image of height H, width W, and I = [0, 1]. A classification model processes x and outputs class scores y ∈ [0, 1]S for each class in a pre- defined set of classes S, with S = |S|. Let L and D be the number of intermediate layers and the number of fea- tures of a multi-layer classification model, respectively. We describe the intermediate feature representations of x as f = f(ℓ) = (f(ℓ) d ) ∈ RD, with layer index ℓ ∈ {1, ..., L} and feature index d ∈ {1, ..., D}. In the vision trans- former [26] architecture, f = f(ℓ) = (f(ℓ) n,c) ∈ RN×C is a two-dimensional entity, where N and C are the number of patches and number of per-patch channels, respectively. In addition, we introduce the patch index n ∈ {1, ..., N} and the per-patch channel index c ∈ {1, ..., C}. 3.2. Training on Cached Features We investigate pretrained vision transformers [26] with L transformer blocks (TBs) followed by a multi-head atten- tion pooling (MAP) [30] and a classification layer (CL). Fig. 2a presents a simplified illustration. For simplicity, we neglect all operations before the first transformer block (e.g., patchifying, positional embedding, etc.). To cache intermediate feature representations, we pro- cess each image x from an image dataset Dx through the network up until transformer blockL. Next, we store the re- sulting features f. After processing Dx we obtain a (frozen) feature dataset Df , with f ∈ Df (Fig. 2b). Finally, we train a lightweight model using the cached (frozen) features. Fig. 2c shows an example where a single MAP layer followed by a classification layer is trained using the feature dataset Df . Since our focus is fast training, we defer a detailed analysis on larger models to future work. 3.3. Frozen Feature Augmentation (FroFA) Data augmentation is a common tool to improve general- ization and is typically applied on the input, or in our case: 2(Frozen) Pretrained Model TB TB TB MAP CL (a) Step 1: Select a (frozen) pretrained model and a layer for caching. (Frozen) Pretrained Model image dataset (frozen) feature dataset TB TB TB (b) Step 2: Process an image dataset and cache the (frozen) features. Lightweight Model (frozen) feature dataset MAP CL frozen feature augmentation (FroFA)  (c) Step 3: Train on (augmented) frozen features. Figure 2. Pipeline for caching and training on (frozen) fea- tures. (2a): Given a (frozen) pretrained vision transformer, withL Transformer blocks (TBs), a multi-head attention pooling (MAP) layer, and a classification layer (CL), we select its L-th Trans- former block for caching. (2b): Next, we feed images x ∈ Dx to cache (frozen) features f ∈ Df . (2c): Finally, we use Df to train a lightweight model on top. We investigate frozen feature augmentation (FroFA) af ∈ Af in this scenario. images. A natural question arises: How to map such image augmentations to intermediate feature representations? Recall that the feature representation f = (fn,c) ∈ RN×C (layer index ℓ omitted) is two-dimensional. We first reshape it to a three-dimensional representation, i.e., f∗ = (f∗ n1,n2,c) ∈ R √ N× √ N×C. (1) We further define f∗ c = f∗ :,:,c ∈ R √ N× √ N×1 (2) as a two-dimensional representation of the c-th channel. Images and feature representations differ in two funda- mental aspects: channel dimensionality and value range. Before adapting image augmentations to the feature space, it is crucial to handle these differences. Channel dimensionality: RGB images have just three channels while intermediate representations possess an ar- bitrary number of channels. To address this, we ignore im- age augmentations that rely on three color channels, e.g., color jitter, and consider augmentations which can have an arbitrary number of channels instead, denoted asCa, cover- ing a majority of commonly applied image augmentations. Value range: RGB values lie within a specific range I, e.g., I = [0, 1] or I = {0, ...,255} ⊂N0, while in theory features have no such constraints. Assuming H = √ N and W = √ N, we define an image augmentation as ax : I √ N× √ N×Ca → I √ N× √ N×Ca , ax ∈ Ax, (3) where Ax is the set of image augmentations andCa = C is an arbitrary number of channels. To also address the value range mismatch, we introduce a deterministic feature-to- image mapping tf→x : R √ N× √ N×Ct → I √ N× √ N×Ct (4) that maps each element of f∗ (1) from R to I. In our exper- iments, we use xf = tf→x(f∗) = f∗ − fmin fmax − fmin , (5) where fmin and fmax are the minimum and maximum value of f∗, respectively, with elements of xf now in I = [0, 1]. We further define an image-to-feature mapping tf←x : I √ N× √ N×Ct → R √ N× √ N×Ct (6) that maps xf back to the original feature value range, with Ct = C by default. In this case, we simply invert (4) and use f∗ = tf←x(xf ) =xf · (fmax − fmin) +fmin. (7) Combining (3), (4), and (6), we obtain a generic (frozen) feature augmentation (FroFA) as a function composition af = tf←x ◦ ax ◦ tf→x. (8) We use three variations of af : 1. (Default) FroFA: We applyaf (8) once across the entire feature representation. We set Ca = Ct = C and com- pute fmin and fmax in (5), (7) across all elements of f∗. Further, as normally done in pixel space,ax (3) samples a random augmentation value and changes all elements of xf using the same value. For example, employing random contrast in a FroFA fashion scales each element of xf by the exact same randomly sampled factor. 2. Channel FroFA (cFroFA) : For each channel in the mapped features xf (5), ax (3) samples a random aug- mentation value per channel and applies that value to all elements in that channel. By using cFroFA for our ran- dom contrast example, we obtain C independently sam- pled scaling factors, one for each channel. 3. Channel2 FroFA (c2FroFA): In addition to applying augmentations per channel as done in cFroFA,tf→x (4) and tx←f (6) also operate per channel. In this case,fmin and fmax are the per-channel maximum and minimum, respectively. In contrast, FroFA and cFroFA use the maximum and minimum across the entire feature. We 3denote this variant as c 2FroFA since both the mappings (4), (6) and the augmentation (3) are applied on a per- channel basis. Although not adding additional stochas- ticity, we found that for random brightness this variant gives more stable results across a range of augmentation hyper parameters. While an element-wise FroFA might seem like a natural next step, our initial experiments lead to significantly worse results. We hypothesize that per-element augmentations might lead to substantial changes in the feature appearance. 4. Experimental Setup In this section, we introduce our experimental setup. 4.1. Network Architectures We employ the following pretrained vision transformers from prior work: Ti/16 [49], B/16 [16], and L/16 [16]. Fur- ther, we follow [56] and employ a lightweight multi-head attention pooling (MAP) layer [30] before the final classifi- cation layer on top of the frozen features (cf . Sec. 3.3). 4.2. Datasets Pretraining: We consider three datasets: JFT-3B, ImageNet-21k, and WebLI. First introduced by Hinton et al. [21], JFT is now a widely used proprietary, large-scale dataset [5, 7, 11, 16, 26, 27, 47, 56]. For our investigations we use the JFT-3B version following Zhai et al . [56]. It consists of nearly 3 billion multi-labeled images following a class-hierarchy of 29,593 labels. We further use ImageNet- 21k [14, 44] which consists of 14,197,122 (multi)-labeled images and 21,841 distinct labels. We equally split the first 51,200 images into a validation and test set and use the remaining 14,145,922 images for training. As a third dataset, we use WebLI [5] which is a recently introduced web-scale multilingual image-text dataset. Please refer to the Appendix, Sec. A3.1, for more details. Few-shot transfer : After pretraining we use eight datasets for few-shot transfer: ILSVRC-2012 [14, 44], CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. ILSVRC-2012, also known as ImageNet-1k, is a slimmed version of ImageNet-21k and contains 1,281,167 training images of 1,000 classes. We use it as our main few-shot benchmark throughout the paper. We randomly sample 1-shot, 5-shot, 10-shot, and 25-shot versions from the first 10% of the training set. We further create addi- tional disjoint sets by using the next four 10% fractions of the training set. In addition, we follow previous works [3] and create a ‘minival’ set using the last 1% (12,811 images) of the ILSVRC-2012 training set. The ‘minival’ set is used for hyper parameter tuning and design decisions while the official ILSVRC-2012 validation set is used as a test set. In summary, our setup consists of 1,000, 5,000, 10,000, or 25,000 training images, 12,811 validation images (‘mini- val’), and 50,000 test images (‘validation’). For the other seven datasets, we also select a training, validation, and test split and create few-shot versions. More details on how these splits are created can be found in the Appendix, Sec. A3.1. We follow a similar procedure as with ILSVRC-2012 and use 10% of the training images to cre- ate 1-shot, 5-shot, 10-shot, and 25-shot versions of each dataset. We further use each validation set for hyper pa- rameter tuning and report final results on the respective test set. 4.3. Data Augmentation We reuse the set of augmentations first defined in AutoAug- ment [9] and adopted in later works, such as RandAugment [10] and TrivialAugment [39]. In addition, we also consider a few other image augmentations [35, 48, 58]. We select five geometric augmentations, i.e., rotate, shear-x, shear-y, translate-x, and translate-y; four crop & drop augmenta- tions, i.e., crop, resized crop, inception crop [48], and patch dropout [35]; seven stylistic augmentations, i.e., brightness, contrast, equalize, invert, posterize, sharpness, and solarize; and two other augmentations, i.e., JPEG and mixup [58]. In total, we end up with eighteen distinct augmentations . Note that all data augmentations incorporate random oper- ations, e.g., a random shift in x- and y-direction (translate- x and translate-y, respectively), a randomly selected set of patches (patch dropout), a random additive value to each feature (brightness), or a random mix of two features and their respective classes (mixup). Please refer to the Ap- pendix, Sec. A3.2, for more details. We focus on the following set of experiments: 1. We investigate FroFA for all eighteen augmentations. 2. For our top-performing FroFAs, namely, brightness, contrast, and posterize, we incorporate additional stochasticity using cFroFA and c 2FroFA variants ( cf . Sec. 3.3). 3. We investigate a sequential protocol where two of the best three (c/c 2)FroFAs are arranged sequentially, namely, brightness c 2FroFA, contrast FroFA, and pos- terize cFroFA. We test all six possible combinations. 4. Finally, we also apply variations of RandAugment [10] and TrivialAugment [39] directly on top of cached frozen features. More details and results can be found in the Appendix, Secs. A3.2 and A4, respectively. 4.4. Training & Evaluation Details We describe some base settings for pretraining, few- shot learning, and evaluation. Please refer to Appendix, Sec. A3.3 for more training details. Pretraining: We use the Big Vision code base for https://github.com/google-research/big_vision 4pretraining. We take the Ti/16, B/16, and L/16 models pre- trained on JFT-3B from Zhai et al. [56]. In addition, we pretrain Ti/16, B/16 and L/16 on ImageNet-21k following the settings of Steiner et al. [46]. To further explore trans- fer capabilities we also use an L/16 model with sigmoid language-image pretraining (SigLIP) [57] on WebLI [5]. Few-shot learning: We use the Scenic code base [12] for few-shot learning. We train the lightweight MAP-based head by sweeping across five batch sizes (32, 64, 128, 256, and 512), four learning rates (0.01, 0.03, 0.06, and 0.1), and five training step sizes (1,000; 2000; 4,000; 8,000; and 16,000), yielding 100 configurations for each shot. We use the respective validation set for early stopping and to find the best sweep setting. Our cached-feature setup fits on a single-host TPUv2 platform where our experiments run in the order of minutes. Evaluation: We report the top-1 accuracy across all our few-shot datasets. On ILSVRC-2012, we tune few-shot models exclusively on our validation set (our ILSVRC-2012 ‘minival’, cf . Sec. 4.2) and report results on our test set (of- ficial ILSVRC-2012 ‘validation’ set, cf . Sec. 4.2). 4.5. Baseline Models We establish two baselines: MAP and linear probe. MAP: We first cache theN×C-shaped (frozen) features from the last transformer block. Afterwards, we train a lightweight MAP head from scratch using the cached fea- tures followed by the final classification layer ( cf . Fig. 2). For simplicity, the MAP head follows the same architectural design as the underlying pretrained model. In some exper- iments, we additionally apply weight decay (wd), denoted as MAPwd. We sweep across [ADD V ALUES] and use the respective validation set for early stopping and to find the best sweep setting. Linear probe: We use cached1×C-shaped outputs from the pretrained MAP head to solve an L2-regularized regres- sion problem with a closed-form solution [56]. We sweep the L2 decay factor using exponents of 2 ranging from -20 up to 10. This setting is our auxiliary baseline. 5. Finding the Optimal FroFA Setup We focus our first investigations on an L/16 model pre- trained on JFT-3B, i.e., our largest model and largest im- age classification pretraining dataset, followed by few-shot learning on subsets of ILSVRC-2012 training set, i.e., our largest few-shot dataset. We will refer to this setup as our L/16 JFT-3B base setup. 5.1. Baseline Performance We first report the baseline performance in Tab. 1. We ob- serve a large gap between MAP and linear probe in the 1- https://github.com/google-research/scenic Method 1-shot 5-shot 10-shot 25-shot MAP 57.9 78.8 80.9 83.2 Linear probe 66.5 79.6 81.5 82.4 Table 1. Average top-1 accuracy for baseline settings on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5) and follow the respective baseline setting ( cf . Sec. 4.5). The best setting for each baseline is found using our ILSVRC- 2012 validation set. Further, each shot is sampled five times. The best result per shot is boldfaced. shot setting (-8.6% absolute) which significantly decreases in the 5-, 10-, and 25-shot settings to -0.8%, -0.6%, and +0.8% absolute, respectively. In the following, our main point of comparison is the MAP baseline. This might be counter-intuitive since the performance is worse than linear probe in most cases. How- ever, the higher input dimensionality in the MAP-based set- ting (cf . Sec. 4.5) gives us the option to reshape the input to three dimensions ( cf . Sec. 3.3) which opens up more room and variety for frozen feature augmentations (Fro- FAs). Later in Sec. 6.4, we compare the performance of our best augmentations to the linear probe baseline. 5.2. Default FroFA As a next step, we investigate the effect of adding a single FroFA to the MAP baseline setting. We first focus on the default FroFA formulation which uses a single randomly sampled value per input ( cf . Sec. 3.3). Results are shown in Tab. 2 where we report gains with respect to the MAP baseline using eighteen distinct FroFAs categorized into ge- ometric, crop & drop, stylistic, and other. Geometric: Interestingly, all geometric augmentations consistently lead to worse performance across all settings. Crop & drop: A simple crop or a resized crop yield a significant performance boost in the 1-shot setting of +3.0% and +1.9% absolute, respectively. Further, patch dropout provides modest gains in the 1-shot regime. Dropping patches is related to training efficiency, so we investigate this further. Fig. 3a shows the top-1 accuracy on 1- and 25- shot as a function of number of patches. More results can be found in Appendix, Sec. A4.1. Similar to observations by Liu et al. [35] we can randomly drop a large fraction of patches (>50%) without loosing performance. A key dif- ference is that Liu et al. only investigated the effect in the image space, while we provide evidence that patch dropout also transfers to the feature space. Finally, inception crop does not improve performance. Stylistic: The largest gains can be observed when em- ploying a stylistic FroFA, in particular brightness, contrast, and posterize. We identified brightness as the best perform- ing FroFA with absolute gains of 4.8% on 1-shot, 1.1% on 5-shot, and up to 0.6% on 10-shot. 5Geometric Crop & drop Stylistic Other Shots MAP rotate shear-x shear-y translate-x translate-y crop res. crop incept. crop patch drop. brightness contrast equalize invert posterize sharpness* solarize* JPEG* mixup 1 57.9 −1.3 −0.6 −0.8 −1.2 −1.4 +3.0 +1.9 +0.0 +0.4 +4.8 +2.8 +1.0 +2.7 +3.7 −0.1 +1.0 −0.1 −1.4 5 78.8 −0.3 −0.2 −0.2 −0.3 −0.3 +0.0 −0.2 +0.0 +0.0 +1.1 +0.8 +0.5 −0.3 +0.8 +0.1 −0.1 −0.3 −0.3 10 80.9 −0.2 −0.1 −0.1 −0.2 −0.2 +0.0 −0.2 +0.0 +0.0 +0.6 +0.6 +0.4 +0.0 +0.6 +0.1 +0.0 −0.1 +0.2 25 83.2 −0.2 −0.1 −0.2 −0.1 −0.2 +0.0 −0.1 −0.1 +0.0 +0.1 +0.1 +0.0 −0.2 +0.0 +0.0 +0.0 +0.0 +0.1 Table 2. (Average) top-1 accuracy for default FroFA on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). In total, we investigate eighteen FroFAs, categorized intogeometric, crop & drop, stylistic, and other. We sweep across a base sweep ( cf . Sec. 4.4) and the respective augmentation sweep (cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 validation set. Each shot is sampled five times, except for JPEG, sharpness, and solarize (marked with ‘*’). We highlight deterioration by shades of red and improvement by shades of green . Best three FroFAs are boldfaced. 1 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA (a) Patch dropout FroFA 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot + brightness cFroFA + brightness c2FroFA (b) Channel variants (c/c2) of brightness FroFA Figure 3. Average top-1 accuracy for FroFA variantson our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each FroFA operation point (cf . Appendix, Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. Brightness Contrast Posterize Shots MAP c c 2 c c 1 57.9 +4.8 +5.9 +6.1 +2.8 +2.5 +3.7 +5.9 5 78.8 +1.1 +1.5 +1.6 +0.8 +0.0 +0.8 +0.8 10 80.9 +0.6 +1.1 +0.9 +0.6 +0.0 +0.6 +0.5 25 83.2 +0.1 +0.4 +0.3 +0.1 −0.1 +0.0 +0.0 Table 3. Average top-1 accuracy for a selection of default ( ) and channel (c/c 2) FroFA on our ILSVRC-2012 test set. Ab- solute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep (cf . Sec. 4.4) and the respective augmentation sweep ( cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 val- idation set. Each shot is sampled five times. The best results per shot and FroFA are boldfaced (multiple ones if close, i.e., ±0.2). Other: Neither JPEG nor mixup yield performance gains but rather more or less worsen the performance. 5.3. Channel FroFA Next, we investigate channel FroFA (cFroFA) for bright- ness, contrast, and posterize. Results are shown in Tab. 3, where we report absolute gains with respect to the MAP baseline. First, contrast cFroFA worsens performance across all shots. Second, posterize cFroFA improves perfor- mance on 1-shot from +3.7% to +5.9% while maintaining performance on all other shots. Lastly, brightness cFroFA significantly improves performance across all shots, i.e., from +4.8% to +5.9% on 1-shot, from +1.1% to +1.5% on 5-shot, from +0.6% to +1.1% on 10-shot, and from +0.1% to +0.4% on 25-shot. Giving the strong improvements for brightness cFroFA, we further test brightness c 2FroFA (see Tab. 3). On a first look, both variants perform equally well. In Fig. 3b, we further report the top-1 accuracy on 1-shot and 25-shot as a function of the brightness augmentation level. Results across other shots are similar and can be found in Appendix, Sec. A4.1. We clearly observe that brightness cFroFA is much more sensitive to the brightness level than brightness c2FroFA. Aross all shots, brightness cFroFA only works well for small brightness levels (0.1 to 0.5), while the c2FroFA variant performs better than the MAP baseline across the board. We attribute the better sensitivity prop- 6erties of brightness c2FroFA to the channel-wise mappings (5), (7) since this is the only change between cFroFA and c2FroFA. We did not a observe similar effect when switch- ing from cFroFA posterize to c2FroFA posterize. 5.4. Sequential FroFA Finally, out of our best three augmentations, i.e., bright- ness c 2FroFA (B-c 2), contrast FroFA (C), and posterize cFroFA (P-c), we combine two of them sequentially. We end up with a total of six combinations. Tab. 4 compares the performance of these six combinations against our prior best (B-c 2). On 1-shot, (B-c 2→P-c) significantly outper- forms (B-c2), improving absolute gains from 6.1% to 7.7%, while maintaining performance on other shots. We con- clude that advanced FroFA protocols may further improve performance. As an initial investigation, we applied varia- tions of RandAugment and TrivialAugment using our best three FroFAs ( cf . Tab. 3), however, with limited success. We include results in the Appendix, Sec. A4.2, and leave a deeper investigation to future works. 6. FroFA on More Datasets and Architectures How well does our best non-sequential augmentation strat- egy (brightness c 2FroFA) transfer across multiple dataset and architectures settings? In Secs. 6.1 to 6.3, we report results on seven other downstream few-shot datasets, two additional architectures, and two additional pretraining se- tups, respectively. This time, however, we also incorpo- rate weight decay in all MAP-based models . Further, in Secs. 6.2 and 6.3, we solely focus on the improvements over the MAP baseline and include a discussion on the improve- ments over the linear probe baseline in Secs. 6.1 and 6.4. 6.1. Transfer to Other Downstream Datasets In Tab. 5, we report results on seven additional transfer datasets, i.e., CIFAR10, CIFAR100, DMLab, DTD, Re- sisc45, SUN397, and SVHN. We compare the weight- decayed MAP and L2-regularized linear probe baseline to our approach, i.e., weight-decayed MAP combined with brightness c2FroFA (MAPwd + FroFA). We observe that across almost all shots and transfer datasets, MAP wd + FroFA shows the best results. Moreover, MAP wd + FroFA outperforms L2-regularized linear probe with only one exception, i.e., SUN397 (1-shot). With respect to the mean across all seven datasets, MAP wd + FroFA is signifi- cantly better than MAPwd, with improvements ranging from +4.4% absolute on 1-shot to +1.0% absolute on 25-shot. Fig. 1, left, displays the absolute accuracy gains averaged across all eight transfer datasets, including ILSVRC-2012. As before, our approach, i.e., MAPwd + FroFA, yields the best results across all shots. We further observe that the gains decrease with higher shots which aligns with our pre- vious observations. Shots MAP B-c 2 B-c2→C C→ B-c2 B-c2→P-c P-c→ B-c2 C→P-c P-c→C 1 57.9 +6.1 +4.0 +2.7 +7.7 +5.2 +5.0 +3.1 5 78.8 +1.6 +1.5 +0.2 +1.5 +0.4 +1.3 +0.0 10 80.9 +0.9 +1.2 +0.1 +1.0 +0.1 +0.9 +0.3 25 83.2 +0.3 +0.4 −0.7 +0.2 −0.5 +0.2 −0.4 Table 4. Average top-1 accuracy for a sequential FroFA pro- tocol on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We combine the best settings of brightness c 2FroFA (B- c2), contrast FroFA (C), and posterize cFroFA (P-c) sequentially (two at a time, order indicated by ‘ ↑’). We sweep across a base sweep (cf . Sec. 4.4) to first find the best setting on our ILSVRC- 2012 validation set. Each shot is sampled five times. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Trans. dataset Method 1-shot 5-shot 10-shot 25-shot CIFAR10 MAPwd 85.1 96.7 97.1 97.5 Linear probe 80.9 94.1 96.7 97.3 MAPwd + FroFA 93.8 97.6 97.8 97.8 CIFAR100 MAPwd 63.1 82.7 85.5 86.8 Linear probe 58.4 80.9 83.8 85.1 MAPwd + FroFA 67.8 84.0 86.2 87.1 DMLab MAPwd 24.4 30.3 30.2 36.5 Linear probe 24.0 26.3 25.6 30.9 MAPwd + FroFA 27.1 29.4 30.3 36.8 DTD MAPwd 49.2 68.2 74.1 80.8 Linear probe 46.9 65.9 71.3 77.3 MAPwd + FroFA 53.5 70.7 76.1 82.2 Resisc45 MAPwd 63.2 86.9 89.8 90.7 Linear probe 67.1 85.6 88.2 91.0 MAPwd + FroFA 67.6 87.2 89.7 91.5 SUN397 MAPwd 51.3 73.5 77.7 80.3 Linear probe 56.7 70.9 75.6 78.6 MAPwd + FroFA 56.2 75.9 78.9 81.2 SVHN MAPwd 20.7 23.9 30.2 47.4 Linear probe 11.8 15.0 18.7 21.5 MAPwd + FroFA 21.8 31.0 43.5 50.3 Mean MAPwd 51.0 66.0 69.2 74.3 Linear probe 49.1 62.7 65.7 68.8 MAPwd + FroFA 55.4 68.0 71.8 75.3 Table 5. Top-1 accuracy of our best FroFA for additional transfer datasets using a JFT-3B L/16 model. Results are re- ported on the respective test set ( cf . Sec. A3.1). We compare results to a weight-decayed MAP baseline, i.e., MAP wd, and an L2-regularized linear probe. Depending on the setting, we sweep across a base,cf . Sec. 4.4, a weight decay or L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on the respective validation set. Per shot and dataset, the best result is boldfaced while the second-best result is underlined (multiple ones if close, i.e., ±0.2). 7Ti/16 B/16 L/16 model −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model −0.5 0.0 0.5 5-shot Ti/16 B/16 L/16 model 0.00 0.25 0.50 0.75 1.00 1.25 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 5 25-shot MAPwd linear probe (a) JFT-3B Ti/16 B/16 L/16 model −20 −15 −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 5-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 25-shot (b) ImageNet21k Figure 4. Average top-1 accuracy of brightness c2FroFA for JFT-3B (a) and ImageNet-21k (b) models on our ILSVRC-2012 test set trained on few-shotted ILSVRC-2012 training sets. Absolute gains to the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline are reported. Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. 6.2. Transfer to Other Architectures We employ brightness c2FroFA on two other JFT-pretrained models, namely Ti/16 and B/16. In Fig. 4a, we report im- provements in top-1 accuracy with respect to the weight- decayed MAP baseline. Across all shots and model archi- tectures, incorporating FroFA either maintains or improves performance, except for B/16, 25-shot. Given that larger models tend to be more prone to overfitting in the 1-shot setting, we observe increasing improvements from FroFA when scaling the architecture. With a higher number of shots, the observed improvements over the baseline model become smaller. We attribute this to the strong baseline per- formance leaving lesser headroom for improvements. We refer to the Appendix, Sec. A4.3, for the exact values. 6.3. Transfer to Other Pretraining Setups ImageNet-21k: In Fig. 4b, we report improvements in top- 1 accuracy with respect to the weight-decayed MAP base- line for ImageNet-21k-pretrained Ti/16, B/16, and L/16. Consistent with our JFT-3B observations, across all shots and model architectures, incorporating FroFA either main- tains or improves performance. The improvements dimin- ish as the number of shots increases. This trend is likely due to the higher baseline accuracies at higher shot counts. We again refer to the Appendix, Sec. A4.3, for the exact values. WebLI and SigLIP : We also tested an L/16 model with sigmoid language-image pretraining (SigLIP), follow- ing [57]. We report the absolute accuracy gains averaged across eight datasets. The results are shown in Fig. 1, right. From the results we can conclude that our FroFA setting also transfers to language-image pretrained models further emphasizing its generalizability. 6.4. Linear Probe Comparison on ILSVRC-2012 We will now look at Figs. 4a and 4b, but discuss gains with respect to the L2-regularized linear probe baseline. We start with models pretrained on JFT-3B (cf . Fig. 4a). On 1-shot, we observe that we lack behind linear probe but can close the gap by scaling up the model size. On 5- to 25-shot, with the exception of Ti/16 on 5-shot, brightness c 2FroFA significantly outperforms the linear probe baseline. On ImageNet-21k (cf . Fig. 4b), we observe even larger gaps to linear probe on 1-shot (up to -20% absolute). How- ever, similar to results on JFT-3B, performance on 5- to 25-shot improves significantly over linear probe or at worst stays the same. 7. Conclusions We investigated eighteen frozen feature augmentations (FroFAs) along three axes: model size, pretraining and transfer few-shot dataset. We show that a training with Fro- FAs, in particular stylistic ones, gives large improvements upon a representative baseline across all shots. In addition, per-channel variants further improve performance, e.g., by 1.6% absolute in the ILSVRC-2012 5-shot setting. Finally, we were able to show that our results transfer. Averaged results across seven downstream tasks show that using a variant of brightness FroFA improves by 4.4% absolute upon the same representative baseline in the 1-shot setting. 8References [1] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images, 2009. 2, 4, 12 [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K ¨uttler, Andrew Lefrancq, Simon Green, V ´ıctor Vald ´es, Amir Sadik, Julian Schrit- twieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hass- abis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv, 1612.03801:1–11, 2016. 4, 12 [3] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet- ter Plain ViT Baselines for ImageNet-1k.arXiv, 2205.01580: 1–3, 2022. 4 [4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recogni- tion. In Proc. of NeurIPS, pages 16664–16678, New Orleans, LA, USA, 2022. 2 [5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad- bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A Jointly-Scaled Multilingual Language- Image Model. InProc. of ICLR, pages 1–33, Kigali, Rwanda, 2023. 1, 2, 4, 5, 12 [6] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens- ing Image Scene Classification: Benchmark and State of the Art. Proc. IEEE, 105(10):1865–1883, 2017. 4, 12 [7] Franc ¸ois Chollet. Xception: Deep Learning With Depthwise Separable Convolutions. In Proc. of CVPR , pages 1063– 6919, Honolulu, HI, USA, 2017. 4 [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing Textures in the Wild. In Proc. of CVPR, pages 3606–3613, Columbus, OH, USA, 2014. 4, 12 [9] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va- sudevan, and Quoc V . Le. AutoAugment: Learning Aug- mentation Strategies From Data. In Proc. of CVPR , pages 113–123, Long Beach, CA, USA, 2019. 2, 4 [10] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. RandAugment: Practical Automated Data Augmenta- tion with a Reduced Search Space. In Proc. of NeurIPS , pages 18613–18624, virtual, 2020. 1, 4, 13 [11] Zihang Dai, Hanxiao Liu, Quoc V . Le, and Mingxing Tan. CoAtNet: Marrying Convolution and Attention for All Data Sizes. In Proc. of NeurIPS, pages 3965–3977, virtual, 2021. 4 [12] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A JAX Library for Computer Vision Research and Beyond. In Proc. of CVPR, pages 21393–21398, New Orleans, LA, USA, 2022. 5 [13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschan- nen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Ku- mar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar- avindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vigh- nesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion Parameters. In Proc. of ICML , pages 7480–7512, Honolulu, HI, USA, 2023. 1, 2, 12 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proc. of CVPR , pages 248–255, Miami, FL, USA, 2009. 1, 2, 4, 12 [15] Terrance DeVries and Graham W. Taylor. Dataset Augmen- tation in Feature Space. In Proc. of ICLR - Workshops, pages 1–12, Toulon, France, 2017. 2 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proc. of ICLR, pages 1–21, virtual, 2021. 1, 2, 4 [17] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP- Adapter: Better Vision-Language Models with Feature Adapters. Int. J. Comput. Vis., pages 1–15, 2023. 2 [18] Edouard Grave, Armand Joulin, and Nicolas Usunier. Im- proving Neural Language Models with a Continuous Cache. In Proc. of ICLR, pages 1–9, Toulon, France, 2017. 2 [19] Xuehai He, Chuanyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. Parameter-Efficient Model Adaptation for Vision Transformers. In Proc. of AAAI, pages 817–825, Washington, DC, USA, 2023. 1, 2 [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. In Proc. of ICLR, pages 1–15, Virtual, 2020. 2 [21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling Knowledge in a Neural Network. In proc. of NIPS - Work- shops, pages 1–9, Montr´eal, QC, Canada, 2014. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2, 4 [22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. In Proc. of ICML , pages 2790–2799, Long Beach, CA, USA, 2019. 2 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR, pages 1–13, virtual, 2022. 2 [24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- 9sual Prompt Tuning. In Proc. of ECCV, pages 709–727, Tel Aviv, Israel, 2022. 2 [25] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, pages 1–15, San Diego, CA, USA, 2015. 13 [26] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big Transfer (BiT): General Visual Representation Learning. In Proc. of ECCV, pages 491–507, virtual, 2020. 2, 4 [27] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, and Efi Kokiopoulou. Three Towers: Flexible Contrastive Learning with Pretrained Image Mod- els. arXiv, 2112.13492:1–32, 2023. 4 [28] Varun Kumar, Hadrien Glaude, Cyprien de Lichy, and Wl- liam Campbell. A Closer Look At Feature Space Data Aug- mentation For Few-Shot Intent Classification. In Proc. of EMNLP - Workshops, pages 1–10, Hong Kong, China, 2019. 2 [29] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. The Omniglot Challenge: a 3-year Progress Re- port. Curr. Opin. Behav. Sci., 29:97–104, 2019. 2 [30] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se- ungjin Choi, and Yee Whye Teh. Set Transformer: A Frame- work for Attention-based Permutation-Invariant Neural Net- works. In Proc. of ICML , pages 3744–3753, Long Beach, CA, USA, 2019. 1, 2, 4 [31] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision Transformer for Small-size Datasets. arXiv, 2112.13492:1–11, 2021. 1 [32] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proc. of EMNLP, pages 3045–3059, virtual, 2021. 2 [33] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You. Data Augmentation via Latent Space Interpolation for Image Classification. In Proc. of ICPR , pages 728–733, Beijing, China, 2018. 2 [34] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai. Efficient Training of Visual Transformers With Small Datasets. In Proc. of NeurIPS , pages 1–13, virtual, 2021. 1 [35] Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Az- izpour, and Kevin Smith. PatchDropout: Economizing Vi- sion Transformers Using Patch Dropout. In Proc. of WACV, pages 3942–3951, Waikoloa, HI, USA, 2023. 2, 4, 5 [36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In Proc. of ICCV, pages 10012–10022, virtual, 2021. 1 [37] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali Shrivastava, and Andrew Gordon Wilson. Learning Multimodal Data Augmentation in Feature Space. In Proc. of ICLR, pages 1–15, Kigali, Rwanda, 2023. 2 [38] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Proc. of ICLR, pages 1–18, New Orleans, LA, USA, 2019. 13 [39] Samuel G. M ¨uller and Frank Hutter. TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation. In Proc. of ICCV, pages 774–782, virtual, 2021. 2, 4, 13 [40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y . Ng. Reading Digits in Nat- ural Images with Unsupervised Feature Learning. In proc. of NIPS - Workshops , pages 1–9, Granada, Spain, 2011. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 4, 12 [41] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah- moud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je- gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi- otr Bojanowski. Dinov2: Learning Robust Visual Features Without Supervision. arXiv, 2304.07193:1–31, 2023. 1 [42] Emin Orhan. A Simple Cache Model for Image Recognition. In Proc. of NeurIPS, pages 10128–10137, Montr´eal, Canada, 2018. 2 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML, pages 8748–8763, virtual, 2021. 1, 2 [44] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. Int. J. Comput. Vis., 115(3):211–252, 2015. 1, 2, 4, 12 [45] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In Proc. of ICML, pages 4596–4604, Stockholm, Sweden, 2018. 13 [46] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers. Trans. Mach. Learn. Res., pages 1–16, 2022. 2, 5, 13 [47] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. In Proc. of ICCV , pages 843–852, Venice, Italy, 2017. 4 [48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception Ar- chitecture for Computer Vision. In Proc. of CVPR , pages 2818–2826, Las Vegas, NV , USA, 2016. 2, 4 [49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training Data-Efficient Image Transformers & Distillation Through Attention. In Proc. of ICML , pages 10347–10357, virtual, 2021. 4 [50] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na- jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben- gio. Manifold Mixup: Better Representations by Interpo- lating Hidden States. In Proc. of ICML, pages 6438–6447, Long Beach, CA, USA, 2019. 2 10[51] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. In Proc. of NIPS , pages 3637–3645, Barcelona, Spain, 2016. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2 [52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra- mid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. In Proc. of ICCV , pages 548–558, virtual, 2021. 1 [53] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN Database: Large-Scale Scene Recognition from Abbey to Zoo. In Proc. of CVPR, pages 3485–3492, San Francisco, CA, USA, 2010. 2, 4, 12 [54] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. SUN Database: Exploring a Large Collection of Scene Categories. Int. J. Comput. Vis., 119(1): 3–22, 2016. 2, 4, 12 [55] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andr´e Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A Large-scale Study of Representation Learn- ing with the Visual Task Adaptation Benchmark. arXiv, 1910.04867:1–33, 2020. 4, 12 [56] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu- cas Beyer. Scaling Vision Transformers. In Proc. of CVPR, pages 12104–12113, New Orleans, LA, USA, 2022. 1, 2, 4, 5, 12, 13 [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre- Training. In Proc. of ICCV , pages 11975–11986, Paris, France, 2023. 1, 5, 8, 13 [58] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond Empirical Risk Mini- mization. In Proc. of ICLR , pages 1–13, Vancouver, BC, Canada, 2018. 1, 2, 4 [59] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun- chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip- Adapter: Training-Free Adaption of CLIP for Few-Shot Classification. In Proc. of ECCV, pages 493–510, Tel Aviv, Israel, 2022. 2 11Frozen Feature Augmentation for Few-Shot Image Classification Supplementary Material A1. Introduction We give additional details and results to complement the main paper. All included citations refer to the main paper’s references. A2. Brightness We provide the code snippet for brightness c2 FroFa def transform_aug_reverse( x, augment, aug_min_val=0, aug_max_val=1.0, x_min_val=None, x_max_val=None, clip=True): \"\"\"Transform to (low, high)-space, perform augmentation, transform back.\"\"\" l = x_min_val if x_min_val is None: l = tf.reduce_min(x) h = x_max_val if x_max_val is None: h = tf.reduce_max(x) # [l, h] --> [0, 1] x = (x - l) / (h - l + 1e-8) # [0, 1] --> [low, high] x = x * (aug_max_val - aug_min_val) x = x + aug_min_val x = tf.cast(augment(x), tf.float32) if clip: tf.clip_by_value(x, aug_min_val, aug_max_val) # [low, high] --> [0, 1] x = (x - aug_min_val) x = x / (aug_max_val - aug_min_val) x = x * (h - l + 1e-8) + l # [0, 1] --> [l, h] return x def get_random_brightness(max_delta=0.1, clip=False): # A random value in [-max_delta, +max_delta] # is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. def _random_brightness(image): return tf.image.random_brightness( image, max_delta) def tar(x): return transform_aug_reverse( x, augment=_random_brightness, aug_min_val=0, aug_max_val=1.0, clip=clip) return tar def get_random_brightness_per_channel_v2( max_delta=0.1, clip=True): \"\"\"Applies channel-wise random brightness transformations.\"\"\" # A random value in [-max_delta, +max_delta] is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. random_brightness = get_random_brightness( max_delta, clip) def _random_brightness_pc(x): x = tf.expand_dims(x, axis=2) # (H, W, 1, C) x = tf.unstack(x, axis=-1) # C x (H, W, 1) x = [random_brightness( {\"image\": x_i})[\"image\"] for x_i in x] return tf.concat(x, axis=-1) return _random_brightness_pc A3. Detailed Experimental Setup In the following, we provide additional details to our exper- imental setup. A3.1. Datasets In this section, we focus on details regarding our pretraining and few-shot datasets. Pretraining: As stated in the main paper, Sec. 4.2, we pretrain our models by either using JFT-3B [56], ImageNet- 21k [14, 44], or WebLI [5]. In JFT-3B, the images are annotated with noisy labels by using a semi-automated pipeline. We follow common practice [13, 56] and ignore the hierarchical aspect of the labels. ImageNet-21k is a superset of the well known ILSVRC-2012 dataset, also known as “ImageNet-1k” or just “ImageNet”. WebLI is a recently introduced image- and-language dataset. It contains 10 billion images and tens of billions image-text pairs with over 100 languages. Few-shot transfer: As stated in the main paper, Sec. 4.2, our experiments concentrate around few-shot transfer on ILSVRC-2012 [14, 44]. We also provide results on CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. When official test and validation splits are available, we use them for eval- uation across all datasets. In general, we use the versions in TensorFlow Datasets. CIFAR10 contains 60,000 images of 10 equally dis- tributed classes split into 50,000 training images and 10,000 test images. We further split the official training dataset into 45,000 training images and 5,000 validation images. CIFAR100 is a superset of CIFAR10 with 100 equally distributed classes and 60,000 images. Similar to CIFAR10, we use 45,000 images for training, 5,000 images for valida- tion and 10,000 images for test. DMLab consists of frames collected from the DeepMind Lab environment. Each frame is annotated with one out https://www.tensorflow.org/datasets 12of six classes. We use 65,550 images for training, 22,628 images for validation, and 22,735 for test. DTD is a collection of 5,640 textural images categorized into 47 distinct classes. Each of the three splits, i.e., train- ing, validation, and test, has exactly 1,880 images. Resisc45 is a benchmark with 31,500 images for image scene classification in remote sensing scenarios. In total, 47 different catogries for scenes are defined. We use the first 23,000 images for training, the subsequent 2,000 images for validation and the last 6,300 images for test. SUN397 is a 397-category database of 108,753 images for scene understanding. We use 76,128 images for training, 10,875 images for validation, and 21,750 images for test. SVHN is a Google Street View dataset with a large col- lection of house number images. In total, 10 distinct classes exist. We use the cropped version with 73,257 images for training and 26,032 images for test. Further, we create a val- idation subset by only using the first 70,000 out of 73,257 training images for actual training and the remaining 3,257 images for validation. A3.2. Data Augmentation In this section, we provide additional details on the used data augmentation techniques and protocols. (c/c2)FroFA: In Tab. 6, we give detailed descriptions of each FroFA, cFroFA, and c 2FroFA setting. We mostly build upon an AutoAugment implementation from Big Vision. To keep it simple, we use v or v1, v2 as sweep parameter(s) for all augmentations. By default, we first re- shape the two-dimensional features f to three-dimensional features f∗ (1) of shape √ N × √ N × C, with N = 196 and C ∈ {192, 768, 1024} in all our experiments. Note that the value of C depends on the architecture. We further want to point out, while some augmentations heavily rely on the three-dimensional representation, e.g., all geometric ones, some others are also transferable to a two-dimensional rep- resentation, e.g., brightness or contrast. As pointed out in the main paper, Tab. 3, brightness c2FroFA, contrast FroFA, and posterize cFroFA are our best FroFAs. For all three, we list the best sweep settings in Tab. 7. Advanced protocols: As mentioned in the main paper, Sec. 4.3, besides our fixed sequential protocol ( cf . Tab. 4) we also tested variations of RandAugment [10] and Triv- ialAugment [39]. In all protocols, we sample from the best settings of brightness c2FroFA, contrast FroFA, and poster- ize cFroFA. In particular, we use v = 1.0 for brightness c2FroFA, v = 6.0 for contrast FroFA, and v1 = 1, v2 = 8 for posterize cFroFA ( cf . Tab. 6). We re-use the abbrevi- ations from Tab. 4 in the following, i.e., B-c 2, C, and P- c, respectively. For the RandAugment and TrivialAugment https://github.com/google- research/big_vision/ blob/main/big_vision/pp/autoaugment.py variations, we uniformly sample from either the best three FroFAs, i.e., Atop3 = {B-c2, C, P-c}, or the best two Fro- FAs, i.e., Atop2 = A3 \\ {C}. Further, our RandAugment variation randomly constructs a sequence of augmentations by uniformly sampling the integer sequence length from 1 to |A|, with A ∈ {Atop2, Atop3} depending on whether Atop2 or Atop3 is used. A3.3. Training Details Pretraining: In the JFT-3B setup, we use pretrained mod- els from Zhai et al. [56]. The models are pretrained using a sigmoid cross-entropy loss. The weights are optimized by Adafactor [45] in half-precision mode, β1 = 0.9, and β2 = 0.999. Further, (decoupled) weight decay [38] is applied with 3.0 on the head and 0.03 for the rest of the network weights. The learning rate is adapted by a recip- rocal square-root schedule for 4,000,000 steps with a lin- ear warm-up phase of 10,000 steps and a linear cool-down phase of 50,000 steps. The starting learning rate is 0.01 for Ti/16 and L/16 and 0.03 for B/16. The images are prepro- cessed by an224×224 inception crop and a random horizon- tal flip. We set the batch size to 4,096. To stabilize training, a global norm clipping of 1.0 is used. In the ImageNet-21k setup, we follow settings from Steiner et al. [46] and use a sigmoid cross-entropy loss for multi-label pretraining. We use the Adam optimizer [25] in half-precision mode and set β1 = 0.9 and β2 = 0.999. Fur- ther, we apply (decoupled) weight decay with either 0.03 for Ti/16 or 0.1 for B/16 and L/16. We adapt the learning rate using a cosine schedule for roughly 930,000 steps (300 epochs) with a linear warm-up phase of 10,000 steps. We set the starting learning rate to 0.001 for all models. During preprocessing, we crop the images to 224×224 following an inception-style crop and a random horizontal flip. While we don’t use any additional augmentation for Ti/16, we fol- low suggestions by Steiner et al. [46] and use the ‘light1’ and ‘medium2’ augmentation settings for B/16 and L/16, respectively. Finally, we use a batch size of 4,096 and sta- bilize training by using a global norm clipping of 1.0. In the WebLI setup, we take an L/16 model from [57]. In particular, we use [ADD DETAILS]. Few-shot learning: We first cache each few-shot dataset by processing each of them through a pretrained model and store the extracted features (cf . Fig. 2). We resize each im- age to 224×224 before feeding it to the model. We follow up with a training where we mostly use trans- fer learning settings from Steiner et al. [46]. We use a sig- moid cross-entropy loss. This might be non-intuitive given that all of our few-shot datasets are not multi-labeled. How- ever, we didn’t really observe any performance drops com- pared to using the more common softmax cross-entropy loss, so we stick to the sigmoid cross-entropy loss. We use stochastic gradient descent with momentum of 0.9. Simi- 13Augmentation Description Geometric rotate We rotate each of the C feature channels fc (2) by z ∼ U(−v, v). We sweep across v ∈ {15, 30, 45, 60, 75, 90} representing the maximum positive and negative rotation angle in degrees. shear-{x,y} We (horizontally/vertically) shear each of the C feature channels fc (2) by z ∼ U(0, v). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7} representing the maximum level of horizontal or vertical shearing. translate-{x,y} We (horizontally/vertically) translate each of theC feature channels fc (2) by uniformly samplingz from {0, 1, ..., v}. We sweep across integer values 1 ≤ v ≤ 7 representing the maximum horizontal or vertical translation. Crop & drop crop We randomly crop each of the C feature channels fc (2) to v×v at the same spatial position. We sweep across integer values 1 ≤ v ≤ 13 representing the square crop size. resized crop We resize each of the C feature channels fc (2) to v × v and then randomly crop each to 14 × 14 at the same spatial position. We sweep across v ∈ {16, 18, 20, 22, 24, 26, 28, 35, 42} representing the resized squared spatial resolution. inception crop We apply an inception crop with probability v. We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. patch dropout We randomly keep v out of N patches of f having shape N × C. Note that the patch ordering is also randomized. We sweep across v ∈ {1, 2, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 116, 132, 148, 164, 180}. Stylistic brightness We randomly add a value z ∼ U(−v, v) to each of the C feature channels fc (2). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. We test this method using all FroFA variants. In the default FroFA and the cFroFA variants, the features are scaled by (5) taking the minimumfmin and maximum fmax across all chan- nels into account. In the c 2FroFA variant, each channel fc (2) is shifted individually and uses the channel minimum and maximum instead. Further, in the cFroFA and c2FroFA variants we sample C values of z, one for each channel. contrast We randomly scale each of the C feature channels fc (2) by z ∼ U( 1 v , v). We sweep across v ∈ {1.25, 1.5, 2, 3, 4, 5, 6, 7, 9, 10}. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sample C values of z, one for each channel. equalize We first map the features from value range R to the integer subset I = {0, 1, ...,195}, i.e., executing (5) followed up by a discretization step. We choose this value range as preliminary results mapping from R to the more commonly used I = {0, 1, ...,255} instead didn’t show any effects. We continue by equalizing 196 bins and then transforming the results back to the original space using (7). We apply equalize with probability v. In particular, we sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. invert We change the sign of features f∗ with probability v. We sweep acrossv ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. posterize We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. In other words, we use an 8-bit representation for features f∗. Posterize performs a quantization by a bit-wise left and right shift. We uniformly sample the shift value z between integer values v1 and v2. In our sweep, we test a subset of all possible combinations. In particular, we first set v2 = 8and reduce v1 from 7 to 1. We then fix v1 = 1and increase v2 from 2 to 7 again. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sampleC values of z, one for each channel. sharpness We first apply a two-dimensional convolution on f∗ (1) using a 3×3 smoothing filter. Next, we mix the original features with the resulting “smoothed” features using a randomly sampled blending factor z ∼ U(0, v). We sweep across v ∈ {0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0, 3.0}. solarize We do not map features from R to I = [0, 1], but stay in R. We compute the minimum fmin and maximum fmax across features f∗. We conditionally subtract all values smaller than0.5·fmin from fmin or larger than0.5·fmax from fmax. We apply this method with a probabilityv and sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Other JPEG We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. We then perform a JPEG compression of each channel by randomly sampling a JPEG quality z ∼ U(v1, v2). We sweep across combinations of v1 ∈ {10, 25, 50, 75} and v2 ∈ {25, 50, 75, 100}, with v2 > v1. mixup We do not map features from R to [0, 1], but stay in R. We mix two features f∗ i , f∗ j according to z ·f∗ i + (1−z) ·f∗ j by sampling a random value z ∼ B(α, α), with Beta distribution B(α, α) parameterized by α = v. The labels are mixed using the same procedure. We sweep across v ∈ {0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Table 6. Details on our used set of augmentations. For simplicity, instead of introducing a new hyper parameter for each data augmenta- tion, we re-use v as a sweep parameter that is set during a sweep and differs for each augmentation. If not stated otherwise, each method is only applied as default FroFA and we first map featuresf (two-dimensional representation) or f∗ (three-dimensional representation) from value range R to I = [0, 1] using (5). By default, we assume a three-dimensional representation f∗ although some augmentations would work also in the two-dimensional representation f, i.e., a reshaping is not necessary. lar to the pretraining setup, we also store the internal state in half-precision. We do not apply any weight decay. The learning rate is adapted following a cosine schedule with a linear warm-up phase of 500 steps. In addition, we stabilize 14FroFA Shots Base learning rate Batch size Training steps v or v1, v2 B-c2 1 0.01 512 4,000 1.0 10 0.01 64 16,000 1.0 15 0.01 256 8,000 0.9 25 0.01 512 8,000 0.8 C 1 0.01 32 16,000 6.0 10 0.01 128 8,000 6.0 15 0.01 512 2,000 6.0 25 0.01 256 4,000 7.0 P-c 1 0.01 512 8,000 1, 8 10 0.03 512 8,000 1, 8 15 0.03 512 16,000 1, 8 25 0.03 64 16,000 2, 8 Table 7. Our best sweep settings for our best three FroFAs , namely, brightness cFroFA (B-c 2), contrast (C), and posterize cFroFA (P-c). We list the shots, base learning rate, batch size, number of training steps, and the augmentation parameter, denoted as v or v1, v2 (see Tab. 6 for a detailed explanation ofv and v1, v2). The best sweep settings are found using our ILSVRC-2012 vali- dation set. RA∗ TA∗ Shots MAP B-c 2 Atop2 Atop3 Atop2 Atop3 1 58.4 +6.0 +3.9 +2.4 +4.8 +4.3 5 79.1 +1.5 +1.0 +0.4 +1.4 +1.2 10 80.7 +1.3 +1.0 +0.6 +1.4 +1.4 25 83.0 +0.6 +0.4 +0.0 +0.5 +0.4 Table 8. Top-1 accuracy for advanced FroFA protocols on our ILSVRC-2012 test set. Absolute gains to the MAP baseline (ref- erence run) are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We compare brightness c 2FroFA (B-c2) with our variations of RandAugment (RA∗) and TrivialAugment (TA∗), cf . Sec. A3.2. For the latter, we either use the top-2 ( Atop2) or top- 3 ( Atop3) augmentations. We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 val- idation set. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). training by using a global norm clipping of 1.0. Further, we sweep across batch size, learning rate and number of steps yielding 100 combinations (cf . Sec. 4.4) for each shot. A4. Additional Experimental Results In this section, we show additional experimental results. A4.1. Patch Dropout and Brightness In Fig. 3, we only report results for 1-shot and 25- shot settings using patch dropout FroFA and brightness (c/c2)FroFA. We extend this by also reporting results for 5-shot and 10-shot settings in Figs. 5 and 6. We observe the same effects in the other settings as well. A4.2. Advanced FroFA Protocols In Tab. 8, we report results for our RandAugment (RA ∗) and TrivialAugment (TA∗) variations. We did not average across five runs and thus only report absolute gains with re- spect to a reference run. Therefore, numbers which are also reported in the main paper, e.g., Tab. 4, are slightly differ- ent. All in all, we observe that both RA ∗ and TA∗ do not improve upon the best single augmentation, i.e., brightness c2FroFA (B-c2). We also observe that increasing the set of augmentations from Atop2 to Atop3 rather worsens the per- formance for both RA∗ and TA∗. A4.3. Detailed FroFA Transfer Results In Tab. 9, we report exact numbers for Fig. 4, i.e., Ti/16, B/16, and L/16 pretrained on either ImageNet-21k or JFT- 3B and subsequently finetuned on few-shotted ILSVRC- 2012 training sets. Numbers for the two baselines, i.e., MAP ( with weight decay) and linear probe, and our best method, i.e., MAP ( with weight decay) combined with brightness c2FroFA (MAP + FroFA), are reported. In addi- tion, we report numbers, where we use MAPwithout weight decay in Tab. 10. As before, we observe that our method performs worse on all 1-shot settings, but is on par or sig- nificantly better than MAP and/or linear probe on most 5- to 25-shot settings. 151 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 72 74 76 78 5-shot 1 50 100 150 number of patches 76 77 78 79 80 81 10-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA Figure 5. Average top-1 accuracy for patch dropout FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each number of patches (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 74 76 78 80 5-shot 0.1 0.3 0.5 0.7 0.9 brightness level 79 80 81 82 10-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot MAP + brightness cFroFA + brightness c2FroFA Figure 6. Top-1 accuracy for channel variants (c/c2) of brightness FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each brightness level (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAPwd 20.5 53.6 59.7 64.9 19.1 46.4 53.6 60.2 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAPwd + FroFA 20.6 54.5 60.1 65.2 19.6 47.2 53.6 60.3 B/16 MAPwd 30.5 71.7 75.3 78.0 51.3 74.8 77.5 79.8 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAPwd + FroFA 30.6 73.3 76.0 78.1 52.5 75.1 77.6 79.5 L/16 MAPwd 38.7 75.9 78.6 80.6 62.0 79.9 81.5 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAPwd + FroFA 39.3 78.0 80.0 81.0 63.7 80.4 82.0 83.6 Table 9. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline, as well as our best FroFA-based approach, i.e., weight-decayed MAP combined with brightness c 2FroFA (MAPwd + FroFA). Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAPwd + FroFA, is on par or significantly better than MAPwd and/or linear probe on most 5- to 25-shot settings. 16ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAP 20.4 53.2 59.5 64.7 17.9 45.5 53.5 60.1 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAP + FroFA 22.1 54.9 60.1 65.0 20.3 47.2 53.6 60.1 B/16 MAP 31.3 70.3 75.1 78.1 48.9 73.4 76.5 79.4 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAP + FroFA 30.6 73.4 76.3 78.3 52.4 75.2 77.8 79.9 L/16 MAP 38.8 74.9 78.5 80.7 57.9 78.8 80.9 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAP + FroFA 39.3 78.0 80.0 81.2 63.9 80.3 82.0 83.6 Table 10. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the MAP and L2-regularized linear probe baseline, as well as our best FroFA-based approach,i.e., MAP combined with brightness c2FroFA (MAP + FroFA). Depending on the setting, we sweep across a base,cf . Sec. 4.4, an L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAP + FroFA, is on par or significantly better than MAP and linear probe on most 5- to 25-shot settings. 17",
      "references": [
        "Learning Multiple Layers of Features from Tiny Images",
        "DeepMind Lab",
        "Better Plain ViT Baselines for ImageNet-1k",
        "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
        "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
        "Remote Sensing Image Scene Classification: Benchmark and State of the Art",
        "Xception: Deep Learning With Depthwise Separable Convolutions",
        "Describing Textures in the Wild",
        "AutoAugment: Learning Augmentation Strategies From Data",
        "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space",
        "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
        "Scenic: A JAX Library for Computer Vision Research and Beyond",
        "Scaling Vision Transformers to 22 Billion Parameters",
        "ImageNet: A Large-Scale Hierarchical Image Database",
        "Dataset Augmentation in Feature Space",
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
        "Improving Neural Language Models with a Continuous Cache",
        "Parameter-Efficient Model Adaptation for Vision Transformers",
        "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
        "Distilling Knowledge in a Neural Network",
        "Parameter-Efficient Transfer Learning for NLP",
        "LoRA: Low-Rank Adaptation of Large Language Models",
        "Visual Prompt Tuning",
        "Adam: A Method for Stochastic Optimization",
        "Big Transfer (BiT): General Visual Representation Learning",
        "Three Towers: Flexible Contrastive Learning with Pretrained Image Models",
        "A Closer Look At Feature Space Data Augmentation For Few-Shot Intent Classification",
        "The Omniglot Challenge: a 3-year Progress Report",
        "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
        "Vision Transformer for Small-size Datasets",
        "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "Data Augmentation via Latent Space Interpolation for Image Classification",
        "Efficient Training of Visual Transformers With Small Datasets",
        "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
        "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows",
        "Learning Multimodal Data Augmentation in Feature Space",
        "Decoupled Weight Decay Regularization",
        "TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation",
        "Reading Digits in Natural Images with Unsupervised Feature Learning",
        "Dinov2: Learning Robust Visual Features Without Supervision",
        "A Simple Cache Model for Image Recognition",
        "Learning Transferable Visual Models From Natural Language Supervision",
        "ImageNet Large Scale Visual Recognition Challenge",
        "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
        "How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers",
        "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
        "Rethinking the Inception Architecture for Computer Vision",
        "Training Data-Efficient Image Transformers & Distillation Through Attention",
        "Manifold Mixup: Better Representations by Interpolating Hidden States",
        "Matching Networks for One Shot Learning",
        "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
        "SUN Database: Large-Scale Scene Recognition from Abbey to Zoo",
        "SUN Database: Exploring a Large Collection of Scene Categories",
        "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
        "Scaling Vision Transformers",
        "Sigmoid Loss for Language Image Pre-Training",
        "Mixup: Beyond Empirical Risk Minimization",
        "Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification"
      ],
      "meta_data": {
        "arxiv_id": "2403.10519v2",
        "authors": [
          "Andreas Bär",
          "Neil Houlsby",
          "Mostafa Dehghani",
          "Manoj Kumar"
        ],
        "published_date": "2024-03-15T17:59:40Z",
        "github_url": "https://github.com/google-research/big_vision"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Frozen Feature Augmentation (FroFA) as a novel strategy for improving few-shot image classification. By applying data augmentations directly to frozen features extracted from pretrained vision transformers, the authors demonstrate consistent performance gains across various models, pretraining datasets, and downstream few-shot tasks.",
        "methodology": "The approach involves caching intermediate feature representations from pretrained vision transformers and then applying classic image augmentation techniques (e.g., brightness, contrast, posterize) in the feature space. Three variants of FroFA are proposed: the default FroFA, channel FroFA (cFroFA), and channel2 FroFA (c2FroFA), which introduce per-channel stochasticity. A lightweight Multi-head Attention Pooling (MAP) head is then trained on these augmented features using weight decay and early stopping based on validation data.",
        "experimental_setup": "The experiments are conducted on models pretrained on large-scale datasets such as JFT-3B, ImageNet-21k, and WebLI. Few-shot learning is evaluated on multiple benchmarks including ILSVRC-2012 (ImageNet-1k), CIFAR10, CIFAR100, DMLab, DTD, Resisc45, SUN397, and SVHN with varying shot settings (1-, 5-, 10-, and 25-shot). The empirical evaluation compares the proposed method against baselines like a weight-decayed MAP head and an L2-regularized linear probe. Hyperparameter sweeps (e.g., augmentation level, learning rate, batch size) and validation on separate minival/test splits are used to ascertain the best settings.",
        "limitations": "The study observes that geometric augmentations adversely affect performance, and the improvements are less pronounced in the extremely low-data (e.g., 1-shot) regime. The approach is confined to the frozen feature setting, which might limit its applicability when full model fine-tuning is desired. Additionally, sequential or combined augmentations such as those inspired by RandAugment did not consistently outperform the best performing single augmentation, indicating challenges in augmentation scheduling and selection.",
        "future_research_directions": "Future work could explore extending FroFA to full fine-tuning or other deep architectures, refining the selection and combination of augmentations for improved robustness, and applying the method to other domains or modalities. Furthermore, investigating more adaptive or automated augmentation strategies and understanding the underlying reasons for the effectiveness of particular augmentations in feature space remain promising areas of research.",
        "experimental_code": "# File: big_vision/configs/common_fewshot.py\n\"\"\"Most common few-shot eval configuration.\"\"\"\nimport ml_collections as mlc\n\ndef get_fewshot_lsr(target_resolution=224, resize_resolution=256, runlocal=False, pp=None, **kw):\n    \"\"\"Returns a standard-ish fewshot eval configuration.\"\"\"\n    kw.setdefault('representation_layer', 'pre_logits')\n    kw.setdefault('shots', (1, 5, 10, 25))\n    kw.setdefault('l2_reg', 2.0 ** 10)\n    kw.setdefault('num_seeds', 3)\n    kw.setdefault('prefix', '')\n    if not any((f'log_{x}' in kw for x in ['steps', 'percent', 'examples', 'epochs'])):\n        kw['log_steps'] = 25000\n    config = mlc.ConfigDict(kw)\n    config.type = 'fewshot_lsr'\n    config.datasets = {'caltech': ('caltech101', 'train', 'test'), 'cars': ('cars196:2.1.0', 'train', 'test'), 'cifar100': ('cifar100', 'train', 'test'), 'dtd': ('dtd', 'train', 'test'), 'pets': ('oxford_iiit_pet', 'train', 'test'), 'uc_merced': ('uc_merced', 'train[:1000]', 'train[1000:]')} if not runlocal else {'pets': ('oxford_iiit_pet', 'train', 'test')}\n    pp = pp or '|'.join(['decode', f'resize({resize_resolution})', f'central_crop({target_resolution})', 'value_range(-1,1)'])\n    pp += '|keep(\"image\", \"label\")'\n    config.pp_train = pp\n    config.pp_eval = pp\n    config.display_first = [('imagenet', 10)] if not runlocal else [('pets', 10)]\n    return config\n\n# File: big_vision/configs/load_and_eval.py\n\"\"\"A config to load and eval key model using the core train.py.\n\nThe runtime varies widely depending on the model, but each one should reproduce\nthe corresponding paper's numbers.\nThis configuration makes use of the \"arg\" to get_config to select which model\nto run, so a few examples are given below:\n\nRun and evaluate a BiT-M ResNet-50x1 model that was transferred to i1k:\n\nbig_vision.train \\\\\n    --config big_vision/configs/load_and_eval.py:name=bit_paper,batch_size=8 \\\\\n    --config.model_init M-imagenet2012 --config.model.width 1 --config.model.depth 50\n\nRun and evaluate the recommended ViT-B/32 from \"how to train your vit\" paper:\n\nbig_vision.train \\\\\n    --config big_vision/configs/load_and_eval.py:name=vit_i21k,batch_size=8 \\\\\n    --config.model.variant B/32 --config.model_init howto-i21k-B/32\n\"\"\"\nimport big_vision.configs.common as bvcc\nfrom big_vision.configs.common_fewshot import get_fewshot_lsr\n\ndef eval_only(config, batch_size, spec_for_init):\n    \"\"\"Set a few configs that turn trainer into (almost) eval-only.\"\"\"\n    config.total_steps = 0\n    config.input = {}\n    config.input.batch_size = batch_size\n    config.input.data = dict(name='bv:dummy', spec=spec_for_init)\n    config.optax_name = 'identity'\n    config.lr = 0.0\n    config.mesh = [('data', -1)]\n    config.sharding_strategy = [('params/.*', 'fsdp(axis=\"data\")')]\n    config.sharding_rules = [('act_batch', ('data',))]\n    return config\n\ndef get_config(arg=''):\n    config = bvcc.parse_arg(arg, name='bit_paper', batch_size=4)\n    eval_only(config, config.batch_size, spec_for_init=dict(image=dict(shape=(224, 224, 3), dtype='float32')))\n    config.evals = dict(fewshot=get_fewshot_lsr())\n    globals()[config.name](config)\n    return config\n\ndef bit_paper(config):\n    config.num_classes = 1000\n    config.model_name = 'bit_paper'\n    config.model_init = 'M-imagenet2012'\n    config.model = dict(width=1, depth=50)\n\n    def get_eval(split, lbl, dataset='imagenet2012_real'):\n        return dict(type='classification', data=dict(name=dataset, split=split), loss_name='softmax_xent', cache='none', pp_fn=f'decode|resize(384)|value_range(-1, 1)|onehot(1000, key=\"{lbl}\", key_result=\"labels\")|keep(\"image\", \"labels\")')\n    config.evals.test = get_eval('validation', 'original_label')\n    config.evals.real = get_eval('validation', 'real_label')\n    config.evals.v2 = get_eval('test', 'label', 'imagenet_v2')\n\ndef vit_i1k(config):\n    config.num_classes = 1000\n    config.model_name = 'vit'\n    config.model_init = ''\n    config.model = dict(variant='S/16', pool_type='gap', posemb='sincos2d', rep_size=True)\n    config.evals.val = dict(type='classification', data=dict(name='imagenet2012', split='validation'), pp_fn='decode|resize_small(256)|central_crop(224)|value_range(-1, 1)|onehot(1000, key=\"label\", key_result=\"labels\")|keep(\"image\", \"labels\")', loss_name='softmax_xent', cache='none')\n\ndef mlp_mixer_i1k(config):\n    config.num_classes = 1000\n    config.model_name = 'mlp_mixer'\n    config.model_init = ''\n    config.model = dict(variant='L/16')\n    config.evals.val = dict(type='classification', data=dict(name='imagenet2012', split='validation'), pp_fn='decode|resize_small(256)|central_crop(224)|value_range(-1, 1)|onehot(1000, key=\"label\", key_result=\"labels\")|keep(\"image\", \"labels\")', loss_name='softmax_xent', cache='none')\n\ndef vit_i21k(config):\n    config.num_classes = 21843\n    config.model_name = 'vit'\n    config.model_init = ''\n    config.model = dict(variant='B/32', pool_type='tok')\n    config.evals.val = dict(type='classification', data=dict(name='imagenet21k', split='full[:51200]'), pp_fn='decode|resize_small(256)|central_crop(224)|value_range(-1, 1)|onehot(21843)|keep(\"image\", \"labels\")', loss_name='sigmoid_xent', cache='none')\n\n# File: big_vision/pp/autoaugment.py\n\"\"\"AutoAugment and RandAugment policies for enhanced image preprocessing.\n\nAutoAugment Reference: https://arxiv.org/abs/1805.09501\nRandAugment Reference: https://arxiv.org/abs/1909.13719\n\nThis code is forked from\nhttps://github.com/tensorflow/tpu/blob/11d0db15cf1c3667f6e36fecffa111399e008acd/models/official/efficientnet/autoaugment.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport dataclasses\nimport inspect\nimport math\nimport tensorflow.compat.v1 as tf\nfrom tensorflow_addons import image as contrib_image\n_MAX_LEVEL = 10.0\n\n@dataclasses.dataclass\nclass HParams:\n    \"\"\"Parameters for AutoAugment and RandAugment.\"\"\"\n    cutout_const: int\n    translate_const: int\n\ndef policy_v0():\n    \"\"\"Autoaugment policy that was used in AutoAugment Paper.\"\"\"\n    policy = [[('Equalize', 0.8, 1), ('ShearY', 0.8, 4)], [('Color', 0.4, 9), ('Equalize', 0.6, 3)], [('Color', 0.4, 1), ('Rotate', 0.6, 8)], [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)], [('Solarize', 0.4, 2), ('Solarize', 0.6, 2)], [('Color', 0.2, 0), ('Equalize', 0.8, 8)], [('Equalize', 0.4, 8), ('SolarizeAdd', 0.8, 3)], [('ShearX', 0.2, 9), ('Rotate', 0.6, 8)], [('Color', 0.6, 1), ('Equalize', 1.0, 2)], [('Invert', 0.4, 9), ('Rotate', 0.6, 0)], [('Equalize', 1.0, 9), ('ShearY', 0.6, 3)], [('Color', 0.4, 7), ('Equalize', 0.6, 0)], [('Posterize', 0.4, 6), ('AutoContrast', 0.4, 7)], [('Solarize', 0.6, 8), ('Color', 0.6, 9)], [('Solarize', 0.2, 4), ('Rotate', 0.8, 9)], [('Rotate', 1.0, 7), ('TranslateY', 0.8, 9)], [('ShearX', 0.0, 0), ('Solarize', 0.8, 4)], [('ShearY', 0.8, 0), ('Color', 0.6, 4)], [('Color', 1.0, 0), ('Rotate', 0.6, 2)], [('Equalize', 0.8, 4), ('Equalize', 0.0, 8)], [('Equalize', 1.0, 4), ('AutoContrast', 0.6, 2)], [('ShearY', 0.4, 7), ('SolarizeAdd', 0.6, 7)], [('Posterize', 0.8, 2), ('Solarize', 0.6, 10)], [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)], [('Color', 0.8, 6), ('Rotate', 0.4, 5)]]\n    return policy\n\ndef policy_vtest():\n    \"\"\"Autoaugment test policy for debugging.\"\"\"\n    policy = [[('TranslateX', 1.0, 4), ('Equalize', 1.0, 10)]]\n    return policy\n\ndef blend(image1, image2, factor):\n    \"\"\"Blend image1 and image2 using 'factor'.\n  Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n  A value of 1.0 means only image2 is used.  A value between 0.0 and\n  1.0 means we linearly interpolate the pixel values between the two\n  images.  A value greater than 1.0 \"extrapolates\" the difference\n  between the two pixel values, and we clip the results to values\n  between 0 and 255.\n  Args:\n    image1: An image Tensor of type uint8.\n    image2: An image Tensor of type uint8.\n    factor: A floating point value above 0.0.\n  Returns:\n    A blended image Tensor of type uint8.\n  \"\"\"\n    if factor == 0.0:\n        return tf.convert_to_tensor(image1)\n    if factor == 1.0:\n        return tf.convert_to_tensor(image2)\n    image1 = tf.to_float(image1)\n    image2 = tf.to_float(image2)\n    difference = image2 - image1\n    scaled = factor * difference\n    temp = tf.to_float(image1) + scaled\n    if factor > 0.0 and factor < 1.0:\n        return tf.cast(temp, tf.uint8)\n    return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)\n\ndef cutout(image, pad_size, replace=0):\n    \"\"\"Apply cutout (https://arxiv.org/abs/1708.04552) to image.\n  This operation applies a (2*pad_size x 2*pad_size) mask of zeros to\n  a random location within `img`. The pixel values filled in will be of the\n  value `replace`. The located where the mask will be applied is randomly\n  chosen uniformly over the whole image.\n  Args:\n    image: An image Tensor of type uint8.\n    pad_size: Specifies how big the zero mask that will be generated is that\n      is applied to the image. The mask will be of size\n      (2*pad_size x 2*pad_size).\n    replace: What pixel value to fill in the image in the area that has\n      the cutout mask applied to it.\n  Returns:\n    An image Tensor that is of type uint8.\n  \"\"\"\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n    cutout_center_height = tf.random_uniform(shape=[], minval=0, maxval=image_height, dtype=tf.int32)\n    cutout_center_width = tf.random_uniform(shape=[], minval=0, maxval=image_width, dtype=tf.int32)\n    lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n    upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n    left_pad = tf.maximum(0, cutout_center_width - pad_size)\n    right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n    cutout_shape = [image_height - (lower_pad + upper_pad), image_width - (left_pad + right_pad)]\n    padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n    mask = tf.pad(tf.zeros(cutout_shape, dtype=image.dtype), padding_dims, constant_values=1)\n    mask = tf.expand_dims(mask, -1)\n    mask = tf.tile(mask, [1, 1, 3])\n    image = tf.where(tf.equal(mask, 0), tf.ones_like(image, dtype=image.dtype) * replace, image)\n    return image\n\ndef solarize(image, threshold=128):\n    return tf.where(image < threshold, image, 255 - image)\n\ndef solarize_add(image, addition=0, threshold=128):\n    added_image = tf.cast(image, tf.int64) + addition\n    added_image = tf.cast(tf.clip_by_value(added_image, 0, 255), tf.uint8)\n    return tf.where(image < threshold, added_image, image)\n\ndef color(image, factor):\n    \"\"\"Equivalent of PIL Color.\"\"\"\n    degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n    return blend(degenerate, image, factor)\n\ndef contrast(image, factor):\n    \"\"\"Equivalent of PIL Contrast.\"\"\"\n    degenerate = tf.image.rgb_to_grayscale(image)\n    degenerate = tf.cast(degenerate, tf.int32)\n    hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n    mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.0\n    degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n    degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n    degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n    return blend(degenerate, image, factor)\n\ndef brightness(image, factor):\n    \"\"\"Equivalent of PIL Brightness.\"\"\"\n    degenerate = tf.zeros_like(image)\n    return blend(degenerate, image, factor)\n\ndef posterize(image, bits):\n    \"\"\"Equivalent of PIL Posterize.\"\"\"\n    shift = 8 - bits\n    return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\n\ndef rotate(image, degrees, replace):\n    \"\"\"Rotates the image by degrees either clockwise or counterclockwise.\n  Args:\n    image: An image Tensor of type uint8.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n    replace: A one or three value 1D tensor to fill empty pixels caused by\n      the rotate operation.\n  Returns:\n    The rotated version of image.\n  \"\"\"\n    degrees_to_radians = math.pi / 180.0\n    radians = degrees * degrees_to_radians\n    image = contrib_image.rotate(wrap(image), radians)\n    return unwrap(image, replace)\n\ndef translate_x(image, pixels, replace):\n    \"\"\"Equivalent of PIL Translate in X dimension.\"\"\"\n    image = contrib_image.translate(wrap(image), [-pixels, 0])\n    return unwrap(image, replace)\n\ndef translate_y(image, pixels, replace):\n    \"\"\"Equivalent of PIL Translate in Y dimension.\"\"\"\n    image = contrib_image.translate(wrap(image), [0, -pixels])\n    return unwrap(image, replace)\n\ndef shear_x(image, level, replace):\n    \"\"\"Equivalent of PIL Shearing in X dimension.\"\"\"\n    image = contrib_image.transform(wrap(image), [1.0, level, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n    return unwrap(image, replace)\n\ndef shear_y(image, level, replace):\n    \"\"\"Equivalent of PIL Shearing in Y dimension.\"\"\"\n    image = contrib_image.transform(wrap(image), [1.0, 0.0, 0.0, level, 1.0, 0.0, 0.0, 0.0])\n    return unwrap(image, replace)\n\ndef autocontrast(image):\n    \"\"\"Implements Autocontrast function from PIL using TF ops.\n  Args:\n    image: A 3D uint8 tensor.\n  Returns:\n    The image after it has had autocontrast applied to it and will be of type\n    uint8.\n  \"\"\"\n\n    def scale_channel(image):\n        \"\"\"Scale the 2D image using the autocontrast rule.\"\"\"\n        lo = tf.to_float(tf.reduce_min(image))\n        hi = tf.to_float(tf.reduce_max(image))\n\n        def scale_values(im):\n            scale = 255.0 / (hi - lo)\n            offset = -lo * scale\n            im = tf.to_float(im) * scale + offset\n            im = tf.clip_by_value(im, 0.0, 255.0)\n            return tf.cast(im, tf.uint8)\n        result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n        return result\n    s1 = scale_channel(image[:, :, 0])\n    s2 = scale_channel(image[:, :, 1])\n    s3 = scale_channel(image[:, :, 2])\n    image = tf.stack([s1, s2, s3], 2)\n    return image\n\ndef sharpness(image, factor):\n    \"\"\"Implements Sharpness function from PIL using TF ops.\"\"\"\n    orig_image = image\n    image = tf.cast(image, tf.float32)\n    image = tf.expand_dims(image, 0)\n    kernel = tf.constant([[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32, shape=[3, 3, 1, 1]) / 13.0\n    kernel = tf.tile(kernel, [1, 1, 3, 1])\n    strides = [1, 1, 1, 1]\n    with tf.device('/cpu:0'):\n        degenerate = tf.nn.depthwise_conv2d(image, kernel, strides, padding='VALID', rate=[1, 1])\n    degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n    degenerate = tf.squeeze(tf.cast(degenerate, tf.uint8), [0])\n    mask = tf.ones_like(degenerate)\n    padded_mask = tf.pad(mask, [[1, 1], [1, 1], [0, 0]])\n    padded_degenerate = tf.pad(degenerate, [[1, 1], [1, 1], [0, 0]])\n    result = tf.where(tf.equal(padded_mask, 1), padded_degenerate, orig_image)\n    return blend(result, orig_image, factor)\n\ndef equalize(image):\n    \"\"\"Implements Equalize function from PIL using TF ops.\"\"\"\n\n    def scale_channel(im, c):\n        \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n        im = tf.cast(im[:, :, c], tf.int32)\n        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n        nonzero = tf.where(tf.not_equal(histo, 0))\n        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n\n        def build_lut(histo, step):\n            lut = (tf.cumsum(histo) + step // 2) // step\n            lut = tf.concat([[0], lut[:-1]], 0)\n            return tf.clip_by_value(lut, 0, 255)\n        result = tf.cond(tf.equal(step, 0), lambda: im, lambda: tf.gather(build_lut(histo, step), im))\n        return tf.cast(result, tf.uint8)\n    s1 = scale_channel(image, 0)\n    s2 = scale_channel(image, 1)\n    s3 = scale_channel(image, 2)\n    image = tf.stack([s1, s2, s3], 2)\n    return image\n\ndef invert(image):\n    \"\"\"Inverts the image pixels.\"\"\"\n    image = tf.convert_to_tensor(image)\n    return 255 - image\n\ndef wrap(image):\n    \"\"\"Returns 'image' with an extra channel set to all 1s.\"\"\"\n    shape = tf.shape(image)\n    extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n    extended = tf.concat([image, extended_channel], 2)\n    return extended\n\ndef unwrap(image, replace):\n    \"\"\"Unwraps an image produced by wrap.\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed\n  (set to 128).  Operations like translate and shear on a wrapped\n  Tensor will leave 0s in empty locations.  Some transformations look\n  at the intensity of values to do preprocessing, and we want these\n  empty pixels to assume the 'average' value, rather than pure black.\n  Args:\n    image: A 3D Image Tensor with 4 channels.\n    replace: A one or three value 1D tensor to fill empty pixels.\n  Returns:\n    image: A 3D image Tensor with 3 channels.\n  \"\"\"\n    image_shape = tf.shape(image)\n    flattened_image = tf.reshape(image, [-1, image_shape[2]])\n    alpha_channel = flattened_image[:, 3]\n    replace = tf.concat([replace, tf.ones([1], image.dtype)], 0)\n    flattened_image = tf.where(tf.equal(alpha_channel, 0), tf.ones_like(flattened_image, dtype=image.dtype) * replace, flattened_image)\n    image = tf.reshape(flattened_image, image_shape)\n    image = tf.slice(image, [0, 0, 0], [image_shape[0], image_shape[1], 3])\n    return image\nNAME_TO_FUNC = {'AutoContrast': autocontrast, 'Equalize': equalize, 'Invert': invert, 'Rotate': rotate, 'Posterize': posterize, 'Solarize': solarize, 'SolarizeAdd': solarize_add, 'Color': color, 'Contrast': contrast, 'Brightness': brightness, 'Sharpness': sharpness, 'ShearX': shear_x, 'ShearY': shear_y, 'TranslateX': translate_x, 'TranslateY': translate_y, 'Cutout': cutout}\n\ndef _randomly_negate_tensor(tensor):\n    \"\"\"With 50% prob turn the tensor negative.\"\"\"\n    should_flip = tf.cast(tf.floor(tf.random_uniform([]) + 0.5), tf.bool)\n    final_tensor = tf.cond(should_flip, lambda: tensor, lambda: -tensor)\n    return final_tensor\n\ndef _rotate_level_to_arg(level):\n    level = level / _MAX_LEVEL * 30.0\n    level = _randomly_negate_tensor(level)\n    return (level,)\n\ndef _shrink_level_to_arg(level):\n    \"\"\"Converts level to ratio by which we shrink the image content.\"\"\"\n    if level == 0:\n        return (1.0,)\n    level = 2.0 / (_MAX_LEVEL / level) + 0.9\n    return (level,)\n\ndef _enhance_level_to_arg(level):\n    return (level / _MAX_LEVEL * 1.8 + 0.1,)\n\ndef _shear_level_to_arg(level):\n    level = level / _MAX_LEVEL * 0.3\n    level = _randomly_negate_tensor(level)\n    return (level,)\n\ndef _translate_level_to_arg(level, translate_const):\n    level = level / _MAX_LEVEL * float(translate_const)\n    level = _randomly_negate_tensor(level)\n    return (level,)\n\ndef level_to_arg(hparams):\n    return {'AutoContrast': lambda level: (), 'Equalize': lambda level: (), 'Invert': lambda level: (), 'Rotate': _rotate_level_to_arg, 'Posterize': lambda level: (int(level / _MAX_LEVEL * 4),), 'Solarize': lambda level: (int(level / _MAX_LEVEL * 256),), 'SolarizeAdd': lambda level: (int(level / _MAX_LEVEL * 110),), 'Color': _enhance_level_to_arg, 'Contrast': _enhance_level_to_arg, 'Brightness': _enhance_level_to_arg, 'Sharpness': _enhance_level_to_arg, 'ShearX': _shear_level_to_arg, 'ShearY': _shear_level_to_arg, 'Cutout': lambda level: (int(level / _MAX_LEVEL * hparams.cutout_const),), 'TranslateX': lambda level: _translate_level_to_arg(level, hparams.translate_const), 'TranslateY': lambda level: _translate_level_to_arg(level, hparams.translate_const)}\n\ndef _parse_policy_info(name, prob, level, replace_value, augmentation_hparams):\n    \"\"\"Return the function that corresponds to `name` and update `level` param.\"\"\"\n    func = NAME_TO_FUNC[name]\n    args = level_to_arg(augmentation_hparams)[name](level)\n    if 'prob' in inspect.getfullargspec(func).args:\n        args = tuple([prob] + list(args))\n    if 'replace' in inspect.getfullargspec(func).args:\n        assert 'replace' == inspect.getfullargspec(func).args[-1]\n        args = tuple(list(args) + [replace_value])\n    return (func, prob, args)\n\ndef _apply_func_with_prob(func, image, args, prob):\n    \"\"\"Apply `func` to image w/ `args` as input with probability `prob`.\"\"\"\n    assert isinstance(args, tuple)\n    if 'prob' in inspect.getfullargspec(func).args:\n        prob = 1.0\n    should_apply_op = tf.cast(tf.floor(tf.random_uniform([], dtype=tf.float32) + prob), tf.bool)\n    augmented_image = tf.cond(should_apply_op, lambda: func(image, *args), lambda: image)\n    return augmented_image\n\ndef select_and_apply_random_policy(policies, image):\n    \"\"\"Select a random policy from `policies` and apply it to `image`.\"\"\"\n    policy_to_select = tf.random_uniform([], maxval=len(policies), dtype=tf.int32)\n    for i, policy in enumerate(policies):\n        image = tf.cond(tf.equal(i, policy_to_select), lambda selected_policy=policy: selected_policy(image), lambda: image)\n    return image\n\ndef build_and_apply_nas_policy(policies, image, augmentation_hparams):\n    \"\"\"Build a policy from the given policies passed in and apply to image.\n  Args:\n    policies: list of lists of tuples in the form `(func, prob, level)`, `func`\n      is a string name of the augmentation function, `prob` is the probability\n      of applying the `func` operation, `level` is the input argument for\n      `func`.\n    image: tf.Tensor that the resulting policy will be applied to.\n    augmentation_hparams: Hparams associated with the NAS learned policy.\n  Returns:\n    A version of image that now has data augmentation applied to it based on\n    the `policies` pass into the function.\n  \"\"\"\n    replace_value = [128, 128, 128]\n    tf_policies = []\n    for policy in policies:\n        tf_policy = []\n        for policy_info in policy:\n            policy_info = list(policy_info) + [replace_value, augmentation_hparams]\n            tf_policy.append(_parse_policy_info(*policy_info))\n\n        def make_final_policy(tf_policy_):\n\n            def final_policy(image_):\n                for func, prob, args in tf_policy_:\n                    image_ = _apply_func_with_prob(func, image_, args, prob)\n                return image_\n            return final_policy\n        tf_policies.append(make_final_policy(tf_policy))\n    augmented_image = select_and_apply_random_policy(tf_policies, image)\n    return augmented_image\n\ndef distort_image_with_autoaugment(image, augmentation_name):\n    \"\"\"Applies the AutoAugment policy to `image`.\n  AutoAugment is from the paper: https://arxiv.org/abs/1805.09501.\n  Args:\n    image: `Tensor` of shape [height, width, 3] representing an image.\n    augmentation_name: The name of the AutoAugment policy to use. The available\n      options are `v0` and `test`. `v0` is the policy used for\n      all of the results in the paper and was found to achieve the best results\n      on the COCO dataset. `v1`, `v2` and `v3` are additional good policies\n      found on the COCO dataset that have slight variation in what operations\n      were used during the search procedure along with how many operations are\n      applied in parallel to a single image (2 vs 3).\n  Returns:\n    A tuple containing the augmented versions of `image`.\n  \"\"\"\n    available_policies = {'v0': policy_v0, 'test': policy_vtest}\n    if augmentation_name not in available_policies:\n        raise ValueError('Invalid augmentation_name: {}'.format(augmentation_name))\n    policy = available_policies[augmentation_name]()\n    augmentation_hparams = HParams(cutout_const=100, translate_const=250)\n    return build_and_apply_nas_policy(policy, image, augmentation_hparams)\n\ndef distort_image_with_randaugment(image, num_layers, magnitude):\n    \"\"\"Applies the RandAugment policy to `image`.\n  RandAugment is from the paper https://arxiv.org/abs/1909.13719,\n  Args:\n    image: `Tensor` of shape [height, width, 3] representing an image.\n    num_layers: Integer, the number of augmentation transformations to apply\n      sequentially to an image. Represented as (N) in the paper. Usually best\n      values will be in the range [1, 3].\n    magnitude: Integer, shared magnitude across all augmentation operations.\n      Represented as (M) in the paper. Usually best values are in the range\n      [5, 30].\n  Returns:\n    The augmented version of `image`.\n  \"\"\"\n    replace_value = [128] * 3\n    tf.logging.info('Using RandAug.')\n    augmentation_hparams = HParams(cutout_const=40, translate_const=100)\n    available_ops = ['AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize', 'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness', 'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Cutout', 'SolarizeAdd']\n    for layer_num in range(num_layers):\n        op_to_select = tf.random_uniform([], maxval=len(available_ops), dtype=tf.int32)\n        random_magnitude = float(magnitude)\n        with tf.name_scope('randaug_layer_{}'.format(layer_num)):\n            for i, op_name in enumerate(available_ops):\n                prob = tf.random_uniform([], minval=0.2, maxval=0.8, dtype=tf.float32)\n                func, _, args = _parse_policy_info(op_name, prob, random_magnitude, replace_value, augmentation_hparams)\n                image = tf.cond(tf.equal(i, op_to_select), lambda selected_func=func, selected_args=args: selected_func(image, *selected_args), lambda: image)\n    return image\n\n# File: big_vision/models/proj/image_text/naflex_vit.py\n\"\"\"NaFlex ViT = NaViT + FlexiViT.\n\nBased on:\n* FlexiViT: https://arxiv.org/abs/2212.08013\n* NaViT: https://arxiv.org/abs/2307.06304\n\"\"\"\nimport re\nfrom big_vision.models import vit\nimport big_vision.models.proj.image_text.utils as it_utils\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef _decode_posemb(posemb):\n    if (m := re.fullmatch('learn_2d(\\\\(\\\\d+\\\\))', posemb)):\n        grid_size = int(m.groups()[0][1:-1])\n        return ('learn_2d', grid_size)\n    return (posemb, None)\n\ndef _pos_emb_resize(pos_emb, shapes, coords, l):\n    \"\"\"Resizes the positional embeddings to match the input image size.\n  \n  Args:\n    pos_emb: Positional embeddings.\n    shapes: Image shapes (usually `coords.max(axis=1) + 1`).\n    coords: Patch coordinates.\n    l: Maximum number of patches per side. Necesary in order to have a static\n      return shape.\n\n  Setting l to 64 is a heuristic. Ideally, we would use\n  `l = tokens.shape[1]` here, but that requires too much memory,\n  especially for high-resolution inputs. Using a lower value\n  effectively limits the maximum resolution to `l x patch_size`.\n  Resolutions above that will lead to NaNs in the positional\n  embeddings and NaN model outputs.\n  Note: this value can be adjusted post-hoc without retraining.\n\n  Returns:\n    Postional embeddings for every patch.\n  \"\"\"\n\n    def resize_fn(shape, coords):\n        emb = jax.image.scale_and_translate(pos_emb, shape=(l, l, pos_emb.shape[-1]), spatial_dims=(0, 1), scale=shape / jnp.asarray(pos_emb.shape[:2]), translation=jnp.asarray([0, 0]), method='bilinear', antialias=True)\n        gather_dim = jax.lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0, 1), start_index_map=(0, 1, 2))\n        return jax.lax.gather(emb, jnp.pad(coords, [[0, 0], [0, 1]]), gather_dim, [1, 1, emb.shape[-1]], mode='fill')\n    return it_utils.batch_shmap(jax.vmap(resize_fn, in_axes=(0, 0), out_axes=0), shapes, coords)\n\nclass Encoder1DBlock(nn.Module):\n    \"\"\"Single transformer encoder block (MHSA + MLP).\"\"\"\n    mlp_dim: int | None = None\n    num_heads: int = 12\n    dropout: float = 0.0\n    dtype_mm: str = 'float32'\n\n    @nn.compact\n    def __call__(self, x, mask=None, deterministic=True):\n        if mask is not None:\n            mask = mask[..., None, :, :]\n        out = {}\n        x = nn.with_logical_constraint(x, ('act_batch', 'act_len', 'act_emb'))\n        y = nn.LayerNorm()(x)\n        y = out['sa'] = nn.MultiHeadDotProductAttention(num_heads=self.num_heads, kernel_init=nn.initializers.xavier_uniform(), deterministic=deterministic, dtype=self.dtype_mm)(y, y, mask=mask)\n        y = nn.with_logical_constraint(y, ('act_batch', 'act_len', 'act_emb'))\n        y = nn.Dropout(rate=self.dropout)(y, deterministic)\n        x = out['+sa'] = x + y\n        y = nn.LayerNorm()(x)\n        y = out['mlp'] = vit.MlpBlock(mlp_dim=self.mlp_dim, dropout=self.dropout, dtype_mm=self.dtype_mm)(y, deterministic)\n        y = nn.with_logical_constraint(y, ('act_batch', 'act_len', 'act_emb'))\n        y = nn.Dropout(rate=self.dropout)(y, deterministic)\n        x = out['+mlp'] = x + y\n        x = nn.with_logical_constraint(x, ('act_batch', 'act_len', 'act_emb'))\n        return (x, out)\n\nclass Encoder(nn.Module):\n    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n    depth: int\n    mlp_dim: int | None = None\n    num_heads: int = 12\n    dropout: float = 0.0\n    scan: bool = False\n    remat_policy: str = 'nothing_saveable'\n    dtype_mm: str = 'float32'\n\n    @nn.compact\n    def __call__(self, x, mask=None, deterministic=True):\n        out = {}\n        if self.scan:\n            block = nn.remat(Encoder1DBlock, prevent_cse=False, static_argnums=(3,), policy=getattr(jax.checkpoint_policies, self.remat_policy, None))\n            x, scan_out = nn.scan(block, variable_axes={'params': 0}, split_rngs={'params': True, 'dropout': True}, in_axes=nn.broadcast, length=self.depth)(name='encoderblock', dtype_mm=self.dtype_mm, mlp_dim=self.mlp_dim, num_heads=self.num_heads, dropout=self.dropout)(x, mask, deterministic)\n            for lyr in range(self.depth):\n                out[f'block{lyr:02d}'] = jax.tree.map(lambda o, l=lyr: o[l], scan_out)\n        else:\n            for lyr in range(self.depth):\n                block_cur = Encoder1DBlock(name=f'encoderblock_{lyr}', dtype_mm=self.dtype_mm, mlp_dim=self.mlp_dim, num_heads=self.num_heads, dropout=self.dropout)\n                x, out[f'block{lyr:02d}'] = block_cur(x, mask, deterministic)\n            out['pre_ln'] = x\n        return (nn.LayerNorm(name='encoder_norm')(x), out)\n\nclass MAPHead(nn.Module):\n    \"\"\"Multihead Attention Pooling.\"\"\"\n    mlp_dim: int | None = None\n    num_heads: int = 12\n\n    @nn.compact\n    def __call__(self, x, mask=None):\n        n, l, d = x.shape\n        probe = self.param('probe', nn.initializers.xavier_uniform(), (1, 1, d), x.dtype)\n        probe = jnp.tile(probe, [n, 1, 1])\n        if mask is not None:\n            mask = mask[..., None, None, :]\n        x = nn.MultiHeadDotProductAttention(num_heads=self.num_heads, kernel_init=nn.initializers.xavier_uniform())(probe, x, mask=mask)\n        y = nn.LayerNorm()(x)\n        x = x + vit.MlpBlock(mlp_dim=self.mlp_dim)(y)\n        return x[:, 0]\n\nclass _Model(nn.Module):\n    \"\"\"ViT model.\"\"\"\n    num_classes: int | None = None\n    width: int = 768\n    depth: int = 12\n    mlp_dim: int | None = None\n    num_heads: int = 12\n    rep_size: int | bool = False\n    pool_type: str = 'gap'\n    head_zeroinit: bool = True\n    scan: bool = False\n    remat_policy: str = 'nothing_saveable'\n    dtype_mm: str = 'float32'\n    posemb: str = 'learn_2d(64)'\n    nposemb: int | None = None\n    patchln_pre: bool = False\n    patchln_post: bool = False\n\n    @nn.compact\n    def __call__(self, image, *, train=False):\n        out = {}\n        patches, ptype, yabs, xabs = image\n        patches = jnp.asarray(patches, self.dtype_mm)\n        if self.patchln_pre:\n            patches = nn.LayerNorm(name='patchln_pre')(patches)\n        tokens = out['stem'] = nn.Dense(self.width, name='embedding', dtype=self.dtype_mm)(patches)\n        if self.patchln_post:\n            tokens = nn.LayerNorm(name='patchln_post')(tokens)\n        x = tokens\n        posemb, posemb_grid_size = _decode_posemb(self.posemb)\n        if posemb == 'learn_2d':\n            posembs = self.param('pos_embedding', nn.initializers.normal(stddev=1 / np.sqrt(self.width)), (self.nposemb, self.nposemb, self.width), self.dtype_mm)\n            coords = jnp.stack([yabs, xabs], axis=-1)\n            shapes = coords.max(axis=1) + 1\n            x += _pos_emb_resize(posembs, shapes, coords, posemb_grid_size or 64)\n        else:\n            raise ValueError(f\"Unknown posemb: '{self.posemb}'\")\n        out['with_posemb'] = x\n        sa_mask = ptype == 1\n        sa_mask = jnp.logical_and(sa_mask[..., :, None], sa_mask[..., None, :])\n        x, out['encoder'] = Encoder(depth=self.depth, mlp_dim=self.mlp_dim, num_heads=self.num_heads, scan=self.scan, remat_policy=self.remat_policy, dtype_mm=self.dtype_mm, name='Transformer')(x, mask=sa_mask, deterministic=not train)\n        out['encoded'] = x\n        pool_mask = ptype == 1\n        if self.pool_type == 'map':\n            maphead = MAPHead(num_heads=self.num_heads, mlp_dim=self.mlp_dim)\n            x = maphead(x, mask=pool_mask)\n        elif self.pool_type == 'gap':\n            pool_mask = pool_mask[..., None]\n            x = jnp.sum(x * pool_mask, axis=1) / jnp.sum(pool_mask, axis=1)\n        elif self.pool_type == 'max':\n            pool_mask = pool_mask[..., None]\n            ignore = jnp.where(pool_mask, 0, jnp.finfo(x.dtype).min)\n            x = jnp.max(pool_mask * x + ignore, axis=1)\n        elif self.pool_type == 'none':\n            pass\n        else:\n            raise ValueError(f\"Unknown pool type: '{self.pool_type}'\")\n        out['head_input'] = x\n        if self.rep_size:\n            rep_size = self.width if self.rep_size is True else self.rep_size\n            hid = nn.Dense(rep_size, name='pre_logits')\n            x = nn.tanh(hid(x))\n        out['pre_logits'] = x\n        if self.num_classes:\n            kw = {'kernel_init': nn.initializers.zeros} if self.head_zeroinit else {}\n            head = nn.Dense(self.num_classes, name='head', **kw)\n            x = out['logits'] = head(x)\n        return (x, out)\n\ndef Model(num_classes=None, *, variant=None, **kw):\n    \"\"\"Factory function, because linen really don't like what I'm doing!\"\"\"\n    return _Model(num_classes, **{**vit.decode_variant(variant), **kw})\nload = vit.load\n\n# File: big_vision/train.py\n\"\"\"Training loop example.\n\nThis is a basic variant of a training loop, good starting point for fancy ones.\n\"\"\"\nimport functools\nimport importlib\nimport multiprocessing.pool\nimport os\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport big_vision.evaluators.common as eval_common\nimport big_vision.input_pipeline as input_pipeline\nimport big_vision.optax as bv_optax\nimport big_vision.sharding as bv_sharding\nimport big_vision.utils as u\nfrom clu import parameter_overview\nimport flax.linen as nn\nimport jax\nfrom jax.experimental import multihost_utils\nfrom jax.experimental.array_serialization import serialization as array_serial\nfrom jax.experimental.shard_map import shard_map\nimport jax.numpy as jnp\nfrom ml_collections import config_flags\nimport numpy as np\nimport optax\nimport tensorflow as tf\nfrom tensorflow.io import gfile\nconfig_flags.DEFINE_config_file('config', None, 'Training configuration.', lock_config=True)\nflags.DEFINE_string('workdir', default=None, help='Work unit directory.')\nflags.DEFINE_boolean('cleanup', default=False, help='Delete workdir (only) after successful completion.')\njax.config.parse_flags_with_absl()\njax.config.update('jax_transfer_guard', 'disallow')\njax.config.update('jax_threefry_partitionable', True)\nNamedSharding = jax.sharding.NamedSharding\nP = jax.sharding.PartitionSpec\n\ndef main(argv):\n    del argv\n    if os.environ.get('BV_JAX_INIT'):\n        jax.distributed.initialize()\n    tf.config.set_visible_devices([], 'GPU')\n    config = flags.FLAGS.config\n    workdir = flags.FLAGS.workdir\n    logging.info(f'\\x1b[33mHello from process {jax.process_index()} holding {jax.local_device_count()}/{jax.device_count()} devices and writing to workdir {workdir}.\\x1b[0m')\n    logging.info(f'The config:\\n{config}')\n    save_ckpt_path = None\n    if workdir:\n        gfile.makedirs(workdir)\n        save_ckpt_path = os.path.join(workdir, 'checkpoint.bv')\n    pool = multiprocessing.pool.ThreadPool(1)\n    for m in config.get('pp_modules', ['ops_general', 'ops_image', 'ops_text']):\n        importlib.import_module(f'big_vision.pp.{m}')\n    xid, wid = (-1, -1)\n    fillin = lambda s: s\n\n    def info(s, *a):\n        logging.info('\\x1b[33mNOTE\\x1b[0m: ' + s, *a)\n\n    def write_note(note):\n        if jax.process_index() == 0:\n            info('%s', note)\n    mw = u.BigVisionMetricWriter(xid, wid, workdir, config)\n    u.chrono.inform(measure=mw.measure, write_note=write_note)\n    config_mesh = config.get('mesh', [('data', jax.device_count())])\n    sharding_rules = config.get('sharding_rules', [('act_batch', 'data')])\n    write_note('Creating device mesh...')\n    mesh = u.create_device_mesh(config_mesh, allow_split_physical_axes=config.get('mesh_allow_split_physical_axes', False))\n    repl_sharding = jax.sharding.NamedSharding(mesh, P())\n    devices_flat = mesh.devices.flatten()\n    write_note('Initializing train dataset...')\n    batch_size = config.input.batch_size\n    if batch_size % jax.device_count() != 0:\n        raise ValueError(f'Batch size ({batch_size}) must be divisible by device number ({jax.device_count()})')\n    info(\"Global batch size %d on %d hosts results in %d local batch size. With %d dev per host (%d dev total), that's a %d per-device batch size.\", batch_size, jax.process_count(), batch_size // jax.process_count(), jax.local_device_count(), jax.device_count(), batch_size // jax.device_count())\n    train_ds, ntrain_img = input_pipeline.training(config.input)\n    total_steps = u.steps('total', config, ntrain_img, batch_size)\n\n    def get_steps(name, default=ValueError, cfg=config):\n        return u.steps(name, cfg, ntrain_img, batch_size, total_steps, default)\n    u.chrono.inform(total_steps=total_steps, global_bs=batch_size, steps_per_epoch=ntrain_img / batch_size)\n    info('Running for %d steps, that means %f epochs', total_steps, total_steps * batch_size / ntrain_img)\n    n_prefetch = config.get('prefetch_to_device', 1)\n    train_iter = input_pipeline.start_global(train_ds, devices_flat, n_prefetch)\n    write_note('Creating model...')\n    model_mod = importlib.import_module(f'big_vision.models.{config.model_name}')\n    model = model_mod.Model(num_classes=config.num_classes, **config.get('model', {}))\n\n    def init(rng):\n        batch = jax.tree.map(lambda x: jnp.zeros(x.shape, x.dtype.as_numpy_dtype), train_ds.element_spec)\n        params = model.init(rng, batch['image'])['params']\n        if 'init_head_bias' in config:\n            params['head']['bias'] = jnp.full_like(params['head']['bias'], config['init_head_bias'])\n        return params\n    rng = jax.random.PRNGKey(u.put_cpu(config.get('seed', 0)))\n    write_note('Inferring parameter shapes...')\n    rng, rng_init = jax.random.split(rng)\n    params_shape = jax.eval_shape(init, rng_init)\n    write_note('Inferring optimizer state shapes...')\n    tx, sched_fns = bv_optax.make(config, nn.unbox(params_shape), sched_kw=dict(total_steps=total_steps, batch_size=batch_size, data_size=ntrain_img))\n    opt_shape = jax.eval_shape(tx.init, params_shape)\n    sched_fns_cpu = [u.jit_cpu()(sched_fn) for sched_fn in sched_fns]\n    if jax.process_index() == 0:\n        num_params = sum((np.prod(p.shape) for p in jax.tree.leaves(params_shape)))\n        mw.measure('num_params', num_params)\n    write_note('Inferring shardings...')\n    train_state_shape = {'params': params_shape, 'opt': opt_shape}\n    strategy = config.get('sharding_strategy', [('.*', 'replicate')])\n    with nn.logical_axis_rules(sharding_rules):\n        train_state_sharding = bv_sharding.infer_sharding(train_state_shape, strategy=strategy, mesh=mesh)\n    write_note('Transferring train_state to devices...')\n    rng_init = u.reshard(rng_init, repl_sharding)\n    params = jax.jit(init, out_shardings=train_state_sharding['params'])(rng_init)\n    opt = jax.jit(tx.init, out_shardings=train_state_sharding['opt'])(params)\n    rng, rng_loop = jax.random.split(rng, 2)\n    rng_loop = u.reshard(rng_loop, repl_sharding)\n    del rng\n    train_state = nn.unbox({'params': params, 'opt': opt})\n    del params, opt\n    write_note('Logging parameter overview...')\n    parameter_overview.log_parameter_overview(train_state['params'], msg='Init params', include_stats='global', jax_logging_process=0)\n\n    @functools.partial(jax.jit, donate_argnums=(0,), out_shardings=(train_state_sharding, repl_sharding))\n    def update_fn(train_state, rng, batch):\n        \"\"\"Update step.\"\"\"\n        images, labels = (batch['image'], batch['labels'])\n        step_count = bv_optax.get_count(train_state['opt'], jittable=True)\n        rng = jax.random.fold_in(rng, step_count)\n        if config.get('mixup') and config.mixup.p:\n            sharded_mixup_fn = shard_map(u.get_mixup(rng, config.mixup.p), mesh=jax.sharding.Mesh(devices_flat, ('data',)), in_specs=P('data'), out_specs=(P(), P('data'), P('data')))\n            rng, (images, labels), _ = sharded_mixup_fn(images, labels)\n        rng, rng_model = jax.random.split(rng, 2)\n\n        def loss_fn(params):\n            logits, _ = model.apply({'params': params}, images, train=True, rngs={'dropout': rng_model})\n            return getattr(u, config.get('loss', 'sigmoid_xent'))(logits=logits, labels=labels)\n        params, opt = (train_state['params'], train_state['opt'])\n        loss, grads = jax.value_and_grad(loss_fn)(params)\n        updates, opt = tx.update(grads, opt, params)\n        params = optax.apply_updates(params, updates)\n        measurements = {'training_loss': loss}\n        gs = jax.tree.leaves(bv_optax.replace_frozen(config.schedule, grads, 0.0))\n        measurements['l2_grads'] = jnp.sqrt(sum([jnp.sum(g * g) for g in gs]))\n        ps = jax.tree.leaves(params)\n        measurements['l2_params'] = jnp.sqrt(sum([jnp.sum(p * p) for p in ps]))\n        us = jax.tree.leaves(updates)\n        measurements['l2_updates'] = jnp.sqrt(sum([jnp.sum(u * u) for u in us]))\n        return ({'params': params, 'opt': opt}, measurements)\n    resume_ckpt_path = None\n    if save_ckpt_path and gfile.exists(f'{save_ckpt_path}-LAST'):\n        resume_ckpt_path = save_ckpt_path\n    elif config.get('resume'):\n        resume_ckpt_path = fillin(config.resume)\n    ckpt_mngr = None\n    if save_ckpt_path or resume_ckpt_path:\n        ckpt_mngr = array_serial.GlobalAsyncCheckpointManager()\n    if resume_ckpt_path:\n        write_note(f'Resuming training from checkpoint {resume_ckpt_path}...')\n        jax.tree.map(lambda x: x.delete(), train_state)\n        del train_state\n        shardings = {**train_state_sharding, 'chrono': jax.tree.map(lambda _: repl_sharding, u.chrono.save())}\n        loaded = u.load_checkpoint_ts(resume_ckpt_path, tree=shardings, shardings=shardings)\n        train_state = {key: loaded[key] for key in train_state_sharding.keys()}\n        u.chrono.load(jax.device_get(loaded['chrono']))\n        del loaded\n    elif config.get('model_init'):\n        write_note(f'Initialize model from {config.model_init}...')\n        train_state['params'] = model_mod.load(train_state['params'], config.model_init, config.get('model'), **config.get('model_load', {}))\n        train_state['params'] = u.reshard(train_state['params'], train_state_sharding['params'])\n        parameter_overview.log_parameter_overview(train_state['params'], msg='restored params', include_stats='global', jax_logging_process=0)\n\n    def eval_logits_fn(train_state, batch):\n        logits, out = model.apply({'params': train_state['params']}, batch['image'])\n        return (logits, out)\n\n    def eval_loss_fn(train_state, batch):\n        logits, _ = model.apply({'params': train_state['params']}, batch['image'])\n        loss_fn = getattr(u, config.get('loss', 'sigmoid_xent'))\n        return {'loss': loss_fn(logits=logits, labels=batch['labels'], reduction=False)}\n    eval_fns = {'predict': eval_logits_fn, 'loss': eval_loss_fn}\n\n    @functools.lru_cache(maxsize=None)\n    def evaluators():\n        return eval_common.from_config(config, eval_fns, lambda s: write_note(f'Init evaluator: {s}…\\n{u.chrono.note}'), lambda key, cfg: get_steps(key, default=None, cfg=cfg), devices_flat)\n    write_note('Inferring the first step number...')\n    first_step_device = bv_optax.get_count(train_state['opt'], jittable=True)\n    first_step = int(jax.device_get(first_step_device))\n    u.chrono.inform(first_step=first_step)\n    if first_step in (total_steps, 0):\n        write_note('Running initial or final evals...')\n        mw.step_start(first_step)\n        for name, evaluator, _, prefix in evaluators():\n            if config.evals[name].get('skip_first') and first_step != total_steps:\n                continue\n            write_note(f'{name} evaluation...\\n{u.chrono.note}')\n            with u.chrono.log_timing(f'z/secs/eval/{name}'):\n                with mesh, nn.logical_axis_rules(sharding_rules):\n                    for key, value in evaluator.run(train_state):\n                        mw.measure(f'{prefix}{key}', value)\n    prof = None\n    write_note('Starting training loop, compiling the first step...')\n    for step, batch in zip(range(first_step + 1, total_steps + 1), train_iter):\n        mw.step_start(step)\n        with jax.profiler.StepTraceAnnotation('train_step', step_num=step):\n            with u.chrono.log_timing('z/secs/update0', noop=step > first_step + 1):\n                with mesh, nn.logical_axis_rules(sharding_rules):\n                    train_state, measurements = update_fn(train_state, rng_loop, batch)\n        if jax.process_index() == 0:\n            prof = u.startstop_prof(prof, step, first_step, get_steps('log_training'))\n        if u.itstime(step, get_steps('log_training'), total_steps, host=0) or (u.chrono.warmup and jax.process_index() == 0):\n            for i, sched_fn_cpu in enumerate(sched_fns_cpu):\n                mw.measure(f\"global_schedule{(i if i else '')}\", sched_fn_cpu(u.put_cpu(step - 1)))\n            measurements = jax.device_get(measurements)\n            for name, value in measurements.items():\n                mw.measure(name, value)\n            u.chrono.tick(step)\n            for k in ('training_loss', 'l2_grads', 'l2_updates', 'l2_params'):\n                if not np.isfinite(measurements.get(k, 0.0)):\n                    raise RuntimeError(f\"{k} became nan or inf somewhere within steps [{step - get_steps('log_training')}, {step}]\")\n        keep_last = total_steps if get_steps('ckpt', None) else None\n        keep_ckpt_steps = get_steps('keep_ckpt', None) or keep_last\n        if save_ckpt_path and ((keep := u.itstime(step, keep_ckpt_steps, total_steps, first=False)) or u.itstime(step, get_steps('ckpt', None), total_steps, first=True)):\n            u.chrono.pause(wait_for=train_state)\n            ckpt = {**train_state}\n            with jax.transfer_guard('allow'):\n                chrono_ckpt = multihost_utils.broadcast_one_to_all(u.chrono.save())\n            chrono_shardings = jax.tree.map(lambda _: repl_sharding, chrono_ckpt)\n            ckpt = ckpt | {'chrono': u.reshard(chrono_ckpt, chrono_shardings)}\n            u.save_checkpoint_ts(ckpt_mngr, ckpt, save_ckpt_path, step, keep)\n            u.chrono.resume()\n        for name, evaluator, log_steps, prefix in evaluators():\n            if u.itstime(step, log_steps, total_steps, first=False, last=True):\n                u.chrono.pause(wait_for=train_state)\n                u.chrono.tick(step)\n                write_note(f'{name} evaluation...\\n{u.chrono.note}')\n                with u.chrono.log_timing(f'z/secs/eval/{name}'):\n                    with mesh, nn.logical_axis_rules(sharding_rules):\n                        for key, value in evaluator.run(train_state):\n                            mw.measure(f'{prefix}{key}', jax.device_get(value))\n                u.chrono.resume()\n        mw.step_end()\n    if jax.process_index() == 0 and prof is not None:\n        u.startstop_prof(prof)\n    write_note(f'Done!\\n{u.chrono.note}')\n    pool.close()\n    pool.join()\n    mw.close()\n    if ckpt_mngr:\n        ckpt_mngr.wait_until_finished()\n    u.sync()\n    u.maybe_cleanup_workdir(workdir, flags.FLAGS.cleanup, info)\nif __name__ == '__main__':\n    app.run(main)",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Existing soft augmentation paradigms fail to account for the highly variable and nonlinear uncertainty introduced by aggressive transformations. Traditional fixed or heuristically softened targets create a mismatch between the degraded visual quality and rigid supervision, leading to suboptimal calibration and limited robustness in data-scarce scenarios. There is a need for an adaptive, meta-learning based mechanism that can estimate per-sample uncertainty and adjust supervision accordingly.",
    "method": "We propose Adaptive Uncertainty-Aware Soft Augmentation (AUASA), a novel framework that integrates meta-learning to dynamically estimate augmentation-induced uncertainty. AUASA includes: (1) an uncertainty meta-model that leverages augmentation parameters and intermediate feature representations to predict sample-specific uncertainty; (2) a target refinement module that uses the predicted uncertainty to adjust soft target labels and loss weightings through a dynamic exponential decay mechanism; and (3) an auxiliary calibration loss that minimizes the Expected Calibration Error (ECE) during training. This multi-task loss framework enables the network to reconcile aggressive and mild augmentations, enhancing both accuracy and model calibration. The method departs from previous approaches by learning the calibration and target softening functions in a data-driven manner rather than relying on fixed or heuristic functions.",
    "experimental_setup": "We will validate AUASA on benchmark datasets such as CIFAR-10, CIFAR-100, and a subset of ImageNet using standard CNN architectures like ResNet-18 and WideResNet. The experimental protocol includes: (i) training a baseline using standard cross-entropy with fixed soft target augmentations and AC-CWSA; (ii) training the same architectures with AUASA; (iii) conducting ablation studies to isolate the impact of the uncertainty meta-model and calibration loss. Evaluation will be performed using top-1 classification accuracy and Expected Calibration Error (ECE), with additional training stability assessments under aggressive augmentation regimes.",
    "primary_metric": "accuracy",
    "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Uncertainty meta-model predicting augmentation-induced uncertainty\nclass UncertaintyMetaModel(nn.Module):\n    def __init__(self, input_dim=4, hidden_dim=16):\n        super(UncertaintyMetaModel, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, aug_features):\n        # predicts uncertainty (higher value means more uncertainty)\n        return self.fc(aug_features).squeeze()\n\n# Adaptive Uncertainty-Aware Soft Augmentation Loss\n\ndef adaptive_uncertainty_loss(logits, targets, aug_metric, uncertainty, k=0.1, alpha=0.5):\n    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n    # Weight from augmentation measured metric: higher aug_metric -> lower weight\n    weight_from_aug = torch.exp(-k * aug_metric)\n    # Further modulate weight using predicted uncertainty\n    weight_from_uncertainty = torch.exp(-alpha * uncertainty)\n    weight = weight_from_aug * weight_from_uncertainty\n    return (ce_loss * weight).mean()\n\n# Example simple CNN for demonstration on CIFAR-10\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(SimpleCNN, self).__init__()\n        self.conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n        self.fc = nn.Linear(16 * 8 * 8, num_classes)\n    def forward(self, x):\n        x = F.relu(self.conv(x))\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# Function to simulate measure of augmentation strength and quality\n\ndef measure_augmentation(aug_params):\n    # Suppose aug_params is a tensor containing transformation intensity and quality metric\n    # Example: aug_params = [rotation, crop, quality]\n    # Calculate a composite aug metric: higher when transformation is stronger and quality is lower\n    rotation, crop, quality = aug_params\n    return rotation + crop * (2 - quality)\n\n# Training step integrating AUASA\n\ndef train_step(model, uncertainty_model, optimizer, images, labels, aug_params):\n    model.train()\n    uncertainty_model.train()\n    optimizer.zero_grad()\n    logits = model(images)\n    # Compute augmentation strength metric\n    aug_metric = torch.tensor(measure_augmentation(aug_params), dtype=torch.float32, device=images.device)\n    # For uncertainty prediction, form a simple feature vector: [rotation, crop, quality, aug_metric]\n    aug_features = torch.tensor([aug_params[0], aug_params[1], aug_params[2], aug_metric.item()], dtype=torch.float32, device=images.device).unsqueeze(0)\n    predicted_uncertainty = uncertainty_model(aug_features)\n    loss = adaptive_uncertainty_loss(logits, labels, aug_metric, predicted_uncertainty, k=0.1, alpha=0.5)\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n# Example Usage\nif __name__ == '__main__':\n    model = SimpleCNN(num_classes=10)\n    uncertainty_model = UncertaintyMetaModel(input_dim=4, hidden_dim=16)\n    optimizer = torch.optim.Adam(list(model.parameters()) + list(uncertainty_model.parameters()), lr=0.001)\n    # Dummy batch input\n    images = torch.randn(32, 3, 32, 32)\n    labels = torch.randint(0, 10, (32,))\n    aug_params = [15.0, 0.2, 0.7]  # Example: rotation=15, crop=0.2, quality=0.7\n    loss = train_step(model, uncertainty_model, optimizer, images, labels, aug_params)\n    print('Training loss:', loss)\n",
    "expected_result": "We expect AUASA to improve classification accuracy by 3-5 percentage points on CIFAR-10 relative to fixed soft augmentation baselines. Moreover, the Expected Calibration Error (ECE) should decrease by at least 20%, and training curves will exhibit reduced variance under aggressive augmentation regimes.",
    "expected_conclusion": "The proposed AUASA method demonstrates a novel integration of meta-learning for uncertainty prediction with adaptive soft target refinement, effectively bridging the gap between aggressive data augmentation and rigid supervision. This advancement not only enhances data-efficient image classification accuracy but also improves model calibration and training stability, offering significant academic insights and practical benefits for scenarios with limited labeled data."
  },
  "experimental_design": {
    "experiment_summary": "This experiment aims to demonstrate the superiority of the proposed Adaptive Uncertainty-Aware Soft Augmentation (AUASA) method for image classification tasks under aggressive data augmentation. The task is to improve standard CNN classification (e.g., ResNet-18) on the CIFAR-10 dataset by dynamically estimating per-sample augmentation-induced uncertainty, thereby refining soft target labels and weighting the classification loss with an auxiliary calibration loss. The workflow involves training a baseline model with fixed soft target augmentation and then training it with the proposed AUASA method. Ablation studies will also be conducted to isolate the contributions of the uncertainty meta-model and calibration loss. The experiment is scaled to fit within the Runner environment (self-hosted, gpu-runner with NVIDIA A100/H200 and sufficient VRAM/large memory), using a single dataset and a single model to keep training and hyperparameter search within practical limits.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA A100 or H200, VRAM: 80 GB or more, RAM: 2048 GB or more"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "This metric represents the top-1 classification accuracy on the test set. It is computed as the number of correctly classified samples divided by the total number of samples. A prediction is counted as correct if the model's highest-probability class matches the ground truth label. This metric is primary because it directly reflects the classification performance which the AUASA method aims to improve. A learning curve plot showing accuracy progression over epochs is an appropriate visualization."
      },
      {
        "name": "Expected Calibration Error (ECE)",
        "description": "ECE measures the calibration of the model by comparing the predicted probabilities with the actual correctness frequency. It is calculated by partitioning the predictions into bins and computing the weighted average difference between accuracy and confidence for each bin. This metric is computed as ECE = Σ|acc(B_m) - conf(B_m)| * (|B_m|/N). It is suitable here because AUASA explicitly incorporates an auxiliary calibration loss aiming to reduce ECE, and bar plots or reliability diagrams are appropriate for visualizing calibration performance."
      }
    ],
    "models_to_use": [
      "ResNet-18 (approximately 11M parameters)"
    ],
    "datasets_to_use": [
      "CIFAR-10"
    ],
    "proposed_method": {
      "method_name": "Adaptive Uncertainty-Aware Soft Augmentation (AUASA)",
      "description": "AUASA integrates a meta-learning based uncertainty estimation module and a target refinement mechanism into the training process. It uses an uncertainty meta-model to dynamically predict per-sample uncertainty from augmentation parameters and intermediate feature representations, which in turn refines soft labels using a dynamic exponential decay mechanism. An auxiliary calibration loss based on Expected Calibration Error (ECE) ensures improved model calibration.",
      "training_config": {
        "learning_rate": 0.001,
        "batch_size": 128,
        "epochs": 100,
        "optimizer": "adam",
        "warmup_steps": 500,
        "weight_decay": 0.0001,
        "gradient_clip": 5.0,
        "scheduler": "cosine",
        "seed": 42,
        "additional_params": "{\"k\": 0.1, \"alpha\": 0.5, \"dynamic_decay\": true, \"auxiliary_calibration_loss_weight\": 0.2}"
      },
      "optuna_config": {
        "enabled": true,
        "n_trials": 20,
        "search_spaces": [
          {
            "param_name": "learning_rate",
            "distribution_type": "loguniform",
            "low": 0.0001,
            "high": 0.01
          },
          {
            "param_name": "weight_decay",
            "distribution_type": "uniform",
            "low": 1e-05,
            "high": 0.001
          }
        ]
      }
    },
    "comparative_methods": [
      {
        "method_name": "Fixed Soft Augmentation Baseline (AC-CWSA)",
        "description": "This baseline method uses a standard fixed or heuristically softened target augmentation scheme (e.g., AC-CWSA). It employs fixed target softening without dynamically adjusting for per-sample uncertainty. Its performance serves as a reference to evaluate the dynamic benefits provided by AUASA in terms of both accuracy and model calibration.",
        "training_config": {
          "learning_rate": 0.001,
          "batch_size": 128,
          "epochs": 100,
          "optimizer": "adam",
          "warmup_steps": 500,
          "weight_decay": 0.0001,
          "gradient_clip": 5.0,
          "scheduler": "cosine",
          "seed": 42,
          "additional_params": "{\"softening_constant\": 0.7}"
        },
        "optuna_config": {
          "enabled": true,
          "n_trials": 20,
          "search_spaces": [
            {
              "param_name": "learning_rate",
              "distribution_type": "loguniform",
              "low": 0.0001,
              "high": 0.01
            },
            {
              "param_name": "softening_constant",
              "distribution_type": "uniform",
              "low": 0.5,
              "high": 0.9
            }
          ]
        }
      }
    ]
  },
  "experiment_code": {
    "train_py": "import os\nimport sys\nfrom pathlib import Path\nfrom typing import Optional, Tuple, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig, OmegaConf\n\nimport wandb\nimport optuna\nfrom sklearn.metrics import confusion_matrix\n\nfrom src.preprocess import get_dataloaders, seed_everything\nfrom src.model import (\n    create_backbone,\n    UncertaintyMetaModel,\n    adaptive_uncertainty_loss,\n    label_smoothing_loss,\n    brier_score_loss,\n    compute_aug_metric,\n    compute_ece,\n)\n\n# -----------------------------------------------------------------------------\n# Optimiser & Scheduler helpers\n# -----------------------------------------------------------------------------\n\ndef _get_optimizer(cfg: DictConfig, params):\n    name = str(cfg.training.optimizer).lower()\n    if name == \"adam\":\n        return torch.optim.Adam(params, lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    if name == \"sgd\":\n        return torch.optim.SGD(\n            params,\n            lr=cfg.training.learning_rate,\n            weight_decay=cfg.training.weight_decay,\n            momentum=0.9,\n            nesterov=True,\n        )\n    raise ValueError(f\"Unsupported optimizer: {cfg.training.optimizer}\")\n\n\ndef _get_scheduler(cfg: DictConfig, optimizer, total_steps: int):\n    name = str(cfg.training.scheduler).lower()\n    if name == \"cosine\":\n        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n    if name == \"multistep\":\n        return torch.optim.lr_scheduler.MultiStepLR(\n            optimizer,\n            milestones=[int(total_steps * 0.6), int(total_steps * 0.85)],\n            gamma=0.1,\n        )\n    return None\n\n# -----------------------------------------------------------------------------\n# WandB initialisation\n# -----------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        resume=\"allow\",\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    print(f\"WandB URL: {run.get_url()}\")\n    return run\n\n# -----------------------------------------------------------------------------\n# Validation / Test helper\n# -----------------------------------------------------------------------------\n\ndef _evaluate(\n    model: nn.Module,\n    dataloader: DataLoader,\n    device: torch.device,\n    num_classes: int,\n    max_batches: Optional[int] = None,\n    return_cm: bool = False,\n) -> Tuple[float, float, float, Optional[np.ndarray]]:\n    \"\"\"Return (loss, accuracy, ece, confusion_matrix|None).\n\n    Critical fix: ECE computation now uses raw logits collected across the\n    evaluation set. Confusion matrix is created with the FULL set of class\n    labels to guarantee a square matrix regardless of prediction coverage.\n    \"\"\"\n    model.eval()\n    criterion = nn.CrossEntropyLoss()\n    total_loss, total_correct, total_samples = 0.0, 0, 0\n    all_logits: List[torch.Tensor] = []\n    all_labels: List[torch.Tensor] = []\n\n    with torch.no_grad():\n        for idx, (images, labels, _) in enumerate(dataloader):\n            if (max_batches is not None) and (idx >= max_batches):\n                break\n\n            images = images.to(device)\n            labels = labels.to(device)\n            logits = model(images)\n            loss = criterion(logits, labels)\n\n            preds = logits.argmax(dim=1)\n\n            # Accumulate statistics\n            total_loss += loss.item() * images.size(0)\n            total_correct += preds.eq(labels).sum().item()\n            total_samples += images.size(0)\n\n            all_logits.append(logits.detach().cpu())\n            all_labels.append(labels.detach().cpu())\n\n    if total_samples == 0:\n        return 0.0, 0.0, 0.0, None\n\n    all_logits_t = torch.cat(all_logits)  # (N, C)\n    all_labels_t = torch.cat(all_labels)  # (N,)\n    all_preds_t = all_logits_t.argmax(dim=1)\n\n    avg_loss = total_loss / total_samples\n    avg_acc = total_correct / total_samples\n\n    # Correct ECE computation on logits!\n    ece = compute_ece(all_logits_t, all_labels_t)\n\n    cm = None\n    if return_cm:\n        cm = confusion_matrix(\n            all_labels_t.numpy(),\n            all_preds_t.numpy(),\n            labels=list(range(num_classes)),\n        )\n    return avg_loss, avg_acc, ece, cm\n\n# -----------------------------------------------------------------------------\n# Core training loop for a single (fixed) configuration\n# -----------------------------------------------------------------------------\n\ndef _train_single(cfg: DictConfig, device: torch.device, trial_mode: bool):\n    train_loader, val_loader, test_loader, num_classes = get_dataloaders(cfg)\n\n    # Limit batches sharply in trial mode\n    train_max_batches = 2 if trial_mode else None\n    eval_max_batches = 1 if trial_mode else None\n\n    backbone = create_backbone(cfg.model.name, num_classes).to(device)\n    use_auasa = \"adaptive\" in cfg.method.lower() or \"proposed\" in cfg.run_id.lower()\n    if use_auasa:\n        uncertainty_model = UncertaintyMetaModel(input_dim=4, hidden_dim=16).to(device)\n        parameters = list(backbone.parameters()) + list(uncertainty_model.parameters())\n    else:\n        uncertainty_model = None\n        parameters = list(backbone.parameters())\n\n    optimizer = _get_optimizer(cfg, parameters)\n    total_steps = cfg.training.epochs * (\n        train_max_batches if train_max_batches is not None else len(train_loader)\n    )\n    scheduler = _get_scheduler(cfg, optimizer, total_steps)\n\n    # Post-init assertion – ensure output dimensions are valid\n    assert (\n        backbone(torch.randn(2, 3, 32, 32, device=device)).shape[1] == num_classes\n    ), \"Model output dimension mismatch with dataset classes\"\n\n    run = _init_wandb(cfg)\n\n    best_val_acc = 0.0\n    global_step = 0\n\n    for epoch in range(cfg.training.epochs):\n        backbone.train()\n        if uncertainty_model:\n            uncertainty_model.train()\n\n        total_loss_e, total_correct_e, total_samples_e = 0.0, 0, 0\n\n        for b_idx, (imgs, lbls, aug_p) in enumerate(train_loader):\n            if (train_max_batches is not None) and (b_idx >= train_max_batches):\n                break\n\n            if epoch == 0 and b_idx == 0:\n                # Batch-start assertion\n                assert imgs.size(0) == lbls.size(0), \"Image/label size mismatch in first batch\"\n\n            imgs = imgs.to(device)\n            lbls = lbls.to(device)\n            aug_p = aug_p.to(device)\n\n            optimizer.zero_grad()\n            logits = backbone(imgs)\n\n            if use_auasa:\n                aug_metric = compute_aug_metric(aug_p)\n                aug_feat = torch.cat([aug_p, aug_metric.unsqueeze(1)], dim=1)\n                uncertainty = uncertainty_model(aug_feat)\n                k = float(getattr(cfg.training.additional_params, \"k\", 0.1))\n                alpha = float(getattr(cfg.training.additional_params, \"alpha\", 0.5))\n                loss_main = adaptive_uncertainty_loss(\n                    logits, lbls, aug_metric, uncertainty, k=k, alpha=alpha\n                )\n                aux_w = float(\n                    getattr(cfg.training.additional_params, \"auxiliary_calibration_loss_weight\", 0.2)\n                )\n                loss = loss_main + aux_w * brier_score_loss(logits, lbls)\n            else:\n                smooth = float(getattr(cfg.training.additional_params, \"softening_constant\", 0.1))\n                loss = label_smoothing_loss(logits, lbls, smooth_factor=smooth)\n\n            loss.backward()\n\n            grads = [p.grad for p in parameters if p.requires_grad]\n            # Pre-optimizer assertions\n            assert all(g is not None for g in grads), \"Some gradients are None before optimizer.step()\"\n            assert any(g.abs().sum().item() > 0 for g in grads), \"All gradients zero before optimizer.step()\"\n\n            if cfg.training.gradient_clip > 0:\n                torch.nn.utils.clip_grad_norm_(parameters, cfg.training.gradient_clip)\n\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n\n            with torch.no_grad():\n                preds = logits.argmax(dim=1)\n                batch_acc = preds.eq(lbls).float().mean().item()\n\n            total_loss_e += loss.item() * imgs.size(0)\n            total_correct_e += preds.eq(lbls).sum().item()\n            total_samples_e += imgs.size(0)\n\n            global_step += 1\n            if run:\n                wandb.log({\"train_loss\": loss.item(), \"train_acc\": batch_acc}, step=global_step)\n\n        epoch_loss = total_loss_e / max(1, total_samples_e)\n        epoch_acc = total_correct_e / max(1, total_samples_e)\n\n        val_loss, val_acc, val_ece, _ = _evaluate(\n            backbone, val_loader, device, num_classes, max_batches=eval_max_batches\n        )\n\n        if run:\n            wandb.log(\n                {\n                    \"epoch\": epoch,\n                    \"train_loss_epoch\": epoch_loss,\n                    \"train_acc_epoch\": epoch_acc,\n                    \"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"val_ece\": val_ece,\n                },\n                step=global_step,\n            )\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            ckpt_dir = Path(cfg.results_dir) / cfg.run_id\n            ckpt_dir.mkdir(parents=True, exist_ok=True)\n            torch.save(\n                {\n                    \"backbone\": backbone.state_dict(),\n                    \"uncertainty_model\": uncertainty_model.state_dict() if uncertainty_model else None,\n                },\n                ckpt_dir / \"best.pt\",\n            )\n\n    # ------------------------------------------------------------------\n    # Final evaluation on test set (always compute confusion matrix)\n    # ------------------------------------------------------------------\n    test_loss, test_acc, test_ece, test_cm = _evaluate(\n        backbone,\n        test_loader,\n        device,\n        num_classes,\n        max_batches=eval_max_batches,\n        return_cm=True,\n    )\n\n    if run:\n        run.summary[\"best_val_acc\"] = best_val_acc\n        run.summary[\"test_loss\"] = test_loss\n        run.summary[\"test_ece\"] = test_ece\n        run.summary[\"accuracy\"] = test_acc  # primary metric\n        if test_cm is not None:\n            run.summary[\"confusion_matrix\"] = test_cm.tolist()\n        run.finish()\n\n# -----------------------------------------------------------------------------\n# Optuna objective (lightweight tuning)\n# -----------------------------------------------------------------------------\n\ndef _objective(trial: optuna.Trial, cfg: DictConfig, device: torch.device):\n    # Suggest hyper-parameters according to configured search spaces\n    for space in cfg.optuna.search_spaces:\n        dist = space.distribution_type.lower()\n        if dist == \"loguniform\":\n            val = trial.suggest_float(space.param_name, space.low, space.high, log=True)\n        elif dist == \"uniform\":\n            val = trial.suggest_float(space.param_name, space.low, space.high)\n        else:\n            raise ValueError(f\"Unsupported distribution: {dist}\")\n        OmegaConf.update(cfg, f\"training.{space.param_name}\", val, merge=False)\n\n    # Short training for speed during optimisation\n    original_epochs = cfg.training.epochs\n    cfg.training.epochs = min(3, original_epochs)\n\n    train_loader, val_loader, _, num_classes = get_dataloaders(cfg)\n\n    model = create_backbone(cfg.model.name, num_classes).to(device)\n    opt = _get_optimizer(cfg, model.parameters())\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_acc = 0.0\n    for _ in range(cfg.training.epochs):\n        model.train()\n        for imgs, lbls, _ in train_loader:\n            opt.zero_grad()\n            imgs, lbls = imgs.to(device), lbls.to(device)\n            loss = criterion(model(imgs), lbls)\n            loss.backward()\n            opt.step()\n        # Val after each epoch – 2 batches only\n        _, val_acc, _, _ = _evaluate(\n            model, val_loader, device, num_classes, max_batches=2\n        )\n        best_val_acc = max(best_val_acc, val_acc)\n\n    cfg.training.epochs = original_epochs  # restore\n    return 1.0 - best_val_acc  # minimise 1-acc\n\n# -----------------------------------------------------------------------------\n# Hydra entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # ------------------------------------------------------------------\n    # Merge run-specific YAML (external)\n    # ------------------------------------------------------------------\n    run_cfg_path = Path(get_original_cwd()) / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run config not found: {run_cfg_path}\")\n\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_path))\n\n    # ------------------------------------------------------------------\n    # Mode overrides\n    # ------------------------------------------------------------------\n    mode = str(cfg.mode).lower()\n    if mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.batch_size = min(8, cfg.training.batch_size)\n    elif mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    Path(cfg.results_dir).mkdir(parents=True, exist_ok=True)\n\n    seed_everything(int(cfg.training.seed))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Optuna hyper-parameter search (if enabled & not trial mode)\n    # ------------------------------------------------------------------\n    if int(cfg.optuna.n_trials) > 0:\n        study = optuna.create_study(direction=\"minimize\")\n        study.optimize(lambda t: _objective(t, cfg, device), n_trials=int(cfg.optuna.n_trials))\n        # Apply best params\n        for k, v in study.best_trial.params.items():\n            OmegaConf.update(cfg, f\"training.{k}\", v, merge=False)\n\n    _train_single(cfg, device, trial_mode=(mode == \"trial\"))\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats\n\n# -----------------------------------------------------------------------------\n# Load global WandB settings\n# -----------------------------------------------------------------------------\n\ndef _load_global_wandb_cfg() -> Dict[str, str]:\n    root = Path(__file__).resolve().parent.parent\n    cfg = OmegaConf.load(root / \"config\" / \"config.yaml\")\n    return {\"entity\": cfg.wandb.entity, \"project\": cfg.wandb.project}\n\n# -----------------------------------------------------------------------------\n# Simple CLI parser (KEY=VALUE)\n# -----------------------------------------------------------------------------\n\ndef _parse_cli() -> Dict[str, str]:\n    if len(sys.argv) < 3:\n        raise ValueError(\"Usage: python -m src.evaluate results_dir=PATH run_ids='[ ]'\")\n    kv = {}\n    for arg in sys.argv[1:]:\n        if \"=\" not in arg:\n            raise ValueError(f\"Malformed arg {arg}\")\n        k, v = arg.split(\"=\", 1)\n        kv[k] = v\n    if {\"results_dir\", \"run_ids\"} - kv.keys():\n        raise ValueError(\"Missing required arguments\")\n    return kv\n\n# -----------------------------------------------------------------------------\n# Per-run processing\n# -----------------------------------------------------------------------------\n\ndef _export_run(run: wandb.apis.public.Run, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    hist = run.history()\n    summary = dict(run.summary._json_dict)\n    config = dict(run.config)\n\n    (out_dir / \"metrics.json\").write_text(\n        json.dumps({\"history\": hist.to_dict(orient=\"list\"), \"summary\": summary, \"config\": config}, indent=2)\n    )\n\n    # Learning curves (if present)\n    if {\"train_acc_epoch\", \"val_acc\"}.issubset(hist.columns):\n        plt.figure(figsize=(6, 4))\n        sns.lineplot(x=hist.index, y=hist[\"train_acc_epoch\"], label=\"train\")\n        sns.lineplot(x=hist.index, y=hist[\"val_acc\"], label=\"val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"Learning Curves – {run.id}\")\n        plt.tight_layout()\n        fig_p = out_dir / f\"{run.id}_learning_curve.pdf\"\n        plt.savefig(fig_p)\n        plt.close()\n        print(fig_p)\n\n    # Confusion matrix\n    if \"confusion_matrix\" in summary:\n        cm = np.array(summary[\"confusion_matrix\"])\n        plt.figure(figsize=(5, 4))\n        sns.heatmap(cm, cmap=\"Blues\", annot=False)\n        plt.title(f\"Confusion Matrix – {run.id}\")\n        plt.tight_layout()\n        fig_p = out_dir / f\"{run.id}_confusion_matrix.pdf\"\n        plt.savefig(fig_p)\n        plt.close()\n        print(fig_p)\n\n# -----------------------------------------------------------------------------\n# Aggregate metrics across runs\n# -----------------------------------------------------------------------------\n\ndef _aggregate(runs: List[wandb.apis.public.Run], results_dir: Path):\n    primary_metric = \"accuracy\"\n    agg: Dict = {\"primary_metric\": primary_metric, \"metrics\": {}}\n\n    for r in runs:\n        for k, v in r.summary.items():\n            if isinstance(v, (float, int)):\n                agg[\"metrics\"].setdefault(k, {})[r.id] = float(v)\n\n    proposed = [r for r in runs if \"proposed\" in r.id.lower() or \"adaptive\" in r.id.lower()]\n    baseline = [r for r in runs if \"comparative\" in r.id.lower() or \"baseline\" in r.id.lower()]\n\n    def _best(rs):\n        if not rs:\n            return None, 0.0\n        best = max(rs, key=lambda x: float(x.summary.get(primary_metric, 0.0)))\n        return best.id, float(best.summary.get(primary_metric, 0.0))\n\n    best_prop_id, best_prop_val = _best(proposed)\n    best_base_id, best_base_val = _best(baseline)\n\n    agg[\"best_proposed\"] = {\"run_id\": best_prop_id, \"value\": best_prop_val}\n    agg[\"best_baseline\"] = {\"run_id\": best_base_id, \"value\": best_base_val}\n\n    # For metrics that should be minimized (loss, error), we invert sign\n    maximize = primary_metric.lower() not in {\"loss\", \"error\", \"ece\", \"perplexity\"}\n    if maximize:\n        gap_pct = (\n            (best_prop_val - best_base_val) / best_base_val * 100.0 if best_base_val != 0 else 0.0\n        )\n    else:\n        gap_pct = (\n            (best_base_val - best_prop_val) / best_base_val * 100.0 if best_base_val != 0 else 0.0\n        )\n    agg[\"gap\"] = gap_pct\n\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n    (comp_dir / \"aggregated_metrics.json\").write_text(json.dumps(agg, indent=2))\n\n    # Bar chart for primary metric\n    if primary_metric in agg[\"metrics\"]:\n        m = agg[\"metrics\"][primary_metric]\n        plt.figure(figsize=(8, 4))\n        sns.barplot(x=list(m.keys()), y=list(m.values()), palette=\"viridis\")\n        plt.ylabel(primary_metric)\n        plt.title(\"Primary Metric Comparison\")\n        for i, (k, v) in enumerate(m.items()):\n            plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        fig_p = comp_dir / \"comparison_accuracy_bar_chart.pdf\"\n        plt.savefig(fig_p)\n        plt.close()\n        print(fig_p)\n\n        labels = [\"proposed\" if k == best_prop_id else \"baseline\" for k in m.keys()]\n        plt.figure(figsize=(6, 4))\n        sns.boxplot(x=labels, y=list(m.values()))\n        sns.swarmplot(x=labels, y=list(m.values()), color=\"black\", size=3)\n        plt.ylabel(primary_metric)\n        plt.title(\"Distribution – Primary Metric\")\n        plt.tight_layout()\n        fig_p = comp_dir / \"comparison_accuracy_box_plot.pdf\"\n        plt.savefig(fig_p)\n        plt.close()\n        print(fig_p)\n\n        # Significance test if multiple runs each side\n        if len(proposed) >= 2 and len(baseline) >= 2:\n            prop_scores = [float(r.summary.get(primary_metric, 0.0)) for r in proposed]\n            base_scores = [float(r.summary.get(primary_metric, 0.0)) for r in baseline]\n            t_stat, p_val = stats.ttest_ind(prop_scores, base_scores, equal_var=False)\n            (comp_dir / \"significance_test.json\").write_text(\n                json.dumps({\"t_statistic\": float(t_stat), \"p_value\": float(p_val)}, indent=2)\n            )\n            print(comp_dir / \"significance_test.json\")\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    cli = _parse_cli()\n    res_dir = Path(cli[\"results_dir\"]).expanduser().resolve()\n    run_ids = json.loads(cli[\"run_ids\"])\n\n    wb_cfg = _load_global_wandb_cfg()\n    api = wandb.Api()\n\n    runs: List[wandb.apis.public.Run] = []\n    for rid in run_ids:\n        run = api.run(f\"{wb_cfg['entity']}/{wb_cfg['project']}/{rid}\")\n        runs.append(run)\n        _export_run(run, res_dir / rid)\n\n    _aggregate(runs, res_dir)\n",
    "preprocess_py": "import random\nfrom pathlib import Path\nfrom typing import Tuple, List, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\n\n# Global cache root – MUST be exactly `.cache/`\nCACHE_ROOT = Path(\".cache\")\n\n# -----------------------------------------------------------------------------\n# Reproducibility helpers\n# -----------------------------------------------------------------------------\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# -----------------------------------------------------------------------------\n# Augmentation with parameter capture\n# -----------------------------------------------------------------------------\n\nclass AugmentAndRecord:\n    \"\"\"Training augmentation pipeline that also returns normalised augmentation parameters.\n\n    Returned parameter tensor p has three elements in [0,1] representing:\n    1. rotation magnitude / max_rotation\n    2. crop reduction (i.e. 1 - crop_scale)\n    3. colour jitter magnitude / max_jitter\n    \"\"\"\n\n    def __init__(\n        self,\n        max_rotation: int = 15,\n        min_crop: float = 0.8,\n        max_jitter: float = 0.4,\n        mean: Tuple[float, float, float] = (0.4914, 0.4822, 0.4465),\n        std: Tuple[float, float, float] = (0.2470, 0.2435, 0.2616),\n    ):\n        self.max_rotation = max_rotation\n        self.min_crop = min_crop\n        self.max_jitter = max_jitter\n        self.normalize = transforms.Normalize(mean, std)\n\n    def __call__(self, img):\n        rot_deg = random.uniform(-self.max_rotation, self.max_rotation)\n        crop_scale = random.uniform(self.min_crop, 1.0)\n        jitter = random.uniform(0.0, self.max_jitter)\n\n        transform = transforms.Compose(\n            [\n                transforms.RandomRotation((rot_deg, rot_deg)),\n                transforms.RandomResizedCrop(32, scale=(crop_scale, 1.0)),\n                transforms.ColorJitter(\n                    brightness=jitter, contrast=jitter, saturation=jitter, hue=0\n                ),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n        img_t = transform(img)\n        params = torch.tensor(\n            [\n                abs(rot_deg) / self.max_rotation,\n                1.0 - crop_scale,\n                jitter / (self.max_jitter if self.max_jitter > 0 else 1.0),\n            ],\n            dtype=torch.float32,\n        )\n        return img_t, params\n\n\nclass EvalTransform:\n    def __init__(self, mean: Tuple[float, float, float], std: Tuple[float, float, float]):\n        self.t = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n\n    def __call__(self, img):\n        return self.t(img), torch.zeros(3, dtype=torch.float32)\n\n\nclass CIFAR10WithParams(datasets.CIFAR10):\n    \"\"\"Wrap torchvision CIFAR-10 to expose augmentation param tensor.\"\"\"\n\n    def __init__(self, root: Any, train: bool, transform=None, download: bool = False):\n        super().__init__(root=root, train=train, transform=None, download=download)\n        self._tfm = transform\n\n    def __getitem__(self, idx):\n        img, lbl = super().__getitem__(idx)\n        img_t, p = self._tfm(img)\n        return img_t, lbl, p\n\n# -----------------------------------------------------------------------------\n# Dataloader factory\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg):\n    seed_everything(int(cfg.training.seed))\n    CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n\n    if cfg.dataset.name.lower() != \"cifar10\":\n        raise ValueError(\"Only CIFAR-10 supported in this implementation.\")\n\n    mean = tuple(cfg.dataset.preprocessing.normalization.mean)\n    std = tuple(cfg.dataset.preprocessing.normalization.std)\n\n    tf_train = AugmentAndRecord(mean=mean, std=std)\n    tf_eval = EvalTransform(mean, std)\n\n    full_train = CIFAR10WithParams(CACHE_ROOT, train=True, transform=tf_train, download=True)\n\n    val_count = int(cfg.dataset.split.val)\n    indices = list(range(len(full_train)))\n    random.shuffle(indices)\n    val_idx = indices[:val_count]\n    train_idx = indices[val_count:]\n\n    train_set = Subset(full_train, train_idx)\n\n    val_base = CIFAR10WithParams(CACHE_ROOT, train=True, transform=tf_eval, download=False)\n    val_set = Subset(val_base, val_idx)\n\n    test_set = CIFAR10WithParams(CACHE_ROOT, train=False, transform=tf_eval, download=True)\n\n    dl_train = DataLoader(\n        train_set,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True,\n    )\n    dl_val = DataLoader(\n        val_set,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n    )\n    dl_test = DataLoader(\n        test_set,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    return dl_train, dl_val, dl_test, 10\n",
    "model_py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n# -----------------------------------------------------------------------------\n# Backbone factory\n# -----------------------------------------------------------------------------\n\ndef create_backbone(name: str, num_classes: int) -> nn.Module:\n    name = name.lower()\n    if name == \"resnet18\":\n        model = models.resnet18(weights=None)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        return model\n    raise ValueError(f\"Unsupported backbone: {name}\")\n\n# -----------------------------------------------------------------------------\n# AUASA components – Uncertainty meta-model\n# -----------------------------------------------------------------------------\n\nclass UncertaintyMetaModel(nn.Module):\n    def __init__(self, input_dim: int = 4, hidden_dim: int = 16):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n\n    def forward(self, x):  # x: (B, input_dim)\n        return self.net(x).squeeze(-1)\n\n# -----------------------------------------------------------------------------\n# Losses & metrics helpers\n# -----------------------------------------------------------------------------\n\ndef adaptive_uncertainty_loss(\n    logits: torch.Tensor,\n    targets: torch.Tensor,\n    aug_metric: torch.Tensor,\n    uncertainty: torch.Tensor,\n    k: float = 0.1,\n    alpha: float = 0.5,\n):\n    ce = F.cross_entropy(logits, targets, reduction=\"none\")\n    w_aug = torch.exp(-k * aug_metric)\n    w_unc = torch.exp(-alpha * uncertainty)\n    w = w_aug * w_unc\n    return (ce * w).mean()\n\n\ndef label_smoothing_loss(logits: torch.Tensor, targets: torch.Tensor, smooth_factor: float = 0.1):\n    n_cls = logits.size(1)\n    with torch.no_grad():\n        true_dist = torch.empty_like(logits).fill_(smooth_factor / (n_cls - 1))\n        true_dist.scatter_(1, targets.unsqueeze(1), 1 - smooth_factor)\n    return torch.mean(torch.sum(-true_dist * F.log_softmax(logits, dim=1), dim=1))\n\n\ndef brier_score_loss(logits: torch.Tensor, targets: torch.Tensor):\n    prob = F.softmax(logits, dim=1)\n    one_hot = torch.zeros_like(prob).scatter_(1, targets.unsqueeze(1), 1)\n    return ((prob - one_hot) ** 2).sum(1).mean()\n\n# -----------------------------------------------------------------------------\n# Augmentation metric\n# -----------------------------------------------------------------------------\n\ndef compute_aug_metric(p: torch.Tensor) -> torch.Tensor:\n    # p: (B, 3) – [rotation_norm, crop_norm, jitter_norm]\n    rot, crop, jit = p[:, 0], p[:, 1], p[:, 2]\n    return rot + crop * (2 - jit)\n\n# -----------------------------------------------------------------------------\n# ECE computation\n# -----------------------------------------------------------------------------\n\ndef compute_ece(logits: torch.Tensor, labels: torch.Tensor, n_bins: int = 15):\n    \"\"\"Expected Calibration Error on logits.\n\n    logits: Tensor of shape (N, C)\n    labels: Tensor of shape (N,)\n    \"\"\"\n    prob = F.softmax(logits, dim=1)\n    conf, pred = prob.max(dim=1)\n    acc = pred.eq(labels).float()\n\n    bins = torch.linspace(0, 1, n_bins + 1, device=logits.device)\n    ece = torch.zeros(1, device=logits.device)\n    for i in range(n_bins):\n        mask = (conf > bins[i]) & (conf <= bins[i + 1])\n        if mask.sum() == 0:\n            continue\n        ece += (conf[mask].mean() - acc[mask].mean()).abs() * mask.float().mean()\n    return ece.item()\n",
    "main_py": "import subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\nfrom hydra.utils import get_original_cwd\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    if cfg.run is None:\n        raise ValueError(\"run=<run_id> must be specified\")\n    if cfg.mode not in {\"trial\", \"full\"}:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    script = Path(get_original_cwd()) / \"src\" / \"train.py\"\n    cmd = [\n        sys.executable,\n        \"-u\",\n        str(script),\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\"Launching:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[tool.poetry]\nname = \"auasa-experiments\"\nversion = \"0.1.0\"\ndescription = \"Experimental codebase for AUASA paper\"\nauthors = [\"Researcher <researcher@example.com>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.16.0\"\noptuna = \"^3.5.0\"\nseaborn = \"^0.13.0\"\nmatplotlib = \"^3.8.0\"\npandas = \"^2.1.0\"\nscikit-learn = \"^1.3.0\"\nscipy = \"^1.11.0\"\n\n[build-system]\nrequires = [\"poetry>=1.5.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n",
    "config_yaml": "defaults: []\n\nrun: null  # pass via CLI\nresults_dir: ./results\nmode: full  # trial or full\n\nwandb:\n  entity: gengaru617-personal\n  project: 2026-01-22-2\n  mode: online  # updated depending on mode at runtime\n",
    "run_configs": {
      "proposed-resnet18-cifar10": "run_id: proposed-resnet18-cifar10\nmethod: Adaptive Uncertainty-Aware Soft Augmentation (AUASA)\nmodel:\n  name: resnet18\n  num_parameters: 11M\ndataset:\n  name: cifar10\n  preprocessing:\n    normalization:\n      mean: [0.4914, 0.4822, 0.4465]\n      std: [0.2470, 0.2435, 0.2616]\n  split:\n    train: 45000\n    val: 5000\n    test: 10000\ntraining:\n  learning_rate: 0.001\n  batch_size: 128\n  epochs: 100\n  optimizer: adam\n  warmup_steps: 500\n  scheduler: cosine\n  weight_decay: 0.0001\n  gradient_clip: 5.0\n  seed: 42\n  additional_params:\n    k: 0.1\n    alpha: 0.5\n    dynamic_decay: true\n    auxiliary_calibration_loss_weight: 0.2\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: learning_rate\n      distribution_type: loguniform\n      low: 0.0001\n      high: 0.01\n    - param_name: weight_decay\n      distribution_type: uniform\n      low: 1e-05\n      high: 0.001\n",
      "comparative-1-resnet18-cifar10": "run_id: comparative-1-resnet18-cifar10\nmethod: Fixed Soft Augmentation Baseline (AC-CWSA)\nmodel:\n  name: resnet18\n  num_parameters: 11M\ndataset:\n  name: cifar10\n  preprocessing:\n    normalization:\n      mean: [0.4914, 0.4822, 0.4465]\n      std: [0.2470, 0.2435, 0.2616]\n  split:\n    train: 45000\n    val: 5000\n    test: 10000\ntraining:\n  learning_rate: 0.001\n  batch_size: 128\n  epochs: 100\n  optimizer: adam\n  warmup_steps: 500\n  scheduler: cosine\n  weight_decay: 0.0001\n  gradient_clip: 5.0\n  seed: 42\n  additional_params:\n    softening_constant: 0.7\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: learning_rate\n      distribution_type: loguniform\n      low: 0.0001\n      high: 0.01\n    - param_name: softening_constant\n      distribution_type: uniform\n      low: 0.5\n      high: 0.9\n"
    }
  }
}